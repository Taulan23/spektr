#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–ù–ê–õ–ò–ó 5 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í –ë–ï–ó –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò –®–£–ú–ê
–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–∞—É—á–Ω–∏–∫–∞
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import os
import glob

def load_spectral_data_5_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 5 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤"""
    
    # –í—ã–±–∏—Ä–∞–µ–º 5 –≤–∏–¥–æ–≤: –±–µ—Ä–µ–∑–∞, –¥—É–±, –µ–ª—å, –∫–ª–µ–Ω, —Å–æ—Å–Ω–∞
    species_dirs = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '—Å–æ—Å–Ω–∞']
    
    data = []
    labels = []
    
    for species in species_dirs:
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {species}...")
        
        # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
        if species == '–∫–ª–µ–Ω':
            # –î–ª—è –∫–ª–µ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–µ—Å–µ–Ω–Ω–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            pattern = f"–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤/–∫–ª–µ–Ω/*.xlsx"
        else:
            pattern = f"{species}/*.xlsx"
        
        files = glob.glob(pattern)
        
        for file in files[:30]:  # –ë–µ—Ä–µ–º –ø–æ 30 —Å–ø–µ–∫—Ç—Ä–æ–≤ –Ω–∞ –≤–∏–¥
            try:
                df = pd.read_excel(file)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–±—ã—á–Ω–æ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏)
                spectral_data = []
                for col in df.columns:
                    if pd.api.types.is_numeric_dtype(df[col]):
                        spectral_data.extend(df[col].dropna().values)
                
                if len(spectral_data) > 0:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª–∏–Ω—É —Å–ø–µ–∫—Ç—Ä–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Ç–æ—á–µ–∫)
                    spectral_data = spectral_data[:1000]
                    if len(spectral_data) < 1000:
                        spectral_data.extend([0] * (1000 - len(spectral_data)))
                    
                    data.append(spectral_data)
                    labels.append(species)
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file}: {e}")
                continue
    
    return np.array(data), np.array(labels)

def add_noise_to_data(X, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –¥–∞–Ω–Ω—ã–º"""
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

def analyze_5_species_no_augmentation():
    """–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ 5 –≤–∏–¥–æ–≤ –±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    print("="*80)
    print("üå≥ –ê–ù–ê–õ–ò–ó 5 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í –ë–ï–ó –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò –®–£–ú–ê")
    print("="*80)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X, y = load_spectral_data_5_species()
    
    print(f"üìä –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
    print(f"   ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X)}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")
    print(f"   ‚Ä¢ –í–∏–¥—ã: {np.unique(y)}")
    
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (50/50 –∫–∞–∫ —É –Ω–∞—É—á–Ω–∏–∫–∞)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.5, random_state=42, stratify=y_encoded
    )
    
    print(f"\nüìã –†–ê–ó–ë–ò–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"   ‚Ä¢ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   ‚Ä¢ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   ‚Ä¢ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 50/50 (–∫–∞–∫ —É –Ω–∞—É—á–Ω–∏–∫–∞)")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å Extra Trees (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
    print(f"\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò:")
    print(f"   ‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º: Extra Trees")
    print(f"   ‚Ä¢ n_estimators: 1712")
    print(f"   ‚Ä¢ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —à—É–º–∞: –ù–ï–¢")
    
    model = ExtraTreesClassifier(
        n_estimators=1712,
        max_depth=None,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        random_state=42
    )
    
    model.fit(X_train_scaled, y_train)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —à—É–º–∞
    noise_levels = [0.0, 0.01, 0.10]  # 0%, 1%, 10%
    results = {}
    
    print(f"\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –†–ê–ó–ù–´–• –£–†–û–í–ù–Ø–• –®–£–ú–ê:")
    
    for noise_level in noise_levels:
        print(f"\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å {noise_level*100:.0f}% —à—É–º–∞:")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        if noise_level > 0:
            X_test_noisy = add_noise_to_data(X_test_scaled, noise_level)
        else:
            X_test_noisy = X_test_scaled
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test_noisy)
        accuracy = accuracy_score(y_test, y_pred)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        results[noise_level] = {
            'accuracy': accuracy,
            'confusion_matrix': cm,
            'confusion_matrix_normalized': cm_normalized,
            'predictions': y_pred
        }
        
        print(f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        class_names = le.classes_
        for i, class_name in enumerate(class_names):
            class_accuracy = cm_normalized[i, i]
            print(f"   ‚Ä¢ {class_name}: {class_accuracy:.1%}")
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    create_visualizations(results, le.classes_)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_results(results, le.classes_)
    
    print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã:")
    print(f"   ‚Ä¢ confusion_matrices_5_species.png")
    print(f"   ‚Ä¢ results_5_species_no_augmentation.txt")

def create_visualizations(results, class_names):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
    
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    
    for idx, (noise_level, result) in enumerate(results.items()):
        cm_normalized = result['confusion_matrix_normalized']
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df_cm = pd.DataFrame(cm_normalized, 
                           index=class_names, 
                           columns=class_names)
        
        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        sns.heatmap(df_cm, 
                   annot=True, 
                   fmt='.3f', 
                   cmap='RdYlBu_r',
                   ax=axes[idx],
                   cbar_kws={'label': '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å'},
                   square=True,
                   linewidths=0.5,
                   linecolor='white',
                   annot_kws={'size': 10})
        
        accuracy = result['accuracy']
        axes[idx].set_title(f'{noise_level*100:.0f}% —à—É–º–∞\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}', 
                           fontsize=12, fontweight='bold')
        axes[idx].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        axes[idx].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    
    plt.tight_layout()
    plt.savefig('confusion_matrices_5_species.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_results(results, class_names):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"""
    
    with open('results_5_species_no_augmentation.txt', 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê 5 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í –ë–ï–ó –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ò –®–£–ú–ê\n")
        f.write("="*80 + "\n\n")
        
        f.write("–ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò:\n")
        f.write("- –ê–ª–≥–æ—Ä–∏—Ç–º: Extra Trees\n")
        f.write("- n_estimators: 1712\n")
        f.write("- max_depth: None\n")
        f.write("- –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: 50/50 (–∫–∞–∫ —É –Ω–∞—É—á–Ω–∏–∫–∞)\n")
        f.write("- –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —à—É–º–∞: –ù–ï–¢\n\n")
        
        for noise_level, result in results.items():
            f.write(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø {noise_level*100:.0f}% –®–£–ú–ê:\n")
            f.write("-" * 50 + "\n")
            
            accuracy = result['accuracy']
            f.write(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}\n\n")
            
            cm = result['confusion_matrix']
            cm_normalized = result['confusion_matrix_normalized']
            
            f.write("–°–´–†–ê–Ø –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:\n")
            for i, class_name in enumerate(class_names):
                f.write(f"{class_name}: {cm[i].tolist()}\n")
            f.write("\n")
            
            f.write("–ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–ê–Ø –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:\n")
            for i, class_name in enumerate(class_names):
                f.write(f"{class_name}: {[f'{val:.3f}' for val in cm_normalized[i]]}\n")
            f.write("\n")
            
            f.write("–¢–û–ß–ù–û–°–¢–¨ –ü–û –ö–õ–ê–°–°–ê–ú:\n")
            for i, class_name in enumerate(class_names):
                class_accuracy = cm_normalized[i, i]
                f.write(f"{class_name}: {class_accuracy:.1%}\n")
            f.write("\n" + "="*80 + "\n\n")

if __name__ == "__main__":
    analyze_5_species_no_augmentation() 