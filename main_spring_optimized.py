import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
import pickle
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import random
import itertools
from datetime import datetime
import json
warnings.filterwarnings('ignore')

# –§–∏–∫—Å–∏—Ä—É–µ–º random seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
print(f"üé≤ Random seed –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω: {RANDOM_SEED}")

def load_spring_spectral_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_spectra = []
    all_labels = []
    
    print("üå∏ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–µ–Ω–Ω–∏—Ö —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    print("="*60)
    
    # –ü—É—Ç—å –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    
    for tree_type in tree_types:
        folder_path = os.path.join(base_path, tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*_vis.xlsx'))
            print(f"üìÅ {tree_type}: {len(excel_files)} —Ñ–∞–π–ª–æ–≤")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    
                    if df.shape[1] >= 2:
                        # –ë–µ—Ä–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤—Ç–æ—Ä–æ–π —Å—Ç–æ–ª–±–µ—Ü)
                        spectrum = df.iloc[:, 1].values
                        
                        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç NaN
                        spectrum = spectrum[~np.isnan(spectrum)]
                        
                        if len(spectrum) >= 50:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            all_spectra.append(spectrum)
                            all_labels.append(tree_type)
                            
                except Exception as e:
                    print(f"‚ùóÔ∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    continue
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_spectra)} –≤–µ—Å–µ–Ω–Ω–∏—Ö —Å–ø–µ–∫—Ç—Ä–æ–≤")
    return all_spectra, all_labels, tree_types

def preprocess_spectra_adaptive(spectra, labels, target_length=None):
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
    """
    print("\nüîß –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–∞
    if target_length is None:
        lengths = [len(spectrum) for spectrum in spectra]
        target_length = int(np.median(lengths))
        print(f"üìè –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {target_length}")
    
    processed_spectra = []
    processed_labels = []
    
    for i, spectrum in enumerate(spectra):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ø–µ–∫—Ç—Ä—ã
        if len(spectrum) < 30:
            continue
            
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º, –µ—Å–ª–∏ –¥–ª–∏–Ω–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
        if len(spectrum) != target_length:
            processed_spectrum = np.interp(
                np.linspace(0, len(spectrum) - 1, target_length),
                np.arange(len(spectrum)),
                spectrum
            )
        else:
            processed_spectrum = spectrum
            
        processed_spectra.append(processed_spectrum)
        processed_labels.append(labels[i])
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤
    X = np.array(processed_spectra, dtype=np.float32)
    
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(processed_labels)
    
    print(f"üìä –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(np.unique(y))}")
    print(f"üè∑Ô∏è –ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    
    return X, y, label_encoder, target_length

class OptimizedAlexNet1D(nn.Module):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è 1D AlexNet —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    """
    
    def __init__(self, input_length=300, num_classes=7, dropout_rate=0.5, hidden_size=200):
        super(OptimizedAlexNet1D, self).__init__()
        
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (–±–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
        self.conv1 = nn.Conv1d(1, 10, kernel_size=25, stride=4, padding=2)
        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)
        
        self.conv2 = nn.Conv1d(10, 20, kernel_size=15, stride=1, padding=2)
        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2)
        
        self.conv3 = nn.Conv1d(20, 50, kernel_size=2, stride=1, padding=1)
        self.conv4 = nn.Conv1d(50, 50, kernel_size=2, stride=1, padding=1)
        self.conv5 = nn.Conv1d(50, 25, kernel_size=2, stride=1, padding=1)
        
        self.pool3 = nn.MaxPool1d(kernel_size=3, stride=2)
        
        # Batch Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        self.bn1 = nn.BatchNorm1d(10)
        self.bn2 = nn.BatchNorm1d(20)
        self.bn3 = nn.BatchNorm1d(50)
        self.bn4 = nn.BatchNorm1d(50)
        self.bn5 = nn.BatchNorm1d(25)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤
        self._calculate_fc_input_size(input_length)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º —Ä–∞–∑–º–µ—Ä–æ–º
        self.fc1 = nn.Linear(self.fc_input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        
        # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        self.dropout = nn.Dropout(dropout_rate)
        
    def _calculate_fc_input_size(self, input_length):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è"""
        with torch.no_grad():
            dummy_input = torch.randn(1, 1, input_length)
            x = self._conv_forward(dummy_input)
            self.fc_input_size = x.numel()
    
    def _conv_forward(self, x):
        """–ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏"""
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))
        
        x = self.pool3(x)
        
        return x
    
    def forward(self, x):
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        x = self._conv_forward(x)
        
        # Flatten –¥–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
        x = x.view(x.size(0), -1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        
        x = self.fc3(x)
        
        return x

def optimize_hyperparameters(X_train, y_train, X_val, y_val, input_length, num_classes):
    """
    –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
    """
    print("\nüîç –ü–û–î–ë–û–† –û–ü–¢–ò–ú–ê–õ–¨–ù–´–• –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í")
    print("="*60)
    
    # –°–ø–∏—Å–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    optimizers_config = [
        {'name': 'Adam', 'lr': 0.001, 'betas': (0.9, 0.999)},
        {'name': 'AdamW', 'lr': 0.001, 'weight_decay': 0.01},
        {'name': 'RMSprop', 'lr': 0.001, 'momentum': 0.3},
        {'name': 'SGD', 'lr': 0.01, 'momentum': 0.9},
    ]
    
    # –î—Ä—É–≥–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    hidden_sizes = [128, 200, 256]
    dropout_rates = [0.3, 0.5, 0.7]
    
    best_config = None
    best_val_acc = 0.0
    results = []
    
    # –°–æ–∑–¥–∞–µ–º DataLoader
    X_train_tensor = torch.FloatTensor(X_train).unsqueeze(1)
    y_train_tensor = torch.LongTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val).unsqueeze(1)
    y_val_tensor = torch.LongTensor(y_val)
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    total_combinations = len(optimizers_config) * len(hidden_sizes) * len(dropout_rates)
    current_combination = 0
    
    for opt_config in optimizers_config:
        for hidden_size in hidden_sizes:
            for dropout_rate in dropout_rates:
                current_combination += 1
                print(f"\nüîÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {current_combination}/{total_combinations}")
                print(f"   –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {opt_config['name']}")
                print(f"   Hidden Size: {hidden_size}")
                print(f"   Dropout: {dropout_rate}")
                
                # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
                model = OptimizedAlexNet1D(
                    input_length=input_length, 
                    num_classes=num_classes,
                    hidden_size=hidden_size,
                    dropout_rate=dropout_rate
                )
                model.to(device)
                
                # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                if opt_config['name'] == 'Adam':
                    optimizer = optim.Adam(model.parameters(), 
                                         lr=opt_config['lr'], 
                                         betas=opt_config['betas'])
                elif opt_config['name'] == 'AdamW':
                    optimizer = optim.AdamW(model.parameters(), 
                                          lr=opt_config['lr'], 
                                          weight_decay=opt_config['weight_decay'])
                elif opt_config['name'] == 'RMSprop':
                    optimizer = optim.RMSprop(model.parameters(), 
                                            lr=opt_config['lr'], 
                                            momentum=opt_config['momentum'])
                elif opt_config['name'] == 'SGD':
                    optimizer = optim.SGD(model.parameters(), 
                                        lr=opt_config['lr'], 
                                        momentum=opt_config['momentum'])
                
                criterion = nn.CrossEntropyLoss()
                
                # –ö—Ä–∞—Ç–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (50 —ç–ø–æ—Ö)
                val_acc = train_quick_model(model, optimizer, criterion, 
                                          train_loader, val_loader, epochs=50)
                
                config = {
                    'optimizer': opt_config,
                    'hidden_size': hidden_size,
                    'dropout_rate': dropout_rate,
                    'val_accuracy': val_acc
                }
                results.append(config)
                
                print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_acc:.4f}")
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_config = config
                    print(f"   üéØ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è!")
    
    print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø:")
    print(f"   –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {best_config['optimizer']['name']}")
    print(f"   Hidden Size: {best_config['hidden_size']}")
    print(f"   Dropout: {best_config['dropout_rate']}")
    print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}")
    
    return best_config, results

def train_quick_model(model, optimizer, criterion, train_loader, val_loader, epochs=50):
    """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    best_val_acc = 0.0
    
    for epoch in range(epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
        if (epoch + 1) % 10 == 0:
            model.eval()
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for data, target in val_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    _, predicted = torch.max(output.data, 1)
                    val_total += target.size(0)
                    val_correct += (predicted == target).sum().item()
            
            val_acc = val_correct / val_total
            if val_acc > best_val_acc:
                best_val_acc = val_acc
    
    return best_val_acc

def train_final_model(model, train_loader, val_loader, optimizer_config, epochs=200):
    """
    –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    print(f"\nüöÄ –û–ë–£–ß–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò")
    print("="*60)
    print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   - –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {optimizer_config['name']}")
    print(f"   - –≠–ø–æ—Ö–∏: {epochs}")
    print("="*60)
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    if optimizer_config['name'] == 'Adam':
        optimizer = optim.Adam(model.parameters(), 
                             lr=optimizer_config['lr'], 
                             betas=optimizer_config['betas'])
    elif optimizer_config['name'] == 'AdamW':
        optimizer = optim.AdamW(model.parameters(), 
                              lr=optimizer_config['lr'], 
                              weight_decay=optimizer_config['weight_decay'])
    elif optimizer_config['name'] == 'RMSprop':
        optimizer = optim.RMSprop(model.parameters(), 
                                lr=optimizer_config['lr'], 
                                momentum=optimizer_config['momentum'])
    elif optimizer_config['name'] == 'SGD':
        optimizer = optim.SGD(model.parameters(), 
                            lr=optimizer_config['lr'], 
                            momentum=optimizer_config['momentum'])
    
    criterion = nn.CrossEntropyLoss()
    
    train_losses = []
    val_accuracies = []
    best_val_acc = 0.0
    
    for epoch in range(epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()
        
        val_acc = 100 * val_correct / val_total
        avg_train_loss = train_loss / len(train_loader)
        
        train_losses.append(avg_train_loss)
        val_accuracies.append(val_acc)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_spring_alexnet1d_model.pth')
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 25 —ç–ø–æ—Ö
        if (epoch + 1) % 25 == 0:
            print(f"–≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Val Acc: {val_acc:.2f}%, "
                  f"Best Val Acc: {best_val_acc:.2f}%")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    model.load_state_dict(torch.load('best_spring_alexnet1d_model.pth'))
    
    return model, train_losses, val_accuracies, best_val_acc

def comprehensive_noise_testing(model, X_test, y_test, tree_types, noise_levels, n_realizations=1000):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ª–æ–∂–Ω–æ–π —Ç—Ä–µ–≤–æ–≥–∏
    """
    print("\n" + "="*70)
    print("üé≤ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –ì–ê–£–°–°–û–í–°–ö–ò–ú –®–£–ú–û–ú")
    print("üìã –ê–ù–ê–õ–ò–ó –ü–†–ê–í–ò–õ–¨–ù–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ò –õ–û–ñ–ù–û–ô –¢–†–ï–í–û–ì–ò")
    print("="*70)
    
    model.eval()
    results = {}
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ PyTorch —Ç–µ–Ω–∑–æ—Ä—ã
    X_test_tensor = torch.FloatTensor(X_test).unsqueeze(1).to(device)
    
    for noise_level in noise_levels:
        print(f"\nüîä –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {noise_level * 100:.1f}%")
        print("-" * 50)
        
        accuracies = []
        all_confusion_matrices = []
        all_predictions = []
        all_true_labels = []
        
        # 1000 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π —à—É–º–∞
        for realization in range(n_realizations):
            if realization % 100 == 0:
                print(f"  –†–µ–∞–ª–∏–∑–∞—Ü–∏—è {realization + 1}/{n_realizations}...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º
            if noise_level > 0:
                noise = torch.normal(0, noise_level, X_test_tensor.shape).to(device)
                X_test_noisy = X_test_tensor + noise
            else:
                X_test_noisy = X_test_tensor
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = model(X_test_noisy)
                _, predicted = torch.max(outputs, 1)
                predicted = predicted.cpu().numpy()
            
            accuracy = accuracy_score(y_test, predicted)
            accuracies.append(accuracy)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            all_predictions.extend(predicted)
            all_true_labels.extend(y_test)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            cm = confusion_matrix(y_test, predicted, labels=range(len(tree_types)))
            all_confusion_matrices.append(cm)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        mean_accuracy = np.mean(accuracies)
        std_accuracy = np.std(accuracies)
        
        print(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}")
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –ø–æ –≤—Å–µ–º —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º
        mean_confusion_matrix = np.mean(all_confusion_matrices, axis=0)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ª–æ–∂–Ω–æ–π —Ç—Ä–µ–≤–æ–≥–∏
        true_positive_rates = []
        false_alarm_rates = []
        
        for i in range(len(tree_types)):
            # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            tp_rates = []
            # –õ–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å)
            fa_rates = []
            
            for cm in all_confusion_matrices:
                # True Positive Rate (Sensitivity/Recall)
                if cm.sum(axis=1)[i] > 0:
                    tp_rate = cm[i, i] / cm.sum(axis=1)[i]
                    tp_rates.append(tp_rate)
                
                # False Alarm Rate (1 - Specificity)
                if (cm.sum() - cm.sum(axis=1)[i]) > 0:
                    tn = cm.sum() - cm.sum(axis=1)[i] - cm.sum(axis=0)[i] + cm[i, i]
                    fp = cm.sum(axis=0)[i] - cm[i, i]
                    fa_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
                    fa_rates.append(fa_rate)
            
            true_positive_rates.append(tp_rates)
            false_alarm_rates.append(fa_rates)
        
        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
        mean_tp_rates = [np.mean(rates) if rates else 0.0 for rates in true_positive_rates]
        std_tp_rates = [np.std(rates) if rates else 0.0 for rates in true_positive_rates]
        mean_fa_rates = [np.mean(rates) if rates else 0.0 for rates in false_alarm_rates]
        std_fa_rates = [np.std(rates) if rates else 0.0 for rates in false_alarm_rates]
        
        print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º (—à—É–º {noise_level * 100:.1f}%):")
        print(f"\n‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (True Positive Rate):")
        for i, tree in enumerate(tree_types):
            print(f"  {tree}: {mean_tp_rates[i]:.4f} ¬± {std_tp_rates[i]:.4f}")
        
        print(f"\n‚ùå –õ–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞ (False Alarm Rate):")
        for i, tree in enumerate(tree_types):
            print(f"  {tree}: {mean_fa_rates[i]:.4f} ¬± {std_fa_rates[i]:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results[noise_level] = {
            'mean_accuracy': mean_accuracy,
            'std_accuracy': std_accuracy,
            'min_accuracy': np.min(accuracies),
            'max_accuracy': np.max(accuracies),
            'true_positive_rates': mean_tp_rates,
            'false_alarm_rates': mean_fa_rates,
            'tp_std': std_tp_rates,
            'fa_std': std_fa_rates,
            'confusion_matrix': np.round(mean_confusion_matrix).astype(int),
            'all_accuracies': accuracies,
            'mean_confusion_matrix': mean_confusion_matrix
        }
    
    return results

def save_parameters_and_results(best_config, optimization_results, noise_results, 
                               tree_types, best_val_acc, model_params):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ txt —Ñ–∞–π–ª"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –§–∞–π–ª —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    params_filename = f'spring_experiment_parameters_{timestamp}.txt'
    with open(params_filename, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("–ü–ê–†–ê–ú–ï–¢–†–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê - –í–ï–°–ï–ù–ù–ò–ï –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û–ë –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ï:\n")
        f.write(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Random seed: {RANDOM_SEED}\n")
        f.write(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\n\n")
        
        f.write("–î–ê–ù–ù–´–ï:\n")
        f.write("–ò—Å—Ç–æ—á–Ω–∏–∫: –°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤\n")
        f.write(f"–ö–ª–∞—Å—Å—ã: {', '.join(tree_types)}\n")
        f.write("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: 80% –æ–±—É—á–µ–Ω–∏–µ, 20% —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n")
        f.write("–í–∞–ª–∏–¥–∞—Ü–∏—è: –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n\n")
        
        f.write("–û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò:\n")
        f.write(f"–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {best_config['optimizer']['name']}\n")
        for param, value in best_config['optimizer'].items():
            if param != 'name':
                f.write(f"  {param}: {value}\n")
        f.write(f"Hidden Size: {best_config['hidden_size']}\n")
        f.write(f"Dropout Rate: {best_config['dropout_rate']}\n")
        f.write(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_config['val_accuracy']:.4f}\n")
        f.write(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}\n\n")
        
        f.write("–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò:\n")
        f.write("- Conv1d: 10 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel=25, stride=4, padding=2 + BatchNorm\n")
        f.write("- MaxPool1d: kernel=3, stride=2\n")
        f.write("- Conv1d: 20 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel=15, stride=1, padding=2 + BatchNorm\n")
        f.write("- MaxPool1d: kernel=3, stride=2\n")
        f.write("- Conv1d: 50 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel=2, stride=1, padding=1 + BatchNorm\n")
        f.write("- Conv1d: 50 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel=2, stride=1, padding=1 + BatchNorm\n")
        f.write("- Conv1d: 25 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel=2, stride=1, padding=1 + BatchNorm\n")
        f.write("- MaxPool1d: kernel=3, stride=2\n")
        f.write(f"- Linear: {best_config['hidden_size']} –Ω–µ–π—Ä–æ–Ω–æ–≤ + Dropout({best_config['dropout_rate']})\n")
        f.write(f"- Linear: {best_config['hidden_size']} –Ω–µ–π—Ä–æ–Ω–æ–≤ + Dropout({best_config['dropout_rate']})\n")
        f.write("- Linear: 7 –∫–ª–∞—Å—Å–æ–≤ (–≤—ã—Ö–æ–¥)\n\n")
        
        f.write(f"–û–ë–©–ï–ï –ö–û–õ–ò–ß–ï–°–¢–í–û –ü–ê–†–ê–ú–ï–¢–†–û–í: {model_params:,}\n\n")
        
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:\n")
        f.write("–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:\n")
        for i, result in enumerate(optimization_results[:5]):  # –¢–æ–ø-5
            f.write(f"{i+1}. {result['optimizer']['name']}, "
                   f"Hidden={result['hidden_size']}, "
                   f"Dropout={result['dropout_rate']}, "
                   f"Acc={result['val_accuracy']:.4f}\n")
        f.write("\n")
        
        f.write("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –®–£–ú–û–ú:\n")
        f.write("–£—Ä–æ–≤–Ω–∏ —à—É–º–∞: 1%, 5%, 10%\n")
        f.write("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π: 1000\n")
        f.write("–ú–µ—Ç—Ä–∏–∫–∏: —Ç–æ—á–Ω–æ—Å—Ç—å, –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –ª–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞\n\n")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_filename = f'spring_results_analysis_{timestamp}.txt'
    with open(results_filename, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê - –í–ï–°–ï–ù–ù–ò–ï –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï\n")
        f.write("–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ò –õ–û–ñ–ù–ê–Ø –¢–†–ï–í–û–ì–ê\n")
        f.write("=" * 80 + "\n\n")
        
        for noise_level, result in noise_results.items():
            f.write(f"–£–†–û–í–ï–ù–¨ –®–£–ú–ê: {noise_level * 100:.1f}%\n")
            f.write("-" * 50 + "\n")
            f.write(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {result['mean_accuracy']:.4f} ¬± {result['std_accuracy']:.4f}\n")
            f.write(f"–î–∏–∞–ø–∞–∑–æ–Ω —Ç–æ—á–Ω–æ—Å—Ç–∏: {result['min_accuracy']:.4f} - {result['max_accuracy']:.4f}\n\n")
            
            f.write("–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø (True Positive Rate):\n")
            for i, tree in enumerate(tree_types):
                f.write(f"  {tree}: {result['true_positive_rates'][i]:.4f} ¬± {result['tp_std'][i]:.4f}\n")
            f.write("\n")
            
            f.write("–õ–û–ñ–ù–ê–Ø –¢–†–ï–í–û–ì–ê (False Alarm Rate):\n")
            for i, tree in enumerate(tree_types):
                f.write(f"  {tree}: {result['false_alarm_rates'][i]:.4f} ¬± {result['fa_std'][i]:.4f}\n")
            f.write("\n")
            
            f.write("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (—É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–∞—è):\n")
            f.write(str(result['confusion_matrix']) + "\n\n")
        
        f.write("=" * 80 + "\n")
    
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"   - {params_filename} (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)")
    print(f"   - {results_filename} (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)")
    
    return params_filename, results_filename

def plot_comprehensive_analysis(noise_results, tree_types, params_filename):
    """–°—Ç—Ä–æ–∏—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    noise_levels = list(noise_results.keys())
    
    # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à—É—é —Ñ–∏–≥—É—Ä—É —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    fig = plt.figure(figsize=(20, 15))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    plt.subplot(3, 3, 1)
    mean_accuracies = [noise_results[noise]['mean_accuracy'] for noise in noise_levels]
    std_accuracies = [noise_results[noise]['std_accuracy'] for noise in noise_levels]
    plt.errorbar([n*100 for n in noise_levels], mean_accuracies, yerr=std_accuracies, 
                marker='o', capsize=5, capthick=2, linewidth=2, color='blue')
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')
    plt.title('–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
    plt.subplot(3, 3, 2)
    for i, tree in enumerate(tree_types):
        tp_rates = [noise_results[noise]['true_positive_rates'][i] for noise in noise_levels]
        plt.plot([n*100 for n in noise_levels], tp_rates, marker='o', label=tree)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('True Positive Rate')
    plt.title('–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –õ–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
    plt.subplot(3, 3, 3)
    for i, tree in enumerate(tree_types):
        fa_rates = [noise_results[noise]['false_alarm_rates'][i] for noise in noise_levels]
        plt.plot([n*100 for n in noise_levels], fa_rates, marker='s', label=tree)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('False Alarm Rate')
    plt.title('–õ–æ–∂–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –±–µ–∑ —à—É–º–∞
    plt.subplot(3, 3, 4)
    cm_no_noise = noise_results[0.0]['confusion_matrix']
    im = plt.imshow(cm_no_noise, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–±–µ–∑ —à—É–º–∞)')
    plt.colorbar(im)
    tick_marks = np.arange(len(tree_types))
    plt.xticks(tick_marks, tree_types, rotation=45)
    plt.yticks(tick_marks, tree_types)
    
    # –ì—Ä–∞—Ñ–∏–∫ 5: –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —à—É–º–æ–º
    plt.subplot(3, 3, 5)
    max_noise = max(noise_levels)
    cm_max_noise = noise_results[max_noise]['confusion_matrix']
    im = plt.imshow(cm_max_noise, interpolation='nearest', cmap=plt.cm.Reds)
    plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (—à—É–º {max_noise*100}%)')
    plt.colorbar(im)
    plt.xticks(tick_marks, tree_types, rotation=45)
    plt.yticks(tick_marks, tree_types)
    
    # –ì—Ä–∞—Ñ–∏–∫ 6: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ç–æ—á–Ω–æ—Å—Ç–µ–π
    plt.subplot(3, 3, 6)
    accuracies_10 = noise_results[0.1]['all_accuracies']  # 10% —à—É–º–∞
    plt.hist(accuracies_10, bins=50, alpha=0.7, edgecolor='black', color='orange')
    plt.xlabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ (—à—É–º 10%)')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 7: ROC-like –∞–Ω–∞–ª–∏–∑
    plt.subplot(3, 3, 7)
    for i, tree in enumerate(tree_types):
        tp_rates_all = [noise_results[noise]['true_positive_rates'][i] for noise in noise_levels]
        fa_rates_all = [noise_results[noise]['false_alarm_rates'][i] for noise in noise_levels]
        plt.plot(fa_rates_all, tp_rates_all, marker='o', label=tree)
    plt.xlabel('False Alarm Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-–ø–æ–¥–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 8: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    plt.subplot(3, 3, 8)
    x_pos = np.arange(len(tree_types))
    tp_no_noise = [noise_results[0.0]['true_positive_rates'][i] for i in range(len(tree_types))]
    fa_no_noise = [noise_results[0.0]['false_alarm_rates'][i] for i in range(len(tree_types))]
    
    width = 0.35
    plt.bar(x_pos - width/2, tp_no_noise, width, label='True Positive', alpha=0.8)
    plt.bar(x_pos + width/2, fa_no_noise, width, label='False Alarm', alpha=0.8)
    plt.xlabel('–í–∏–¥—ã —Ä–∞—Å—Ç–µ–Ω–∏–π')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title('TP vs FA (–±–µ–∑ —à—É–º–∞)')
    plt.xticks(x_pos, tree_types, rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 9: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ
    plt.subplot(3, 3, 9)
    plt.text(0.1, 0.9, f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:", fontsize=12, fontweight='bold', transform=plt.gca().transAxes)
    plt.text(0.1, 0.8, f"–§–∞–π–ª: {params_filename}", fontsize=10, transform=plt.gca().transAxes)
    plt.text(0.1, 0.7, f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M')}", fontsize=10, transform=plt.gca().transAxes)
    plt.text(0.1, 0.6, f"Random seed: {RANDOM_SEED}", fontsize=10, transform=plt.gca().transAxes)
    plt.text(0.1, 0.5, f"–†–µ–∞–ª–∏–∑–∞—Ü–∏–π —à—É–º–∞: 1000", fontsize=10, transform=plt.gca().transAxes)
    plt.text(0.1, 0.4, f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: 80/20", fontsize=10, transform=plt.gca().transAxes)
    plt.text(0.1, 0.3, f"–ö–ª–∞—Å—Å—ã: {len(tree_types)}", fontsize=10, transform=plt.gca().transAxes)
    plt.axis('off')
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plot_filename = f'spring_comprehensive_analysis_{timestamp}.png'
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_filename}")
    
    return plot_filename

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ—Å–µ–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üå∏ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –í–ï–°–ï–ù–ù–ò–• –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•")
    print("=" * 80)
    print("üéØ –ü–û–î–ë–û–† –ü–ê–†–ê–ú–ï–¢–†–û–í + –ê–ù–ê–õ–ò–ó –õ–û–ñ–ù–û–ô –¢–†–ï–í–û–ì–ò")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    spectra, labels, tree_types = load_spring_spectral_data()

    if len(spectra) == 0:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    X, y, label_encoder, input_length = preprocess_spectra_adaptive(spectra, labels)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 80/20
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    X_train_opt, X_val, y_train_opt, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    print(f"\nüìè –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è): {X_train_opt.shape}")
    print(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_val.shape}")
    print(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    X_train_opt_scaled = scaler.fit_transform(X_train_opt)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
    with open('spring_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    with open('spring_label_encoder.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)

    # –ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    best_config, optimization_results = optimize_hyperparameters(
        X_train_opt_scaled, y_train_opt, X_val_scaled, y_val, 
        input_length, len(tree_types)
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    final_model = OptimizedAlexNet1D(
        input_length=input_length, 
        num_classes=len(tree_types),
        hidden_size=best_config['hidden_size'],
        dropout_rate=best_config['dropout_rate']
    )
    final_model.to(device)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in final_model.parameters())
    print(f"\nüìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    
    # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ)
    X_train_final = scaler.fit_transform(X_train)
    X_train_tensor = torch.FloatTensor(X_train_final).unsqueeze(1)
    y_train_tensor = torch.LongTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_test_scaled).unsqueeze(1)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    y_val_tensor = torch.LongTensor(y_test)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    final_model, train_losses, val_accuracies, best_val_acc = train_final_model(
        final_model, train_loader, val_loader, best_config['optimizer'], epochs=200
    )
    
    # –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º
    noise_levels = [0.0, 0.01, 0.05, 0.1]  # 0%, 1%, 5%, 10%
    
    noise_results = comprehensive_noise_testing(
        final_model, X_test_scaled, y_test, tree_types, noise_levels, n_realizations=1000
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    params_file, results_file = save_parameters_and_results(
        best_config, optimization_results, noise_results, 
        tree_types, best_val_acc, total_params
    )
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plot_filename = plot_comprehensive_analysis(noise_results, tree_types, params_file)
    
    print("\n" + "="*80)
    print("‚úÖ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   - –õ—É—á—à–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {best_config['optimizer']['name']}")
    print(f"   - –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}")
    print(f"   - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print("üìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
    print(f"   - {params_file} (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞)")
    print(f"   - {results_file} (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)")
    print(f"   - {plot_filename} (–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫)")
    print("   - best_spring_alexnet1d_model.pth (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å)")
    print("   - spring_scaler.pkl, spring_label_encoder.pkl (–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞)")
    print("="*80)

if __name__ == "__main__":
    main() 