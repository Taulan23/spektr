#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EXTRA TREES –ê–ù–ê–õ–ò–ó –° 1712 –î–ï–†–ï–í–¨–Ø–ú–ò –ò MAX_DEPTH=NONE
–ü–æ –∑–∞–ø—Ä–æ—Å—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è: —Ä–∞—Å—á–µ—Ç—ã —Å n_estimators=1712 –∏ max_depth=None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import pickle
import os
import glob
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

def load_20_species_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö 20 –≤–∏–¥–æ–≤"""
    
    spring_folder = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    
    print("üå± –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 20 –í–ò–î–û–í...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏
    all_folders = [f for f in os.listdir(spring_folder) 
                   if os.path.isdir(os.path.join(spring_folder, f)) and not f.startswith('.')]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–µ–Ω_–∞–º –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–ø–∫–∏
    if "–∫–ª–µ–Ω_–∞–º" not in all_folders:
        if os.path.exists("–∫–ª–µ–Ω_–∞–º"):
            all_folders.append("–∫–ª–µ–Ω_–∞–º")
    
    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫: {len(all_folders)}")
    
    all_data = []
    all_labels = []
    species_counts = {}
    
    for species in sorted(all_folders):
        if species == "–∫–ª–µ–Ω_–∞–º":
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–ª–µ–Ω_–∞–º
            species_folder = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–ø–∫–µ
            main_folder_path = os.path.join("–∫–ª–µ–Ω_–∞–º", "–∫–ª–µ–Ω_–∞–º")
            if os.path.exists(main_folder_path):
                species_folder = main_folder_path
            elif os.path.exists("–∫–ª–µ–Ω_–∞–º"):
                species_folder = "–∫–ª–µ–Ω_–∞–º"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –ø–∞–ø–∫–µ –≤–µ—Å–µ–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            if species_folder is None:
                spring_path = os.path.join(spring_folder, species)
                if os.path.exists(spring_path):
                    subfolder_path = os.path.join(spring_path, species)
                    if os.path.exists(subfolder_path):
                        species_folder = subfolder_path
                    else:
                        species_folder = spring_path
        else:
            species_folder = os.path.join(spring_folder, species)
        
        if species_folder is None or not os.path.exists(species_folder):
            print(f"   ‚ö†Ô∏è  {species}: –ø–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            continue
            
        files = glob.glob(os.path.join(species_folder, "*.xlsx"))
        
        print(f"   üå≥ {species}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        species_counts[species] = len(files)
        
        species_data = []
        for file in files:
            try:
                df = pd.read_excel(file, header=None)
                spectrum = df.iloc[:, 1].values  # –í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞ - —Å–ø–µ–∫—Ç—Ä
                spectrum = spectrum[~pd.isna(spectrum)]  # –£–±–∏—Ä–∞–µ–º NaN
                species_data.append(spectrum)
            except Exception as e:
                print(f"     ‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {file}: {e}")
                continue
        
        if species_data:
            all_data.extend(species_data)
            all_labels.extend([species] * len(species_data))
    
    print(f"\nüìä –ò–¢–û–ì–û –ó–ê–ì–†–£–ñ–ï–ù–û:")
    for species, count in species_counts.items():
        print(f"   üå≥ {species}: {count} —Å–ø–µ–∫—Ç—Ä–æ–≤")
    
    print(f"\n‚úÖ –û–±—â–∏–π –∏—Ç–æ–≥: {len(all_data)} —Å–ø–µ–∫—Ç—Ä–æ–≤, {len(set(all_labels))} –≤–∏–¥–æ–≤")
    
    return all_data, all_labels, species_counts

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("üîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –°–ü–ï–ö–¢–†–û–í...")
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    min_length = min(len(spectrum) for spectrum in spectra_list)
    print(f"   üìè –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Å–ø–µ–∫—Ç—Ä—ã –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
    processed_spectra = []
    for spectrum in spectra_list:
        truncated = spectrum[:min_length]
        processed_spectra.append(truncated)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
    X = np.array(processed_spectra)
    print(f"   üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    
    return X

def extract_enhanced_features(X):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("‚öôÔ∏è –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í...")
    
    features_list = []
    
    for spectrum in X:
        features = []
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        features.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.min(spectrum),
            np.max(spectrum),
            np.ptp(spectrum),  # peak-to-peak
            np.var(spectrum)
        ])
        
        # –ö–≤–∞–Ω—Ç–∏–ª–∏
        quantiles = np.percentile(spectrum, [10, 25, 75, 90])
        features.extend(quantiles)
        
        # –ú–æ–º–µ–Ω—Ç—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        features.extend([
            np.mean((spectrum - np.mean(spectrum))**3),  # skewness
            np.mean((spectrum - np.mean(spectrum))**4)   # kurtosis
        ])
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
        derivative = np.diff(spectrum)
        features.extend([
            np.mean(derivative),
            np.std(derivative),
            np.max(np.abs(derivative))
        ])
        
        # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        n_bands = 10
        band_size = len(spectrum) // n_bands
        for i in range(n_bands):
            start_idx = i * band_size
            end_idx = min((i + 1) * band_size, len(spectrum))
            if start_idx < len(spectrum):
                band_energy = np.sum(spectrum[start_idx:end_idx] ** 2)
                features.append(band_energy)
            else:
                features.append(0)
        
        # –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏ —Å–ø–µ–∫—Ç—Ä–∞
        mid = len(spectrum) // 2
        first_half = np.mean(spectrum[:mid])
        second_half = np.mean(spectrum[mid:])
        ratio = first_half / second_half if second_half > 0 else 0
        features.append(ratio)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        features.extend([
            np.percentile(spectrum, 5),
            np.percentile(spectrum, 95),
            np.percentile(spectrum, 50) - np.percentile(spectrum, 25),  # IQR
        ])
        
        features_list.append(features)
    
    return np.array(features_list)

def add_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∞–¥–¥–∏—Ç–∏–≤–Ω—ã–π —à—É–º –∫ –¥–∞–Ω–Ω—ã–º"""
    if noise_level == 0:
        return X
    
    # –ê–î–î–ò–¢–ò–í–ù–´–ô –®–£–ú: –∫–∞–∂–¥—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ç—Å—á–µ—Ç –ø–æ–ª—É—á–∞–µ—Ç –°–í–û–ô —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

def train_extra_trees_1712_model(X_train, y_train):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Extra Trees —Å 1712 –¥–µ—Ä–µ–≤—å—è–º–∏ –∏ max_depth=None"""
    
    print("üå≥ –û–ë–£–ß–ï–ù–ò–ï EXTRA TREES –° 1712 –î–ï–†–ï–í–¨–Ø–ú–ò...")
    print("   üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_estimators=1712, max_depth=None")
    
    # –ú–æ–¥–µ–ª—å Extra Trees —Å –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = ExtraTreesClassifier(
        n_estimators=1712,  # –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
        max_depth=None,     # –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    
    print(f"   üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"      - n_estimators: {model.n_estimators}")
    print(f"      - max_depth: {model.max_depth}")
    print(f"      - min_samples_split: {model.min_samples_split}")
    print(f"      - min_samples_leaf: {model.min_samples_leaf}")
    print(f"      - max_features: {model.max_features}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("   üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    model.fit(X_train, y_train)
    
    print("   ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    return model

def evaluate_model_with_noise(model, X_test, y_test, species_names, noise_levels=[0, 1, 5, 10]):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —à—É–º–∞"""
    
    print("üîç –ê–ù–ê–õ–ò–ó –£–°–¢–û–ô–ß–ò–í–û–°–¢–ò –ö –®–£–ú–£...")
    
    results = {}
    confusion_matrices = {}
    
    for noise_level in noise_levels:
        print(f"\n   üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å {noise_level}% —à—É–º–∞...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
        X_test_noisy = add_noise(X_test, noise_level / 100.0)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test_noisy)
        
        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_test, y_pred)
        results[noise_level] = accuracy
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred, labels=range(len(species_names)))
        confusion_matrices[noise_level] = cm
        
        print(f"     üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if noise_level == 0:
            print("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô CLASSIFICATION REPORT (0% —à—É–º–∞):")
            report = classification_report(y_test, y_pred, target_names=species_names, zero_division=0)
            print(report)
    
    return results, confusion_matrices

def create_noise_analysis_visualizations(results, confusion_matrices, species_names, timestamp):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —à—É–º–∞"""
    
    print("üìä –°–û–ó–î–ê–ù–ò–ï –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô...")
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –¢–æ—á–Ω–æ—Å—Ç—å vs —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
    plt.figure(figsize=(20, 15))
    
    # Subplot 1: –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    plt.subplot(2, 3, 1)
    noise_levels = list(results.keys())
    accuracies = list(results.values())
    
    plt.plot(noise_levels, accuracies, 'ro-', linewidth=3, markersize=10, markerfacecolor='red')
    plt.title('Extra Trees (1712 –¥–µ—Ä–µ–≤—å–µ–≤): –¢–æ—á–Ω–æ—Å—Ç—å vs –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞\n20 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤', fontsize=14, fontweight='bold')
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)', fontsize=12)
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    for noise, acc in zip(noise_levels, accuracies):
        plt.annotate(f'{acc:.3f}', (noise, acc), textcoords="offset points", 
                    xytext=(0,10), ha='center', fontsize=10, fontweight='bold')
    
    # Subplot 2: Confusion matrix –¥–ª—è 0% —à—É–º–∞
    plt.subplot(2, 3, 2)
    cm_clean = confusion_matrices[0]
    cm_normalized = cm_clean.astype('float') / cm_clean.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_normalized, 
                xticklabels=species_names, 
                yticklabels=species_names,
                annot=True, 
                fmt='.2f',
                cmap='Reds',
                square=True,
                cbar_kws={'shrink': 0.8})
    
    plt.title('–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è Confusion Matrix\n0% —à—É–º–∞ (1712 –¥–µ—Ä–µ–≤—å–µ–≤)', fontsize=12, fontweight='bold')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
    plt.xticks(rotation=45, ha='right', fontsize=8)
    plt.yticks(rotation=0, fontsize=8)
    
    # Subplot 3: Confusion matrix –¥–ª—è 10% —à—É–º–∞
    plt.subplot(2, 3, 3)
    cm_noisy = confusion_matrices[10]
    cm_normalized_noisy = cm_noisy.astype('float') / cm_noisy.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_normalized_noisy, 
                xticklabels=species_names, 
                yticklabels=species_names,
                annot=True, 
                fmt='.2f',
                cmap='Reds',
                square=True,
                cbar_kws={'shrink': 0.8})
    
    plt.title('–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è Confusion Matrix\n10% —à—É–º–∞ (1712 –¥–µ—Ä–µ–≤—å–µ–≤)', fontsize=12, fontweight='bold')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
    plt.xticks(rotation=45, ha='right', fontsize=8)
    plt.yticks(rotation=0, fontsize=8)
    
    # Subplot 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –≤–∏–¥–∞–º (0% vs 20% —à—É–º–∞)
    plt.subplot(2, 3, 4)
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–∏–¥–∞–º
    cm_clean = confusion_matrices[0]
    cm_noisy = confusion_matrices[10]
    
    accuracy_clean = np.diag(cm_clean) / np.sum(cm_clean, axis=1)
    accuracy_noisy = np.diag(cm_noisy) / np.sum(cm_noisy, axis=1)
    
    x = np.arange(len(species_names))
    width = 0.35
    
    plt.bar(x - width/2, accuracy_clean, width, label='0% —à—É–º–∞', alpha=0.8, color='green')
    plt.bar(x + width/2, accuracy_noisy, width, label='10% —à—É–º–∞', alpha=0.8, color='red')
    
    plt.xlabel('–í–∏–¥—ã –¥–µ—Ä–µ–≤—å–µ–≤', fontsize=10)
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å', fontsize=10)
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–∏–¥–∞–º: 0% vs 10% —à—É–º–∞\n(1712 –¥–µ—Ä–µ–≤—å–µ–≤)', fontsize=12, fontweight='bold')
    plt.xticks(x, species_names, rotation=45, ha='right', fontsize=8)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 5: –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —à—É–º–µ
    plt.subplot(2, 3, 5)
    accuracy_loss = accuracy_clean - accuracy_noisy
    
    plt.bar(x, accuracy_loss, color='orange', alpha=0.8)
    plt.xlabel('–í–∏–¥—ã –¥–µ—Ä–µ–≤—å–µ–≤', fontsize=10)
    plt.ylabel('–ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏', fontsize=10)
    plt.title('–ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ 10% —à—É–º–µ\n(1712 –¥–µ—Ä–µ–≤—å–µ–≤)', fontsize=12, fontweight='bold')
    plt.xticks(x, species_names, rotation=45, ha='right', fontsize=8)
    plt.grid(True, alpha=0.3)
    
    # Subplot 6: –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    plt.subplot(2, 3, 6)
    plt.axis('off')
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats_text = f"""
    üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–û–î–ï–õ–ò (1712 –¥–µ—Ä–µ–≤—å–µ–≤)
    
    üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:
       ‚Ä¢ 0% —à—É–º–∞: {results[0]:.3f} ({results[0]*100:.1f}%)
       ‚Ä¢ 1% —à—É–º–∞: {results[1]:.3f} ({results[1]*100:.1f}%)
       ‚Ä¢ 5% —à—É–º–∞: {results[5]:.3f} ({results[5]*100:.1f}%)
               ‚Ä¢ 10% —à—É–º–∞: {results[10]:.3f} ({results[10]*100:.1f}%)
    
    üìâ –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏:
       ‚Ä¢ 0% ‚Üí 10%: {results[0] - results[10]:.3f} ({(results[0] - results[10])*100:.1f}%)
    
    üå≥ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:
       ‚Ä¢ n_estimators: 1712
       ‚Ä¢ max_depth: None
       ‚Ä¢ min_samples_split: 5
       ‚Ä¢ min_samples_leaf: 2
       ‚Ä¢ max_features: sqrt
    """
    
    plt.text(0.1, 0.9, stats_text, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    filename = f'extra_trees_1712_noise_analysis_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"   üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
    plt.show()

def create_individual_confusion_matrices(confusion_matrices, species_names, timestamp):
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ confusion matrices –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —à—É–º–∞"""
    
    print("üìã –°–û–ó–î–ê–ù–ò–ï –û–¢–î–ï–õ–¨–ù–´–• CONFUSION MATRICES...")
    
    for noise_level in [0, 1, 5, 10]:
        plt.figure(figsize=(12, 10))
        
        cm = confusion_matrices[noise_level]
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(cm_normalized, 
                    xticklabels=species_names, 
                    yticklabels=species_names,
                    annot=True, 
                    fmt='.2f',
                    cmap='Reds',
                    square=True,
                    cbar_kws={'shrink': 0.8})
        
        plt.title(f'Extra Trees (1712 –¥–µ—Ä–µ–≤—å–µ–≤): –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è Confusion Matrix\n{noise_level}% —à—É–º–∞', 
                  fontsize=14, fontweight='bold')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
        plt.xticks(rotation=45, ha='right', fontsize=10)
        plt.yticks(rotation=0, fontsize=10)
        
        filename = f'extra_trees_1712_confusion_matrix_{noise_level}percent_{timestamp}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"   üìä Confusion matrix {noise_level}% —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")
        plt.close()

def save_results(model, scaler, label_encoder, results, confusion_matrices, species_names, timestamp):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
    
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_filename = f'extra_trees_1712_model_{timestamp}.pkl'
    with open(model_filename, 'wb') as f:
        pickle.dump(model, f)
    print(f"   üß† –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    scaler_filename = f'extra_trees_1712_scaler_{timestamp}.pkl'
    with open(scaler_filename, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"   üîß Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_filename}")
    
    label_encoder_filename = f'extra_trees_1712_label_encoder_{timestamp}.pkl'
    with open(label_encoder_filename, 'wb') as f:
        pickle.dump(label_encoder, f)
    print(f"   üè∑Ô∏è Label encoder —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {label_encoder_filename}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    results_filename = f'extra_trees_1712_results_{timestamp}.txt'
    with open(results_filename, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ EXTRA TREES –° 1712 –î–ï–†–ï–í–¨–Ø–ú–ò –ò MAX_DEPTH=NONE\n")
        f.write("="*80 + "\n\n")
        
        f.write("üìã –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò:\n")
        f.write(f"   ‚Ä¢ n_estimators: {model.n_estimators}\n")
        f.write(f"   ‚Ä¢ max_depth: {model.max_depth}\n")
        f.write(f"   ‚Ä¢ min_samples_split: {model.min_samples_split}\n")
        f.write(f"   ‚Ä¢ min_samples_leaf: {model.min_samples_leaf}\n")
        f.write(f"   ‚Ä¢ max_features: {model.max_features}\n\n")
        
        f.write("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –£–†–û–í–ù–Ø–ú –®–£–ú–ê:\n")
        for noise_level in sorted(results.keys()):
            accuracy = results[noise_level]
            f.write(f"   ‚Ä¢ {noise_level}% —à—É–º–∞: {accuracy:.4f} ({accuracy*100:.2f}%)\n")
        
        f.write(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
        f.write(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(results.values()):.4f}\n")
        f.write(f"   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {min(results.values()):.4f}\n")
        f.write(f"   ‚Ä¢ –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (0% ‚Üí 10%): {results[0] - results[10]:.4f}\n")
        f.write(f"   ‚Ä¢ –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è: {((results[0] - results[10]) / results[0] * 100):.2f}%\n")
        
        f.write(f"\nüå≥ –ö–û–õ–ò–ß–ï–°–¢–í–û –í–ò–î–û–í: {len(species_names)}\n")
        f.write(f"üìä –†–ê–ó–ú–ï–† –î–ê–ù–ù–´–•: {model.n_features_in_} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n")
        
    print(f"   üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_filename}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("="*80)
    print("üå≥ EXTRA TREES –ê–ù–ê–õ–ò–ó –° 1712 –î–ï–†–ï–í–¨–Ø–ú–ò –ò MAX_DEPTH=NONE")
    print("="*80)
    print("üìã –ü–æ –∑–∞–ø—Ä–æ—Å—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è")
    print("="*80)
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    all_data, all_labels, species_counts = load_20_species_data()
    
    if len(all_data) == 0:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤
    X = preprocess_spectra(all_data)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_features = extract_enhanced_features(X)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(all_labels)
    species_names = label_encoder.classes_
    
    print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –§–û–†–ú–ê –î–ê–ù–ù–´–•:")
    print(f"   ‚Ä¢ X: {X_features.shape}")
    print(f"   ‚Ä¢ y: {y.shape}")
    print(f"   ‚Ä¢ –ö–ª–∞—Å—Å—ã: {len(species_names)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìè –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"   ‚Ä¢ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"   ‚Ä¢ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å 1712 –¥–µ—Ä–µ–≤—å—è–º–∏
    model = train_extra_trees_1712_model(X_train_scaled, y_train)
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —à—É–º–æ–º
    results, confusion_matrices = evaluate_model_with_noise(
        model, X_test_scaled, y_test, species_names
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
    create_noise_analysis_visualizations(results, confusion_matrices, species_names, timestamp)
    create_individual_confusion_matrices(confusion_matrices, species_names, timestamp)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_results(model, scaler, label_encoder, results, confusion_matrices, species_names, timestamp)
    
    print("\n" + "="*80)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å 1712 –¥–µ—Ä–µ–≤—å—è–º–∏ –∏ max_depth=None –≥–æ—Ç–æ–≤—ã")
    print("üìä –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π:", timestamp)
    print("="*80)

if __name__ == "__main__":
    main() 