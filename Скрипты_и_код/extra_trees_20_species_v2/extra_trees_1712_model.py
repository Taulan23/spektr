
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Extra Trees –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ 20 –≤–µ—Å–µ–Ω–Ω–∏—Ö –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤
1712 –¥–µ—Ä–µ–≤—å–µ–≤, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ None
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import glob
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def load_20_species_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö 20 –≤–∏–¥–æ–≤"""
    
    spring_folder = "../–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    
    print("üå± –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 20 –í–ò–î–û–í...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏
    all_folders = [f for f in os.listdir(spring_folder) 
                   if os.path.isdir(os.path.join(spring_folder, f)) and not f.startswith('.')]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–µ–Ω_–∞–º –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–ø–∫–∏
    if "–∫–ª–µ–Ω_–∞–º" not in all_folders:
        if os.path.exists("–∫–ª–µ–Ω_–∞–º"):
            all_folders.append("–∫–ª–µ–Ω_–∞–º")
    
    print(f"   üìÅ –ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫: {len(all_folders)}")
    
    all_data = []
    all_labels = []
    species_counts = {}
    
    for species in sorted(all_folders):
        if species == "–∫–ª–µ–Ω_–∞–º":
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–ª–µ–Ω_–∞–º (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥–≤—É—Ö –º–µ—Å—Ç–∞—Ö)
            species_folder = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–ø–∫–µ
            main_folder_path = os.path.join("../–∫–ª–µ–Ω_–∞–º", "–∫–ª–µ–Ω_–∞–º")
            if os.path.exists(main_folder_path):
                species_folder = main_folder_path
            elif os.path.exists("../–∫–ª–µ–Ω_–∞–º"):
                species_folder = "../–∫–ª–µ–Ω_–∞–º"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –ø–∞–ø–∫–µ –≤–µ—Å–µ–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            if species_folder is None:
                spring_path = os.path.join(spring_folder, species)
                if os.path.exists(spring_path):
                    subfolder_path = os.path.join(spring_path, species)
                    if os.path.exists(subfolder_path):
                        species_folder = subfolder_path
                    else:
                        species_folder = spring_path
        else:
            species_folder = os.path.join(spring_folder, species)
        
        if species_folder is None or not os.path.exists(species_folder):
            print(f"   ‚ö†Ô∏è  {species}: –ø–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            continue
            
        files = glob.glob(os.path.join(species_folder, "*.xlsx"))
        
        print(f"   üå≥ {species}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        species_counts[species] = len(files)
        
        species_data = []
        for file in files:
            try:
                df = pd.read_excel(file, header=None)
                spectrum = df.iloc[:, 1].values  # –í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞ - —Å–ø–µ–∫—Ç—Ä
                spectrum = spectrum[~pd.isna(spectrum)]  # –£–±–∏—Ä–∞–µ–º NaN
                species_data.append(spectrum)
            except Exception as e:
                print(f"     ‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {file}: {e}")
                continue
        
        if species_data:
            all_data.extend(species_data)
            all_labels.extend([species] * len(species_data))
    
    print(f"\nüìä –ò–¢–û–ì–û –ó–ê–ì–†–£–ñ–ï–ù–û:")
    for species, count in species_counts.items():
        print(f"   üå≥ {species}: {count} —Å–ø–µ–∫—Ç—Ä–æ–≤")
    
    print(f"\n‚úÖ –û–±—â–∏–π –∏—Ç–æ–≥: {len(all_data)} —Å–ø–µ–∫—Ç—Ä–æ–≤, {len(set(all_labels))} –≤–∏–¥–æ–≤")
    
    return all_data, all_labels, species_counts

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("üîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –°–ü–ï–ö–¢–†–û–í...")
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    min_length = min(len(spectrum) for spectrum in spectra_list)
    print(f"   üìè –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Å–ø–µ–∫—Ç—Ä—ã –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
    processed_spectra = []
    for spectrum in spectra_list:
        truncated = spectrum[:min_length]
        processed_spectra.append(truncated)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
    X = np.array(processed_spectra)
    print(f"   üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    
    return X

def extract_enhanced_features(X):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("‚öôÔ∏è –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í...")
    
    features_list = []
    
    for spectrum in X:
        features = []
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        features.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.min(spectrum),
            np.max(spectrum),
            np.ptp(spectrum),  # peak-to-peak
            np.var(spectrum)
        ])
        
        # –ö–≤–∞–Ω—Ç–∏–ª–∏
        quantiles = np.percentile(spectrum, [10, 25, 75, 90])
        features.extend(quantiles)
        
        # –ú–æ–º–µ–Ω—Ç—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        features.extend([
            np.sum(spectrum),
            np.sum(spectrum**2),
            np.sum(spectrum**3),
            np.sum(spectrum**4)
        ])
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if len(spectrum) > 1:
            diff1 = np.diff(spectrum)
            diff2 = np.diff(diff1) if len(diff1) > 1 else [0]
            
            features.extend([
                np.mean(diff1),
                np.std(diff1),
                np.mean(diff2) if len(diff2) > 0 else 0,
                np.std(diff2) if len(diff2) > 0 else 0
            ])
        else:
            features.extend([0, 0, 0, 0])
        
        # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if len(spectrum) > 10:
            # –†–∞–∑–¥–µ–ª—è–µ–º —Å–ø–µ–∫—Ç—Ä –Ω–∞ —á–∞—Å—Ç–∏
            n_parts = 5
            part_size = len(spectrum) // n_parts
            
            for i in range(n_parts):
                start_idx = i * part_size
                end_idx = start_idx + part_size if i < n_parts - 1 else len(spectrum)
                part = spectrum[start_idx:end_idx]
                
                features.extend([
                    np.mean(part),
                    np.std(part),
                    np.max(part),
                    np.min(part)
                ])
        else:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ —Å–ø–µ–∫—Ç—Ä —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
            features.extend([0] * (5 * 4))
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features.extend([
            np.sum(spectrum > np.mean(spectrum)),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
            np.sum(spectrum < np.mean(spectrum)),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –Ω–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
            len(spectrum),  # –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞
            np.argmax(spectrum),  # –∏–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º—É–º–∞
            np.argmin(spectrum),  # –∏–Ω–¥–µ–∫—Å –º–∏–Ω–∏–º—É–º–∞
        ])
        
        features_list.append(features)
    
    return np.array(features_list)

def add_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º –∫ –¥–∞–Ω–Ω—ã–º"""
    if noise_level == 0:
        return X
    noise = np.random.normal(0, noise_level, X.shape).astype(np.float32)
    return X + noise

def train_extra_trees_model(X_train, y_train):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Extra Trees —Å 1712 –¥–µ—Ä–µ–≤—å—è–º–∏"""
    
    print("üå≥ –û–ë–£–ß–ï–ù–ò–ï EXTRA TREES –ú–û–î–ï–õ–ò...")
    
    model = ExtraTreesClassifier(
        n_estimators=1712,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å {model.n_estimators} –¥–µ—Ä–µ–≤—å—è–º–∏")
    
    return model

def evaluate_with_noise(model, X_test, y_test, species_names, noise_levels=[1, 5, 10]):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞"""
    
    print(f"\nüîä –û–¶–ï–ù–ö–ê –° –®–£–ú–û–ú...")
    
    results = {}
    confusion_matrices = {}
    
    for noise_level in noise_levels:
        print(f"\n{'='*60}")
        print(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞: {noise_level}%")
        print(f"{'='*60}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        X_test_noisy = add_noise(X_test, noise_level / 100.0)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        y_pred = model.predict(X_test_noisy)
        y_pred_proba = model.predict_proba(X_test_noisy)
        
        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_test, y_pred)
        results[noise_level] = accuracy
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ {noise_level}% —à—É–º–µ: {accuracy:.7f}")
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)
        confusion_matrices[noise_level] = cm
        
        print(f"–û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_test, y_pred, target_names=species_names, digits=7))
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=species_names, yticklabels=species_names)
        plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {noise_level}% —à—É–º–∞')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f'confusion_matrix_{noise_level}percent.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º = 1)
        cm_normalized = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]
        cm_normalized = np.nan_to_num(cm_normalized)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0
        
        print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (—Å—É–º–º–∞ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º = 1):")
        print(cm_normalized)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        np.save(f'confusion_matrix_{noise_level}percent_normalized.npy', cm_normalized)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm_normalized, annot=True, fmt='.7f', cmap='Blues', 
                   xticklabels=species_names, yticklabels=species_names)
        plt.title(f'–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {noise_level}% —à—É–º–∞')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f'confusion_matrix_{noise_level}percent_normalized.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    return results, confusion_matrices

def create_detailed_excel_analysis(model, X_test, y_test, species_names, timestamp):
    """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π Excel –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ—Å–∏–Ω—ã –∏ —Å–∏—Ä–µ–Ω–∏"""
    
    print("\nüìä –°–û–ó–î–ê–ù–ò–ï –î–ï–¢–ê–õ–¨–ù–û–ì–û EXCEL –ê–ù–ê–õ–ò–ó–ê...")
    
    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –æ—Å–∏–Ω—ã –∏ —Å–∏—Ä–µ–Ω–∏
    –æ—Å–∏–Ω–∞_idx = np.where(species_names == '–æ—Å–∏–Ω–∞')[0][0]
    —Å–∏—Ä–µ–Ω—å_idx = np.where(species_names == '—Å–∏—Ä–µ–Ω—å')[0][0]
    
    target_species = ['–æ—Å–∏–Ω–∞', '—Å–∏—Ä–µ–Ω—å']
    target_indices = [–æ—Å–∏–Ω–∞_idx, —Å–∏—Ä–µ–Ω—å_idx]
    
    # –°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª
    with pd.ExcelWriter(f'detailed_analysis_{timestamp}.xlsx', engine='openpyxl') as writer:
        
        for species_name, target_idx in zip(target_species, target_indices):
            print(f"   üìä –ê–Ω–∞–ª–∏–∑ –¥–ª—è {species_name}...")
            
            # –ù–∞—Ö–æ–¥–∏–º –æ–±—Ä–∞–∑—Ü—ã —Ü–µ–ª–µ–≤–æ–≥–æ –≤–∏–¥–∞
            target_samples_mask = (y_test == target_idx)
            target_samples = X_test[target_samples_mask]
            
            if len(target_samples) == 0:
                print(f"     ‚ö†Ô∏è –ù–µ—Ç –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {species_name}")
                continue
            
            print(f"     üìà –ù–∞–π–¥–µ–Ω–æ {len(target_samples)} –æ–±—Ä–∞–∑—Ü–æ–≤ {species_name}")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º 10% —à—É–º
            noise = np.random.normal(0, 0.1, target_samples.shape).astype(np.float32)
            target_samples_noisy = target_samples + noise
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
            probabilities = model.predict_proba(target_samples_noisy)
            
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
            prob_df = pd.DataFrame(probabilities, columns=species_names)
            prob_df.insert(0, '–û–±—Ä–∞–∑–µ—Ü', range(1, len(prob_df) + 1))
            
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ (1 –¥–ª—è –º–∞–∫—Å–∏–º—É–º–∞, 0 –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö)
            max_prob_matrix = np.zeros_like(probabilities)
            max_indices = np.argmax(probabilities, axis=1)
            max_prob_matrix[np.arange(len(probabilities)), max_indices] = 1
            
            # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            max_prob_df = pd.DataFrame(max_prob_matrix, columns=species_names)
            max_prob_df.insert(0, '–û–±—Ä–∞–∑–µ—Ü', range(1, len(max_prob_df) + 1))
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            mean_probs = prob_df.iloc[:, 1:].mean()
            mean_max_probs = max_prob_df.iloc[:, 1:].mean()
            
            # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            mean_df = pd.DataFrame({
                '–í–∏–¥ –¥–µ—Ä–µ–≤–∞': species_names,
                '–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å': mean_probs.values,
                '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (1/0)': mean_max_probs.values
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
            prob_df.to_excel(writer, sheet_name=f'{species_name}_–¥–µ—Ç–∞–ª—å–Ω—ã–µ_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏', index=False)
            max_prob_df.to_excel(writer, sheet_name=f'{species_name}_–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏', index=False)
            mean_df.to_excel(writer, sheet_name=f'{species_name}_—Å—Ä–µ–¥–Ω–∏–µ_–∑–Ω–∞—á–µ–Ω–∏—è', index=False)
            
            print(f"     ‚úÖ –î–∞–Ω–Ω—ã–µ –¥–ª—è {species_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Excel")
    
    print(f"üíæ Excel —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: detailed_analysis_{timestamp}.xlsx")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üå≥" * 60)
    print("üå≥ EXTRA TREES –î–õ–Ø 20 –í–ï–°–ï–ù–ù–ò–• –í–ò–î–û–í")
    print("üå≥ 1712 –î–ï–†–ï–í–¨–ï–í, –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ì–õ–£–ë–ò–ù–ê NONE")
    print("üå≥" * 60)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    spectra_list, labels, species_counts = load_20_species_data()
    
    if len(spectra_list) == 0:
        print("‚ùå –û—à–∏–±–∫–∞: –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤
    X_spectra = preprocess_spectra(spectra_list)
    
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_features = extract_enhanced_features(X_spectra)
    
    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    species_names = label_encoder.classes_
    
    print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï:")
    print(f"   üî¢ –§–æ—Ä–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_features.shape}")
    print(f"   üè∑Ô∏è  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(species_names)}")
    print(f"   üìã –í–∏–¥—ã: {list(species_names)}")
    
    # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (80/20)
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"\n‚úÇÔ∏è –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/TEST:")
    print(f"   üìä Train: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   üìä Test: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n‚öñÔ∏è –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 7. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = train_extra_trees_model(X_train_scaled, y_train)
    
    # 8. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –û–¶–ï–ù–ö–ê –ù–ê –ß–ò–°–¢–´–• –î–ê–ù–ù–´–•...")
    y_pred_clean = model.predict(X_test_scaled)
    clean_accuracy = accuracy_score(y_test, y_pred_clean)
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {clean_accuracy:.7f}")
    
    # 9. –û—Ü–µ–Ω–∫–∞ —Å —à—É–º–æ–º
    results, confusion_matrices = evaluate_with_noise(
        model, X_test_scaled, y_test, species_names, noise_levels=[1, 5, 10]
    )
    
    # 10. –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ Excel –∞–Ω–∞–ª–∏–∑–∞
    create_detailed_excel_analysis(model, X_test_scaled, y_test, species_names, timestamp)
    
    # 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_filename = f'extra_trees_1712_model_{timestamp}.pkl'
    joblib.dump(model, model_filename)
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
    
    # 12. –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ EXTRA TREES (1712 –¥–µ—Ä–µ–≤–∞):")
    print(f"   üìä –ß–∏—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ: {clean_accuracy:.7f}")
    for noise_level, accuracy in results.items():
        print(f"   üìä {noise_level}% —à—É–º–∞: {accuracy:.7f}")
    
    print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print(f"üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"   üå≥ –ú–æ–¥–µ–ª—å: {model_filename}")
    print(f"   üìä Excel –∞–Ω–∞–ª–∏–∑: detailed_analysis_{timestamp}.xlsx")
    print(f"   üìä –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫:")
    for noise_level in [1, 5, 10]:
        print(f"     üìä {noise_level}% —à—É–º–∞: confusion_matrix_{noise_level}percent.png")
        print(f"     üìä {noise_level}% —à—É–º–∞ (–Ω–æ—Ä–º.): confusion_matrix_{noise_level}percent_normalized.png")

if __name__ == "__main__":
    main() 