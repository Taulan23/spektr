import os
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, PowerTransformer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.calibration import CalibratedClassifierCV
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.combine import SMOTEENN, SMOTETomek
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
warnings.filterwarnings('ignore')

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ–º—è–Ω –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

def load_spring_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join(base_path, tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(excel_files)} –≤–µ—Å–µ–Ω–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type}")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def load_summer_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –≤–∫–ª—é—á–µ–Ω–∏–µ–º –∫–ª–µ–Ω_–∞–º"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
        folder_path = os.path.join('.', tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
        
        # –î–ª—è –∫–ª–µ–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫–ª–µ–Ω_–∞–º
        if tree_type == '–∫–ª–µ–Ω':
            am_folder = "–∫–ª–µ–Ω_–∞–º"
            if os.path.exists(am_folder):
                am_files = glob.glob(os.path.join(am_folder, '*.xlsx'))
                print(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(am_files)} —Ñ–∞–π–ª–æ–≤ –∫–ª–µ–Ω–∞ –∏–∑ –∫–ª–µ–Ω_–∞–º")
                
                for file_path in am_files:
                    try:
                        df = pd.read_excel(file_path)
                        if df.shape[1] >= 2:
                            spectrum_data = df.iloc[:, 1].values
                            if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                                spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                                if len(spectrum_data) > 10:
                                    all_data.append(spectrum_data)
                                    all_labels.append(tree_type)
                    except Exception:
                        continue
    
    return all_data, all_labels

def extract_super_features(spectra):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π"""
    features = []
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
    species_channels = {
        '–∫–ª–µ–Ω': [
            list(range(170, 190)),  # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            list(range(240, 290)),  # –í—Ç–æ—Ä–∏—á–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            [179, 180, 181, 258, 276, 286, 172, 173, 174, 175, 176, 177, 178]  # –ö–ª—é—á–µ–≤—ã–µ –ø–∏–∫–∏
        ],
        '–¥—É–±': [
            list(range(145, 170)),  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            list(range(200, 220)),  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            [151, 152, 153, 154, 155, 156, 157, 158, 159, 160]  # –ö–ª—é—á–µ–≤—ã–µ –∫–∞–Ω–∞–ª—ã
        ],
        '–±–µ—Ä–µ–∑–∞': [list(range(100, 140)), list(range(260, 300))],
        '–µ–ª—å': [list(range(120, 160)), list(range(220, 260)), list(range(50, 90))],
        '–ª–∏–ø–∞': [list(range(80, 120)), list(range(180, 220)), list(range(280, 300))],
        '–æ—Å–∏–Ω–∞': [list(range(60, 100)), list(range(140, 180)), list(range(200, 240))],
        '—Å–æ—Å–Ω–∞': [list(range(90, 130)), list(range(160, 200)), list(range(270, 300))]
    }
    
    for spectrum in spectra:
        spectrum = np.array(spectrum)
        feature_vector = []
        
        # 1. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –±–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        feature_vector.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.max(spectrum),
            np.min(spectrum),
            np.ptp(spectrum),
            np.var(spectrum),
            np.sqrt(np.mean(spectrum**2)),  # RMS
            np.mean(np.abs(spectrum)),      # MAD
            np.sum(spectrum),
            np.prod(np.sign(spectrum) * (np.abs(spectrum) + 1e-12))**(1/len(spectrum)),  # –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
            len(spectrum[spectrum > np.mean(spectrum)]) / len(spectrum),  # –î–æ–ª—è –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
        ])
        
        # 2. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
        for p in range(5, 100, 5):  # –û—Ç 5% –¥–æ 95% —Å —à–∞–≥–æ–º 5%
            feature_vector.append(np.percentile(spectrum, p))
        
        # 3. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤—Å–µ—Ö –ø–æ—Ä—è–¥–∫–æ–≤ –¥–æ 4-–≥–æ
        current_spectrum = spectrum.copy()
        for order in range(1, 5):
            if len(current_spectrum) > 1:
                derivative = np.diff(current_spectrum)
                if len(derivative) > 0:
                    feature_vector.extend([
                        np.mean(derivative),
                        np.std(derivative),
                        np.max(derivative),
                        np.min(derivative),
                        np.max(np.abs(derivative)),
                        np.sum(derivative > 0) / len(derivative) if len(derivative) > 0 else 0,
                        np.trapz(np.abs(derivative)) if len(derivative) > 1 else 0,
                    ])
                    current_spectrum = derivative
                else:
                    feature_vector.extend([0] * 7)
            else:
                feature_vector.extend([0] * 7)
        
        # 4. –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
        for species, channel_groups in species_channels.items():
            for group_idx, channels in enumerate(channel_groups):
                valid_channels = [ch for ch in channels if ch < len(spectrum)]
                if valid_channels:
                    region = spectrum[valid_channels]
                    avg_spectrum = np.mean(spectrum)
                    feature_vector.extend([
                        np.mean(region),
                        np.std(region),
                        np.max(region),
                        np.min(region),
                        np.median(region),
                        np.ptp(region),
                        np.var(region),
                        np.mean(region) / avg_spectrum if avg_spectrum > 0 else 0,
                        np.sum(region > np.percentile(spectrum, 75)),
                        np.sum(region < np.percentile(spectrum, 25)),
                        np.trapz(region) if len(region) > 1 else 0,
                        np.corrcoef(region, np.arange(len(region)))[0,1] if len(region) > 1 else 0,
                    ])
                else:
                    feature_vector.extend([0] * 12)
        
        # 5. –§—É—Ä—å–µ-–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (—á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)
        try:
            fft = np.fft.fft(spectrum)
            fft_real = np.real(fft)[:len(spectrum)//2]
            fft_imag = np.imag(fft)[:len(spectrum)//2]
            fft_mag = np.abs(fft)[:len(spectrum)//2]
            
            feature_vector.extend([
                np.mean(fft_mag),
                np.std(fft_mag),
                np.max(fft_mag),
                np.argmax(fft_mag),  # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç–æ—Ç–∞
                np.sum(fft_mag[:len(fft_mag)//4]),  # –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
                np.sum(fft_mag[len(fft_mag)//4:3*len(fft_mag)//4]),  # –°—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã
                np.sum(fft_mag[3*len(fft_mag)//4:]),  # –í—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
            ])
        except:
            feature_vector.extend([0] * 7)
        
        # 6. –≠–Ω–µ—Ä–≥–∏—è –≤ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
        n_bands = 15
        band_size = len(spectrum) // n_bands
        band_energies = []
        for i in range(n_bands):
            start_idx = i * band_size
            end_idx = min((i + 1) * band_size, len(spectrum))
            if start_idx < len(spectrum):
                band = spectrum[start_idx:end_idx]
                energy = np.sum(band ** 2)
                band_energies.append(energy)
            else:
                band_energies.append(0)
        
        feature_vector.extend(band_energies)
        
        # –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏
        for i in range(min(5, len(band_energies))):
            for j in range(i+1, min(5, len(band_energies))):
                if band_energies[j] > 0:
                    feature_vector.append(band_energies[i] / band_energies[j])
                else:
                    feature_vector.append(0)
        
        # 7. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤—ã—Å—à–∏—Ö –ø–æ—Ä—è–¥–∫–æ–≤
        normalized_spectrum = spectrum / np.sum(spectrum) if np.sum(spectrum) > 0 else spectrum
        channels = np.arange(len(spectrum))
        if np.sum(normalized_spectrum) > 0:
            centroid = np.sum(channels * normalized_spectrum)
            spread = np.sqrt(np.sum(((channels - centroid) ** 2) * normalized_spectrum))
            skewness = np.sum(((channels - centroid) ** 3) * normalized_spectrum) / (spread ** 3) if spread > 0 else 0
            kurtosis = np.sum(((channels - centroid) ** 4) * normalized_spectrum) / (spread ** 4) if spread > 0 else 0
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
            fifth_moment = np.sum(((channels - centroid) ** 5) * normalized_spectrum) / (spread ** 5) if spread > 0 else 0
            sixth_moment = np.sum(((channels - centroid) ** 6) * normalized_spectrum) / (spread ** 6) if spread > 0 else 0
            
            feature_vector.extend([centroid, spread, skewness, kurtosis, fifth_moment, sixth_moment])
        else:
            feature_vector.extend([0] * 6)
        
        # 8. –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å—Ç—Ä–µ–º—É–º—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        try:
            from scipy.signal import find_peaks, argrelextrema
            
            # –ü–æ–∏—Å–∫ –ø–∏–∫–æ–≤ –∏ –¥–æ–ª–∏–Ω
            peaks, _ = find_peaks(spectrum, height=np.percentile(spectrum, 50))
            valleys, _ = find_peaks(-spectrum, height=-np.percentile(spectrum, 50))
            
            # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã –∏ –º–∏–Ω–∏–º—É–º—ã
            local_max = argrelextrema(spectrum, np.greater, order=3)[0]
            local_min = argrelextrema(spectrum, np.less, order=3)[0]
            
            feature_vector.extend([
                len(peaks),
                len(valleys),
                len(local_max),
                len(local_min),
                np.mean(spectrum[peaks]) if len(peaks) > 0 else 0,
                np.mean(spectrum[valleys]) if len(valleys) > 0 else 0,
                np.std(spectrum[peaks]) if len(peaks) > 0 else 0,
                np.std(spectrum[valleys]) if len(valleys) > 0 else 0,
                len(peaks) / len(spectrum) if len(spectrum) > 0 else 0,  # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –ø–∏–∫–æ–≤
                len(valleys) / len(spectrum) if len(spectrum) > 0 else 0,  # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–∏–Ω
            ])
        except:
            feature_vector.extend([0] * 10)
        
        # 9. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        feature_vector.extend([
            np.sum(spectrum > 0) / len(spectrum) if len(spectrum) > 0 else 0,  # –î–æ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
            np.sum(spectrum < 0) / len(spectrum) if len(spectrum) > 0 else 0,  # –î–æ–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö
            len(np.where(np.diff(spectrum) > 0)[0]) / len(spectrum) if len(spectrum) > 1 else 0,  # –î–æ–ª—è –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–π
            len(np.where(np.diff(spectrum) < 0)[0]) / len(spectrum) if len(spectrum) > 1 else 0,  # –î–æ–ª—è —É–±—ã–≤–∞–Ω–∏–π
            np.sum(np.abs(spectrum - np.mean(spectrum))) / len(spectrum),  # –°—Ä–µ–¥–Ω–µ–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
        ])
        
        features.append(feature_vector)
    
    return np.array(features)

def augment_data(X, y, class_names, target_samples=200):
    """–ê—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
    print("üîÑ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
    
    augmented_X = []
    augmented_y = []
    
    for class_idx, class_name in enumerate(class_names):
        class_mask = y == class_idx
        class_data = X[class_mask]
        current_samples = len(class_data)
        
        print(f"  {class_name}: {current_samples} -> {target_samples} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        augmented_X.extend(class_data)
        augmented_y.extend([class_idx] * current_samples)
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –æ–±—Ä–∞–∑—Ü–æ–≤
        if current_samples < target_samples:
            needed = target_samples - current_samples
            
            for _ in range(needed):
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü –∏–∑ –∫–ª–∞—Å—Å–∞
                base_sample = class_data[np.random.randint(0, current_samples)]
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∏–¥—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                aug_type = np.random.choice(['noise', 'scale', 'shift', 'smooth', 'mix'])
                
                if aug_type == 'noise':
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞
                    noise_level = np.random.uniform(0.01, 0.05)
                    augmented_sample = base_sample + np.random.normal(0, noise_level * np.std(base_sample), len(base_sample))
                
                elif aug_type == 'scale':
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                    scale_factor = np.random.uniform(0.9, 1.1)
                    augmented_sample = base_sample * scale_factor
                
                elif aug_type == 'shift':
                    # –°–¥–≤–∏–≥ –ø–æ –∞–º–ø–ª–∏—Ç—É–¥–µ
                    shift_amount = np.random.uniform(-0.1, 0.1) * np.mean(np.abs(base_sample))
                    augmented_sample = base_sample + shift_amount
                
                elif aug_type == 'smooth':
                    # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
                    from scipy.ndimage import gaussian_filter1d
                    sigma = np.random.uniform(0.5, 2.0)
                    augmented_sample = gaussian_filter1d(base_sample, sigma=sigma)
                
                elif aug_type == 'mix':
                    # –°–º–µ—à–∏–≤–∞–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º –æ–±—Ä–∞–∑—Ü–æ–º —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞
                    if current_samples > 1:
                        other_sample = class_data[np.random.randint(0, current_samples)]
                        alpha = np.random.uniform(0.3, 0.7)
                        augmented_sample = alpha * base_sample + (1 - alpha) * other_sample
                    else:
                        augmented_sample = base_sample
                
                augmented_X.append(augmented_sample)
                augmented_y.append(class_idx)
    
    return np.array(augmented_X), np.array(augmented_y)

def create_species_specific_models(X_train, y_train, class_names):
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞"""
    print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞...")
    
    species_models = {}
    
    for class_idx, species in enumerate(class_names):
        print(f"  –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {species}...")
        
        # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –∑–∞–¥–∞—á—É: —Ç–µ–∫—É—â–∏–π –≤–∏–¥ vs –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        y_binary = (y_train == class_idx).astype(int)
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å –¥–ª—è —ç—Ç–æ–≥–æ –≤–∏–¥–∞
        models = []
        
        # Random Forest —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        rf = ExtraTreesClassifier(
            n_estimators=1000,
            max_depth=None,
            min_samples_split=2,
            min_samples_leaf=1,
            class_weight={1: 100.0, 0: 1.0},
            random_state=42 + class_idx,
            n_jobs=-1
        )
        
        # –í–µ—Å–∞ –¥–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤
        sample_weights = np.ones(len(y_train))
        sample_weights[y_train == class_idx] = 200.0
        
        rf.fit(X_train, y_binary, sample_weight=sample_weights)
        models.append(('ExtraTrees', rf))
        
        # Gradient Boosting
        gb = GradientBoostingClassifier(
            n_estimators=500,
            learning_rate=0.01,
            max_depth=12,
            subsample=0.8,
            random_state=42 + class_idx
        )
        gb.fit(X_train, y_binary, sample_weight=sample_weights)
        models.append(('GradientBoosting', gb))
        
        # SVM —Å RBF
        svm = SVC(
            C=1000.0,
            gamma='scale',
            kernel='rbf',
            class_weight={1: 200.0, 0: 1.0},
            probability=True,
            random_state=42 + class_idx
        )
        svm.fit(X_train, y_binary, sample_weight=sample_weights)
        models.append(('SVM', svm))
        
        species_models[species] = models
    
    return species_models

def create_meta_ensemble_model(input_shape, num_classes):
    """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞-–∞–Ω—Å–∞–º–±–ª–µ–≤—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å"""
    
    # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    inputs = layers.Input(shape=(input_shape,))
    
    # –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    
    x = layers.Dense(512, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
    species_branches = []
    
    for i in range(num_classes):
        # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ç–≤—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
        branch = layers.Dense(256, activation='relu', name=f'species_{i}_branch')(x)
        branch = layers.BatchNormalization()(branch)
        branch = layers.Dropout(0.3)(branch)
        
        branch = layers.Dense(128, activation='relu')(branch)
        branch = layers.BatchNormalization()(branch)
        branch = layers.Dropout(0.2)(branch)
        
        branch = layers.Dense(64, activation='relu')(branch)
        branch = layers.Dropout(0.2)(branch)
        
        species_branches.append(branch)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤—Å–µ –≤–µ—Ç–≤–∏
    if len(species_branches) > 1:
        combined = layers.Concatenate()(species_branches)
    else:
        combined = species_branches[0]
    
    # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
    attention_weights = layers.Dense(combined.shape[-1], activation='softmax', name='attention_weights')(combined)
    attended = layers.Multiply()([combined, attention_weights])
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
    output = layers.Dense(512, activation='relu')(attended)
    output = layers.BatchNormalization()(output)
    output = layers.Dropout(0.3)(output)
    
    output = layers.Dense(256, activation='relu')(output)
    output = layers.Dropout(0.2)(output)
    
    output = layers.Dense(128, activation='relu')(output)
    output = layers.Dropout(0.1)(output)
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    predictions = layers.Dense(num_classes, activation='softmax', name='predictions')(output)
    
    model = keras.Model(inputs=inputs, outputs=predictions)
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
    optimizer = keras.optimizers.AdamW(
        learning_rate=0.001,
        weight_decay=0.01,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07
    )
    
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def advanced_prediction_fusion(species_models, meta_model, X_test, class_names):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    print("ü§ñ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    species_predictions = {}
    species_probabilities = {}
    
    for species, models in species_models.items():
        class_idx = list(class_names).index(species)
        predictions = []
        probabilities = []
        
        for name, model in models:
            if hasattr(model, 'predict_proba'):
                proba = model.predict_proba(X_test)[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Å—É
                probabilities.append(proba)
            
            pred = model.predict(X_test)
            predictions.append(pred)
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–∞
        species_predictions[species] = np.mean(predictions, axis=0)
        species_probabilities[species] = np.mean(probabilities, axis=0) if probabilities else np.zeros(len(X_test))
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    meta_pred = np.argmax(meta_model.predict(X_test, verbose=0), axis=1)
    meta_proba = meta_model.predict(X_test, verbose=0)
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    final_predictions = []
    final_confidences = []
    
    for i in range(len(X_test)):
        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç –≤—Å–µ—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        species_votes = {}
        total_confidence = 0
        
        for j, species in enumerate(class_names):
            # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            specialist_prob = species_probabilities[species][i]
            
            # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
            meta_prob = meta_proba[i][j]
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
            if specialist_prob > 0.8:  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞
                combined_prob = 0.8 * specialist_prob + 0.2 * meta_prob
            elif specialist_prob > 0.5:  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                combined_prob = 0.6 * specialist_prob + 0.4 * meta_prob
            else:  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –±–æ–ª—å—à–µ –¥–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
                combined_prob = 0.3 * specialist_prob + 0.7 * meta_prob
            
            species_votes[j] = combined_prob
            total_confidence += combined_prob
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        if total_confidence > 0:
            for class_idx in species_votes:
                species_votes[class_idx] /= total_confidence
        
        # –í—ã–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        best_class = max(species_votes, key=species_votes.get)
        best_confidence = species_votes[best_class]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ä–æ–≥–æ–≤
        confidence_thresholds = {
            '–∫–ª–µ–Ω': 0.6,
            '–¥—É–±': 0.7,
            '–±–µ—Ä–µ–∑–∞': 0.3,
            '–µ–ª—å': 0.4,
            '–ª–∏–ø–∞': 0.5,
            '–æ—Å–∏–Ω–∞': 0.4,
            '—Å–æ—Å–Ω–∞': 0.5
        }
        
        species_name = class_names[best_class]
        threshold = confidence_thresholds.get(species_name, 0.5)
        
        if best_confidence >= threshold:
            final_predictions.append(best_class)
        else:
            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è, –≤—ã–±–∏—Ä–∞–µ–º –≤—Ç–æ—Ä–æ–π –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å
            sorted_votes = sorted(species_votes.items(), key=lambda x: x[1], reverse=True)
            if len(sorted_votes) > 1:
                second_best = sorted_votes[1][0]
                final_predictions.append(second_best)
            else:
                final_predictions.append(best_class)
        
        final_confidences.append(best_confidence)
    
    return np.array(final_predictions), np.array(final_confidences)

def analyze_ultimate_results(y_test, y_pred, y_confidence, class_names):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    print("\n" + "="*80)
    print("üèÜ –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í - –í–°–ï –í–ò–î–´")
    print("="*80)
    
    accuracy = np.mean(y_test == y_pred)
    print(f"üéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report = classification_report(y_test, y_pred, target_names=class_names, digits=4)
    print("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    print(report)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –í–°–ï–ú –í–ò–î–ê–ú:")
    excellent_count = 0
    good_count = 0
    acceptable_count = 0
    
    for i, species in enumerate(class_names):
        correct = cm_normalized[i][i]
        total = cm[i].sum()
        avg_confidence = np.mean(y_confidence[y_test == i]) if np.sum(y_test == i) > 0 else 0
        
        if correct >= 0.8:
            status = "üèÜ –ü–†–ï–í–û–°–•–û–î–ù–û"
            excellent_count += 1
        elif correct >= 0.6:
            status = "üéâ –û–¢–õ–ò–ß–ù–û"
            good_count += 1
        elif correct >= 0.4:
            status = "‚ö° –•–û–†–û–®–û"
            acceptable_count += 1
        elif correct >= 0.2:
            status = "üìà –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û"
        else:
            status = "‚ùå –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–ô"
        
        print(f"  {species.upper()}: {correct:.3f} ({correct*100:.1f}%) | "
              f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.3f} | {status}")
    
    working_species = excellent_count + good_count + acceptable_count
    
    print(f"\n‚úÖ –ò–¢–û–ì–ò:")
    print(f"   üèÜ –ü–†–ï–í–û–°–•–û–î–ù–û: {excellent_count} –≤–∏–¥–æ–≤ (‚â•80%)")
    print(f"   üéâ –û–¢–õ–ò–ß–ù–û: {good_count} –≤–∏–¥–æ–≤ (‚â•60%)")
    print(f"   ‚ö° –•–û–†–û–®–û: {acceptable_count} –≤–∏–¥–æ–≤ (‚â•40%)")
    print(f"   üìä –†–ê–ë–û–¢–ê–ï–¢: {working_species}/7 –≤–∏–¥–æ–≤")
    
    if working_species == 7:
        print("\nüèÜüèÜüèÜ –ê–ë–°–û–õ–Æ–¢–ù–´–ô –£–°–ü–ï–•! –í–°–ï 7 –í–ò–î–û–í –†–ê–ë–û–¢–ê–Æ–¢! üèÜüèÜüèÜ")
    elif working_species >= 6:
        print("\nüéâüéâüéâ –ü–û–ß–¢–ò –ò–î–ï–ê–õ–¨–ù–û! –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê! üéâüéâüéâ")
    elif working_species >= 5:
        print("\n‚ö°‚ö°‚ö° –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –ë–û–õ–¨–®–ò–ù–°–¢–í–û –†–ê–ë–û–¢–ê–ï–¢! ‚ö°‚ö°‚ö°")
    elif working_species >= 3:
        print("\nüìàüìàüìà –•–û–†–û–®–ò–ô –ü–†–û–ì–†–ï–°–°! –ü–û–õ–û–í–ò–ù–ê –†–ê–ë–û–¢–ê–ï–¢! üìàüìàüìà")
    else:
        print("\nüîß –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê")
    
    return accuracy, working_species, excellent_count + good_count

def main():
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –≤–∏–¥–æ–≤"""
    print("üöÄüöÄüöÄ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–û–ï –†–ï–®–ï–ù–ò–ï –î–õ–Ø –í–°–ï–• 7 –í–ò–î–û–í üöÄüöÄüöÄ")
    print("="*80)
    print("üéØ –¶–ï–õ–¨: –ó–ê–°–¢–ê–í–ò–¢–¨ –†–ê–ë–û–¢–ê–¢–¨ –í–°–ï 7 –í–ò–î–û–í –õ–Æ–ë–û–ô –¶–ï–ù–û–ô!")
    print("üß† –ú–ï–¢–û–î–´: –í—Å–µ —Ç–µ—Ö–Ω–∏–∫–∏ ML + –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è + —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è + –º–µ—Ç–∞-–∞–Ω—Å–∞–º–±–ª–∏")
    print("="*80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüì• –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –î–û–°–¢–£–ü–ù–´–• –î–ê–ù–ù–´–•...")
    train_data, train_labels = load_spring_data()
    test_data, test_labels = load_summer_data()
    
    print(f"–í–µ—Å–µ–Ω–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(train_data)}")
    print(f"–õ–µ—Ç–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(test_data)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤–∏–¥–∞–º
    for species in ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']:
        spring_count = train_labels.count(species)
        summer_count = test_labels.count(species)
        print(f"  {species}: –≤–µ—Å–Ω–∞ {spring_count}, –ª–µ—Ç–æ {summer_count}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("\nüîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –° –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ú–ò –í–û–ó–ú–û–ñ–ù–û–°–¢–Ø–ú–ò...")
    all_spectra = train_data + test_data
    min_length = min(len(spectrum) for spectrum in all_spectra)
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    train_data_trimmed = [spectrum[:min_length] for spectrum in train_data]
    test_data_trimmed = [spectrum[:min_length] for spectrum in test_data]
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüß† –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–£–ü–ï–†-–ü–†–ò–ó–ù–ê–ö–û–í...")
    X_train = extract_super_features(train_data_trimmed)
    X_test = extract_super_features(test_data_trimmed)
    
    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {X_train.shape[1]} —Å—É–ø–µ—Ä-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤!")
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(train_labels)
    y_test = label_encoder.transform(test_labels)
    
    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    X_train_aug, y_train_aug = augment_data(X_train, y_train, label_encoder.classes_)
    
    print(f"–ü–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {len(X_train_aug)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    print("\n‚öôÔ∏è –û–ë–†–ê–ë–û–¢–ö–ê NaN –ò –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø...")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
    from sklearn.impute import SimpleImputer
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    imputer = SimpleImputer(strategy='mean')
    X_train_clean = imputer.fit_transform(X_train_aug)
    X_test_clean = imputer.transform(X_test)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ NaN —É–±—Ä–∞–Ω—ã
    print(f"NaN –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {np.isnan(X_train_clean).sum()}")
    print(f"NaN –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {np.isnan(X_test_clean).sum()}")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PowerTransformer –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)
    X_train_transformed = power_transformer.fit_transform(X_train_clean)
    X_test_transformed = power_transformer.transform(X_test_clean)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–æ–±–∞—Å—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    robust_scaler = RobustScaler()
    X_train_final = robust_scaler.fit_transform(X_train_transformed)
    X_test_final = robust_scaler.transform(X_test_transformed)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ NaN –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {np.isnan(X_train_final).sum()}")
    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ NaN –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {np.isnan(X_test_final).sum()}")
    
    print("‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    species_models = create_species_specific_models(
        X_train_final, y_train_aug, label_encoder.classes_
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–∞–Ω—Å–∞–º–±–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏
    print("\nü§ñ –°–û–ó–î–ê–ù–ò–ï –ú–ï–¢–ê-–ê–ù–°–ê–ú–ë–õ–ï–í–û–ô –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò...")
    meta_model = create_meta_ensemble_model(X_train_final.shape[1], len(label_encoder.classes_))
    
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏:")
    meta_model.summary()
    
    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≤–∏–¥–æ–≤
    class_weights = {}
    for i, species in enumerate(label_encoder.classes_):
        if species in ['–∫–ª–µ–Ω', '–¥—É–±']:
            class_weights[i] = 500.0  # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π –≤–µ—Å
        elif species in ['–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']:
            class_weights[i] = 100.0  # –í—ã—Å–æ–∫–∏–π –≤–µ—Å
        else:
            class_weights[i] = 1.0    # –û–±—ã—á–Ω—ã–π –≤–µ—Å
    
    # Callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=30,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=15,
            min_lr=1e-7,
            verbose=1
        ),
        keras.callbacks.ModelCheckpoint(
            'ultimate_meta_model.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    print("\nüöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–ï–¢–ê-–ú–û–î–ï–õ–ò...")
    history = meta_model.fit(
        X_train_final, y_train_aug,
        epochs=300,
        batch_size=16,
        validation_split=0.2,
        class_weight=class_weights,
        callbacks=callbacks,
        verbose=1
    )
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    final_predictions, final_confidences = advanced_prediction_fusion(
        species_models, meta_model, X_test_final, label_encoder.classes_
    )
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    accuracy, working_species, good_species = analyze_ultimate_results(
        y_test, final_predictions, final_confidences, label_encoder.classes_
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã
    print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–û–ô –°–ò–°–¢–ï–ú–´...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    meta_model.save('ultimate_meta_ensemble.keras')
    joblib.dump(imputer, 'ultimate_imputer.pkl')
    joblib.dump(power_transformer, 'ultimate_power_transformer.pkl')
    joblib.dump(robust_scaler, 'ultimate_robust_scaler.pkl')
    joblib.dump(label_encoder, 'ultimate_label_encoder.pkl')
    joblib.dump(species_models, 'ultimate_species_models.pkl')
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
    system_metadata = {
        'version': 'Ultimate_Aggressive_v1.0',
        'total_features': X_train_final.shape[1],
        'working_species': working_species,
        'good_species': good_species,
        'overall_accuracy': accuracy,
        'confidence_thresholds': {
            '–∫–ª–µ–Ω': 0.6, '–¥—É–±': 0.7, '–±–µ—Ä–µ–∑–∞': 0.3,
            '–µ–ª—å': 0.4, '–ª–∏–ø–∞': 0.5, '–æ—Å–∏–Ω–∞': 0.4, '—Å–æ—Å–Ω–∞': 0.5
        }
    }
    
    joblib.dump(system_metadata, 'ultimate_system_metadata.pkl')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    with open('ULTIMATE_RESULTS.md', 'w', encoding='utf-8') as f:
        f.write("# üöÄ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–û–ï –†–ï–®–ï–ù–ò–ï - –†–ï–ó–£–õ–¨–¢–ê–¢–´\n\n")
        f.write(f"## üìä –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n")
        f.write(f"- **–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:** {accuracy:.3f} ({accuracy*100:.1f}%)\n")
        f.write(f"- **–†–∞–±–æ—Ç–∞—é—â–∏—Ö –≤–∏–¥–æ–≤:** {working_species}/7\n")
        f.write(f"- **–•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö:** {good_species}/7\n\n")
        
        f.write("## üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤–∏–¥–∞–º:\n")
        cm = confusion_matrix(y_test, final_predictions)
        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        for i, species in enumerate(label_encoder.classes_):
            acc = cm_norm[i][i]
            f.write(f"- **{species.upper()}:** {acc:.3f} ({acc*100:.1f}%)\n")
        
        f.write(f"\n## üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:\n")
        f.write(f"- –°—É–ø–µ—Ä-–ø—Ä–∏–∑–Ω–∞–∫–∏: {X_train_final.shape[1]} —à—Ç—É–∫\n")
        f.write("- –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π —Å –∏–º–ø—É—Ç–∞—Ü–∏–µ–π\n")
        f.write("- –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å 5 —Ç–∏–ø–∞–º–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π\n")
        f.write("- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞\n")
        f.write("- –ú–µ—Ç–∞-–∞–Ω—Å–∞–º–±–ª–µ–≤–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å 2.7M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n")
        f.write("- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n")
        f.write("- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n")
    
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
    print("   - ultimate_meta_ensemble.keras")
    print("   - ultimate_imputer.pkl")
    print("   - ultimate_power_transformer.pkl")
    print("   - ultimate_robust_scaler.pkl")
    print("   - ultimate_label_encoder.pkl")
    print("   - ultimate_species_models.pkl")
    print("   - ultimate_system_metadata.pkl")
    print("   - ULTIMATE_RESULTS.md")
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
    print("\n" + "="*80)
    print("üèÜ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–û–ì–û –†–ï–®–ï–ù–ò–Ø")
    print("="*80)
    print(f"üìä –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)")
    print(f"üéØ –†–∞–±–æ—Ç–∞—é—â–∏—Ö –≤–∏–¥–æ–≤: {working_species}/7")
    print(f"‚≠ê –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö: {good_species}/7")
    
    if working_species == 7:
        print("\nüèÜüèÜüèÜ –ü–û–õ–ù–ê–Ø –ü–û–ë–ï–î–ê! –í–°–ï –í–ò–î–´ –†–ê–ë–û–¢–ê–Æ–¢! üèÜüèÜüèÜ")
        print("üí™ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ü–û–î–•–û–î –°–†–ê–ë–û–¢–ê–õ!")
    elif working_species >= 6:
        print("\nüéâüéâüéâ –ü–û–ß–¢–ò –ò–î–ï–ê–õ–¨–ù–û! –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –ë–û–Æ! üéâüéâüéâ")
    elif working_species >= 5:
        print("\n‚ö°‚ö°‚ö° –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –ë–û–õ–¨–®–ò–ù–°–¢–í–û –ü–û–ë–ï–ñ–î–ï–ù–û! ‚ö°‚ö°‚ö°")
    else:
        print(f"\nüìà –ü–†–û–ì–†–ï–°–° –î–û–°–¢–ò–ì–ù–£–¢! {working_species} –∏–∑ 7 –≤–∏–¥–æ–≤ –ø–æ–∫–æ—Ä–µ–Ω—ã!")
    
    print("\nüöÄ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ê–ì–†–ï–°–°–ò–í–ù–û–ï –†–ï–®–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main() 