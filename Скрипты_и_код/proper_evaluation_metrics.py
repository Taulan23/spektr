#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–†–ê–í–ò–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –û–¶–ï–ù–ö–ò –î–õ–Ø –†–ï–®–ï–ù–ò–Ø –ü–†–û–ë–õ–ï–ú–´ –í–´–°–û–ö–ò–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
==================================================================

–≠—Ç–æ—Ç –∫–æ–¥ —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
–¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

–ü–†–û–ë–õ–ï–ú–ê: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è accuracy –¥–∞–µ—Ç misleading —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∫–ª–∞—Å—Å–æ–≤
–†–ï–®–ï–ù–ò–ï: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–µ—Ç—Ä–∏–∫, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è —Ä–µ–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score, precision_recall_fscore_support,
    classification_report, confusion_matrix, cohen_kappa_score,
    roc_auc_score, average_precision_score, matthews_corrcoef
)
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import cross_val_score, StratifiedKFold
import warnings
warnings.filterwarnings('ignore')

class ProperEvaluationMetrics:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""

    def __init__(self, model, X, y, class_names=None):
        self.model = model
        self.X = X
        self.y = y
        self.class_names = class_names if class_names else [f"Class_{i}" for i in range(len(np.unique(y)))]
        self.n_classes = len(np.unique(y))

    def calculate_all_metrics(self, y_true, y_pred, y_proba=None):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏"""

        metrics = {}

        # 1. –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)
        metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)
        metrics['matthews_corrcoef'] = matthews_corrcoef(y_true, y_pred)

        # 2. –ú–ï–¢–†–ò–ö–ò –ù–ê –û–°–ù–û–í–ï PRECISION/RECALL
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )

        metrics['precision_macro'] = np.mean(precision)
        metrics['recall_macro'] = np.mean(recall)
        metrics['f1_macro'] = np.mean(f1)

        metrics['precision_weighted'] = np.average(precision, weights=support)
        metrics['recall_weighted'] = np.average(recall, weights=support)
        metrics['f1_weighted'] = np.average(f1, weights=support)

        # 3. –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ö–õ–ê–°–°–ê
        metrics['per_class_precision'] = precision
        metrics['per_class_recall'] = recall
        metrics['per_class_f1'] = f1
        metrics['per_class_support'] = support

        # 4. AUC –ú–ï–¢–†–ò–ö–ò (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
        if y_proba is not None:
            try:
                if self.n_classes == 2:
                    metrics['roc_auc'] = roc_auc_score(y_true, y_proba[:, 1])
                    metrics['pr_auc'] = average_precision_score(y_true, y_proba[:, 1])
                else:
                    # –î–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    y_true_bin = label_binarize(y_true, classes=range(self.n_classes))
                    if y_true_bin.shape[1] == 1:  # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å –≤ y_true
                        metrics['roc_auc_macro'] = np.nan
                        metrics['roc_auc_weighted'] = np.nan
                    else:
                        metrics['roc_auc_macro'] = roc_auc_score(y_true_bin, y_proba, average='macro', multi_class='ovr')
                        metrics['roc_auc_weighted'] = roc_auc_score(y_true_bin, y_proba, average='weighted', multi_class='ovr')
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å AUC –º–µ—Ç—Ä–∏–∫–∏: {e}")

        return metrics

    def cross_validate_with_proper_metrics(self, cv_folds=5):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

        print("üîÑ –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –ú–ï–¢–†–ò–ö–ê–ú–ò")
        print("=" * 60)

        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

        # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è –º–µ—Ç—Ä–∏–∫
        cv_metrics = {
            'accuracy': [],
            'balanced_accuracy': [],
            'f1_macro': [],
            'cohen_kappa': [],
            'matthews_corrcoef': []
        }

        per_class_metrics = {
            'precision': [],
            'recall': [],
            'f1': []
        }

        for fold, (train_idx, val_idx) in enumerate(cv.split(self.X, self.y)):
            print(f"   Fold {fold + 1}/{cv_folds}...", end=" ")

            X_train, X_val = self.X[train_idx], self.X[val_idx]
            y_train, y_val = self.y[train_idx], self.y[val_idx]

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_val)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
            try:
                y_proba = self.model.predict_proba(X_val)
            except:
                y_proba = None

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            fold_metrics = self.calculate_all_metrics(y_val, y_pred, y_proba)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            for metric in cv_metrics.keys():
                if metric in fold_metrics:
                    cv_metrics[metric].append(fold_metrics[metric])

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ per-class –º–µ—Ç—Ä–∏–∫
            per_class_metrics['precision'].append(fold_metrics['per_class_precision'])
            per_class_metrics['recall'].append(fold_metrics['per_class_recall'])
            per_class_metrics['f1'].append(fold_metrics['per_class_f1'])

            print("‚úì")

        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–ò:")
        print("-" * 60)

        results_summary = {}
        for metric, values in cv_metrics.items():
            if values:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ø–∏—Å–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π
                mean_val = np.mean(values)
                std_val = np.std(values)
                results_summary[metric] = {'mean': mean_val, 'std': std_val}
                print(f"   {metric:20s}: {mean_val:.3f} ¬± {std_val:.3f}")

        # Per-class —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ö–õ–ê–°–°–ê–ú:")
        print("-" * 60)

        per_class_summary = {}
        for i, class_name in enumerate(self.class_names):
            if i < len(per_class_metrics['precision'][0]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª–∞—Å—Å–∞
                class_precision = [fold_prec[i] for fold_prec in per_class_metrics['precision']]
                class_recall = [fold_rec[i] for fold_rec in per_class_metrics['recall']]
                class_f1 = [fold_f1[i] for fold_f1 in per_class_metrics['f1']]

                per_class_summary[class_name] = {
                    'precision': {'mean': np.mean(class_precision), 'std': np.std(class_precision)},
                    'recall': {'mean': np.mean(class_recall), 'std': np.std(class_recall)},
                    'f1': {'mean': np.mean(class_f1), 'std': np.std(class_f1)}
                }

                print(f"   {class_name:15s}:")
                print(f"     Precision: {np.mean(class_precision):.3f} ¬± {np.std(class_precision):.3f}")
                print(f"     Recall:    {np.mean(class_recall):.3f} ¬± {np.std(class_recall):.3f}")
                print(f"     F1-score:  {np.mean(class_f1):.3f} ¬± {np.std(class_f1):.3f}")

        return results_summary, per_class_summary

    def analyze_class_imbalance_impact(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏"""

        print("\nüéØ –ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –î–ò–°–ë–ê–õ–ê–ù–°–ê –ö–õ–ê–°–°–û–í")
        print("=" * 60)

        # –ü–æ–¥—Å—á–µ—Ç –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º
        unique, counts = np.unique(self.y, return_counts=True)
        class_distribution = dict(zip(unique, counts))

        print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        total_samples = len(self.y)
        imbalance_ratios = []

        for i, (class_idx, count) in enumerate(class_distribution.items()):
            class_name = self.class_names[class_idx] if class_idx < len(self.class_names) else f"Class_{class_idx}"
            percentage = (count / total_samples) * 100
            print(f"   {class_name:20s}: {count:4d} –æ–±—Ä–∞–∑—Ü–æ–≤ ({percentage:5.1f}%)")
            imbalance_ratios.append(count)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
        max_count = max(imbalance_ratios)
        min_count = min(imbalance_ratios)
        imbalance_ratio = max_count / min_count

        print(f"\nüìà –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {imbalance_ratio:.1f}:1")

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        if imbalance_ratio < 1.5:
            balance_status = "‚úÖ –û–¢–õ–ò–ß–ù–û –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–´"
            expected_accuracy_drop = "0-5%"
        elif imbalance_ratio < 3:
            balance_status = "üü° –£–ú–ï–†–ï–ù–ù–û –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–´"
            expected_accuracy_drop = "5-15%"
        elif imbalance_ratio < 10:
            balance_status = "üü† –£–ú–ï–†–ï–ù–ù–û –ù–ï–°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–´"
            expected_accuracy_drop = "15-30%"
        else:
            balance_status = "üî¥ –°–ò–õ–¨–ù–û –ù–ï–°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–´"
            expected_accuracy_drop = "30-50%"

        print(f"   –°—Ç–∞—Ç—É—Å: {balance_status}")
        print(f"   –û–∂–∏–¥–∞–µ–º–æ–µ –ø–∞–¥–µ–Ω–∏–µ accuracy –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö: {expected_accuracy_drop}")

        return imbalance_ratio, class_distribution

    def create_comprehensive_report(self):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""

        print("\nüìã –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –û–¢–ß–ï–¢: –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
        print("=" * 80)

        # 1. –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
        imbalance_ratio, class_dist = self.analyze_class_imbalance_impact()

        # 2. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv_results, per_class_results = self.cross_validate_with_proper_metrics()

        # 3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –í–´–ë–û–†–£ –ú–ï–¢–†–ò–ö:")
        print("-" * 60)

        if imbalance_ratio < 2:
            print("‚úÖ –î–ª—è –≤–∞—à–∏—Ö —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            print("   ‚Ä¢ Accuracy - –ø–æ–¥—Ö–æ–¥—è—â–∞—è –º–µ—Ç—Ä–∏–∫–∞")
            print("   ‚Ä¢ Balanced Accuracy - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
            print("   ‚Ä¢ F1-macro - –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
            print("   ‚Ä¢ Per-class metrics - –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        else:
            print("‚ö†Ô∏è –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
            print("   ‚Ä¢ Balanced Accuracy (–û–°–ù–û–í–ù–ê–Ø –º–µ—Ç—Ä–∏–∫–∞)")
            print("   ‚Ä¢ F1-macro (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π)")
            print("   ‚Ä¢ Cohen's Kappa (—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å)")
            print("   ‚Ä¢ Per-class Precision/Recall (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞)")
            print("   ‚Ä¢ ‚ùå –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç—É—é Accuracy!")

        # 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        print(f"\nüöÄ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ –ú–û–î–ï–õ–ò:")
        print("-" * 60)

        if imbalance_ratio > 5:
            print("üîß –î–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ class_weight='balanced'")
            print("   ‚Ä¢ –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (SMOTE, ADASYN)")
            print("   ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ cost-sensitive learning")
            print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã")

        # 5. –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        print(f"\n‚ö†Ô∏è –í–ê–ñ–ù–´–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
        print("-" * 60)
        print("   ‚Ä¢ –ù–ï —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¥—Ä—É–≥–∏–µ —É—Å–ª–æ–≤–∏—è")
        print("   ‚Ä¢ –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ —É—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
        print("   ‚Ä¢ –ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ")

        return cv_results, per_class_results, imbalance_ratio

    def visualize_metrics_comparison(self, cv_results):
        """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫"""

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        metrics_to_plot = ['accuracy', 'balanced_accuracy', 'f1_macro', 'cohen_kappa']
        means = [cv_results[m]['mean'] for m in metrics_to_plot if m in cv_results]
        stds = [cv_results[m]['std'] for m in metrics_to_plot if m in cv_results]
        labels = [m.replace('_', ' ').title() for m in metrics_to_plot if m in cv_results]

        bars = ax1.bar(labels, means, yerr=stds, capsize=5, alpha=0.7,
                      color=['lightcoral', 'lightgreen', 'lightblue', 'lightyellow'])

        ax1.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏', fontsize=14, weight='bold')
        ax1.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')
        ax1.set_ylim(0, 1)
        ax1.grid(True, alpha=0.3)

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, mean, std in zip(bars, means, stds):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                    f'{mean:.3f}¬±{std:.3f}', ha='center', va='bottom', fontsize=9)

        # –ì—Ä–∞—Ñ–∏–∫ 2: Accuracy vs Balanced Accuracy
        if 'accuracy' in cv_results and 'balanced_accuracy' in cv_results:
            acc_mean = cv_results['accuracy']['mean']
            bal_acc_mean = cv_results['balanced_accuracy']['mean']

            comparison_data = {
                'Accuracy\n(–º–æ–∂–µ—Ç –æ–±–º–∞–Ω—ã–≤–∞—Ç—å)': acc_mean,
                'Balanced Accuracy\n(—á–µ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)': bal_acc_mean
            }

            bars2 = ax2.bar(comparison_data.keys(), comparison_data.values(),
                           color=['lightcoral', 'lightgreen'], alpha=0.8)

            ax2.set_title('Accuracy vs Balanced Accuracy', fontsize=14, weight='bold')
            ax2.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')
            ax2.set_ylim(0, 1)
            ax2.grid(True, alpha=0.3)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–Ω–æ—Å—Ç—å
            diff = abs(acc_mean - bal_acc_mean)
            ax2.text(0.5, max(acc_mean, bal_acc_mean) + 0.05,
                    f'–†–∞–∑–Ω–æ—Å—Ç—å: {diff:.3f}', ha='center', fontsize=12,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar, value in zip(bars2, comparison_data.values()):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{value:.3f}', ha='center', va='bottom', fontsize=11, weight='bold')

        plt.tight_layout()
        plt.savefig('proper_evaluation_metrics.png', dpi=300, bbox_inches='tight')
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≥—Ä–∞—Ñ–∏–∫: proper_evaluation_metrics.png")

        return fig

    def generate_production_ready_evaluation(self, test_size=0.2):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç production-ready —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏"""

        print("\nüöÄ PRODUCTION-READY –°–ò–°–¢–ï–ú–ê –û–¶–ï–ù–ö–ò")
        print("=" * 80)

        from sklearn.model_selection import train_test_split

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, self.y, test_size=test_size, stratify=self.y, random_state=42
        )

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model.fit(X_train, y_train)
        y_pred = self.model.predict(X_test)

        try:
            y_proba = self.model.predict_proba(X_test)
        except:
            y_proba = None

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        final_metrics = self.calculate_all_metrics(y_test, y_pred, y_proba)

        print("üìä –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:")
        print("-" * 60)

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        key_metrics = ['accuracy', 'balanced_accuracy', 'f1_macro', 'cohen_kappa']
        for metric in key_metrics:
            if metric in final_metrics:
                print(f"   {metric.replace('_', ' ').title():20s}: {final_metrics[metric]:.3f}")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
        print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
        print("-" * 60)

        report = classification_report(y_test, y_pred, target_names=self.class_names,
                                     output_dict=True, zero_division=0)

        for i, class_name in enumerate(self.class_names):
            if class_name in report:
                class_metrics = report[class_name]
                print(f"   {class_name:15s}:")
                print(f"     Precision: {class_metrics['precision']:.3f}")
                print(f"     Recall:    {class_metrics['recall']:.3f}")
                print(f"     F1-score:  {class_metrics['f1-score']:.3f}")
                print(f"     Support:   {class_metrics['support']}")

        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)

        print(f"\nüìä –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö:")
        print("-" * 60)
        print("–°—Ç—Ä–æ–∫–∏ = –∏—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã, –°—Ç–æ–ª–±—Ü—ã = –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")

        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
        cm_df = pd.DataFrame(cm, index=self.class_names, columns=self.class_names)
        print(cm_df)

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._provide_final_recommendations(final_metrics)

        return final_metrics, report, cm

    def _provide_final_recommendations(self, metrics):
        """–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

        print(f"\nüí° –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("=" * 60)

        accuracy = metrics.get('accuracy', 0)
        balanced_accuracy = metrics.get('balanced_accuracy', 0)
        f1_macro = metrics.get('f1_macro', 0)

        accuracy_gap = abs(accuracy - balanced_accuracy)

        if accuracy_gap < 0.05:
            print("‚úÖ –û–¢–õ–ò–ß–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print("   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Accuracy –∏ Balanced Accuracy")
            print("   ‚Ä¢ –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–∞—Ö")
            print("   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞–¥–µ–∂–Ω—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π")
        elif accuracy_gap < 0.15:
            print("üü° –•–û–†–û–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –û–ì–û–í–û–†–ö–ê–ú–ò:")
            print("   ‚Ä¢ –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏")
            print("   ‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª–∞—Å—Å—ã –º–æ–≥—É—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ö—É–∂–µ")
            print("   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
        else:
            print("üî¥ –¢–†–ï–ë–£–ï–¢–°–Ø –£–õ–£–ß–®–ï–ù–ò–ï:")
            print("   ‚Ä¢ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Accuracy –∏ Balanced Accuracy")
            print("   ‚Ä¢ –ú–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–µ–¥–∫–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏")
            print("   ‚Ä¢ –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")

        print(f"\nüìù –î–õ–Ø –ü–£–ë–õ–ò–ö–ê–¶–ò–ò –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï:")
        print("   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞: Balanced Accuracy")
        print("   ‚Ä¢ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: F1-macro, Cohen's Kappa")
        print("   ‚Ä¢ –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ: Per-class Precision/Recall")
        print("   ‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: —É—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è")


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏"""

    print("üìä –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–ê–í–ò–õ–¨–ù–û–ô –°–ò–°–¢–ï–ú–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ï–ô")
    print("=" * 80)
    print("–¶–µ–ª—å: –ü–æ–∫–∞–∑–∞—Ç—å, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∏–∑–±–µ–≥–∞—Ç—å misleading —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 80)

    # –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    from sklearn.datasets import make_classification
    from sklearn.ensemble import ExtraTreesClassifier

    # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_classes=5,
        n_informative=15,
        n_redundant=5,
        weights=[0.5, 0.2, 0.15, 0.1, 0.05],  # –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        random_state=42
    )

    class_names = ['–°–æ—Å–Ω–∞', '–ë–µ—Ä–µ–∑–∞', '–ï–ª—å', '–î—É–±', '–õ–∏–ø–∞']

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = ExtraTreesClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        class_weight='balanced'  # –í–∞–∂–Ω–æ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
    )

    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏
    evaluator = ProperEvaluationMetrics(model, X, y, class_names)

    # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—Ü–µ–Ω–∫—É
    cv_results, per_class_results, imbalance_ratio = evaluator.create_comprehensive_report()

    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    evaluator.visualize_metrics_comparison(cv_results)

    # Production-ready –æ—Ü–µ–Ω–∫–∞
    final_metrics, report, cm = evaluator.generate_production_ready_evaluation()

    print("\n" + "=" * 80)
    print("‚úÖ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("üìä –¢–µ–ø–µ—Ä—å –≤—ã –∑–Ω–∞–µ—Ç–µ, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏")
    print("üéØ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤–∞—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 80)


if __name__ == "__main__":
    main()
