import os
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

def load_spring_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join(base_path, tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def load_summer_data_original_only():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (–ë–ï–ó –∫–ª–µ–Ω_–∞–º)"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join('.', tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def extract_enhanced_maple_features(spectra):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω"""
    features = []
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –∫–ª–µ–Ω–∞ (–∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
    maple_channels_1 = list(range(172, 186))  # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
    maple_channels_2 = list(range(258, 287))  # –í—Ç–æ—Ä–∏—á–Ω–∞—è –æ–±–ª–∞—Å—Ç—å  
    maple_channels_3 = [179, 180, 181, 258, 276, 286]  # –ö–ª—é—á–µ–≤—ã–µ –ø–∏–∫–∏
    oak_channels = list(range(151, 161))      # –î–ª—è –¥—É–±–∞
    
    for spectrum in spectra:
        spectrum = np.array(spectrum)
        feature_vector = []
        
        # 1. –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_vector.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.max(spectrum),
            np.min(spectrum),
            np.ptp(spectrum),  # —Ä–∞–∑–º–∞—Ö
            np.var(spectrum),  # –¥–∏—Å–ø–µ—Ä—Å–∏—è
        ])
        
        # 2. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
        feature_vector.extend([
            np.percentile(spectrum, 10),
            np.percentile(spectrum, 25),
            np.percentile(spectrum, 75),
            np.percentile(spectrum, 90),
        ])
        
        # 3. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (1-—è –∏ 2-—è)
        derivative1 = np.diff(spectrum)
        derivative2 = np.diff(derivative1)
        feature_vector.extend([
            np.mean(derivative1),
            np.std(derivative1),
            np.max(np.abs(derivative1)),
            np.mean(derivative2),
            np.std(derivative2),
        ])
        
        # 4. –£–°–ò–õ–ï–ù–ù–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞ - –æ–±–ª–∞—Å—Ç—å 1
        if len(spectrum) > max(maple_channels_1):
            maple_region1 = spectrum[maple_channels_1]
            avg_spectrum = np.mean(spectrum)
            feature_vector.extend([
                np.mean(maple_region1),
                np.std(maple_region1),
                np.max(maple_region1),
                np.min(maple_region1),
                np.ptp(maple_region1),
                np.mean(maple_region1) / avg_spectrum if avg_spectrum > 0 else 0,
                np.std(maple_region1) / avg_spectrum if avg_spectrum > 0 else 0,
                np.sum(maple_region1 > np.mean(maple_region1)),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–∫–æ–≤
            ])
        else:
            feature_vector.extend([0] * 8)
        
        # 5. –£–°–ò–õ–ï–ù–ù–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞ - –æ–±–ª–∞—Å—Ç—å 2
        valid_maple_2 = [ch for ch in maple_channels_2 if ch < len(spectrum)]
        if valid_maple_2:
            maple_region2 = spectrum[valid_maple_2]
            avg_spectrum = np.mean(spectrum)
            feature_vector.extend([
                np.mean(maple_region2),
                np.std(maple_region2),
                np.max(maple_region2),
                np.mean(maple_region2) / avg_spectrum if avg_spectrum > 0 else 0,
                len(maple_region2[maple_region2 > np.percentile(spectrum, 75)]),  # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            ])
        else:
            feature_vector.extend([0] * 5)
        
        # 6. –ö–õ–Æ–ß–ï–í–´–ï –ø–∏–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞
        valid_maple_3 = [ch for ch in maple_channels_3 if ch < len(spectrum)]
        if valid_maple_3:
            maple_peaks = spectrum[valid_maple_3]
            feature_vector.extend([
                np.mean(maple_peaks),
                np.max(maple_peaks),
                np.sum(maple_peaks),
                np.std(maple_peaks),
            ])
        else:
            feature_vector.extend([0] * 4)
        
        # 7. –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥—É–±–∞
        if len(spectrum) > max(oak_channels):
            oak_region = spectrum[oak_channels]
            feature_vector.extend([
                np.mean(oak_region),
                np.std(oak_region),
                np.max(oak_region) - np.min(oak_region),
                np.mean(oak_region) / np.mean(spectrum) if np.mean(spectrum) > 0 else 0,
            ])
        else:
            feature_vector.extend([0] * 4)
        
        # 8. –≠–Ω–µ—Ä–≥–∏—è –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
        n_bands = 6  # –ë–æ–ª—å—à–µ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        band_size = len(spectrum) // n_bands
        for i in range(n_bands):
            start_idx = i * band_size
            end_idx = min((i + 1) * band_size, len(spectrum))
            if start_idx < len(spectrum):
                band_energy = np.sum(spectrum[start_idx:end_idx] ** 2)
                feature_vector.append(band_energy)
            else:
                feature_vector.append(0)
        
        # 9. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
        normalized_spectrum = spectrum / np.sum(spectrum) if np.sum(spectrum) > 0 else spectrum
        channels = np.arange(len(spectrum))
        if np.sum(normalized_spectrum) > 0:
            centroid = np.sum(channels * normalized_spectrum)
            spread = np.sqrt(np.sum(((channels - centroid) ** 2) * normalized_spectrum))
            feature_vector.extend([centroid, spread])
        else:
            feature_vector.extend([0, 0])
        
        # 10. –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –æ–±–ª–∞—Å—Ç—è–º–∏ (–≤–∞–∂–Ω–æ –¥–ª—è –∫–ª–µ–Ω–∞)
        mid_point = len(spectrum) // 2
        first_half = np.mean(spectrum[:mid_point])
        second_half = np.mean(spectrum[mid_point:])
        ratio = first_half / second_half if second_half > 0 else 0
        feature_vector.append(ratio)
        
        features.append(feature_vector)
    
    return np.array(features)

def create_hierarchical_classifier(X_train, y_train, X_test, y_test, class_names):
    """–°–æ–∑–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
    print("\nüî∏ –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä")
    print("1-–π —É—Ä–æ–≤–µ–Ω—å: –ö–ª–µ–Ω vs –ù–µ-–∫–ª–µ–Ω")
    print("2-–π —É—Ä–æ–≤–µ–Ω—å: –†–∞–∑–ª–∏—á–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤")
    
    # –£—Ä–æ–≤–µ–Ω—å 1: –ö–ª–µ–Ω vs –æ—Å—Ç–∞–ª—å–Ω—ã–µ
    y_binary_train = (y_train == list(class_names).index('–∫–ª–µ–Ω')).astype(int)
    y_binary_test = (y_test == list(class_names).index('–∫–ª–µ–Ω')).astype(int)
    
    # –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–µ–Ω/–Ω–µ-–∫–ª–µ–Ω
    maple_classifier = RandomForestClassifier(
        n_estimators=200, max_depth=15, random_state=42, 
        class_weight='balanced'  # –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    )
    maple_classifier.fit(X_train, y_binary_train)
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    maple_pred_binary = maple_classifier.predict(X_test)
    maple_pred_proba = maple_classifier.predict_proba(X_test)[:, 1]
    
    maple_accuracy = np.mean(y_binary_test == maple_pred_binary)
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ö–ª–µ–Ω vs –ù–µ-–∫–ª–µ–Ω: {maple_accuracy:.3f}")
    
    # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∫–ª–µ–Ω–∞
    maple_true_positives = np.sum((y_binary_test == 1) & (maple_pred_binary == 1))
    maple_total_positives = np.sum(y_binary_test == 1)
    maple_recall = maple_true_positives / maple_total_positives if maple_total_positives > 0 else 0
    
    maple_predicted_positives = np.sum(maple_pred_binary == 1)
    maple_precision = maple_true_positives / maple_predicted_positives if maple_predicted_positives > 0 else 0
    
    print(f"–ö–ª–µ–Ω - Recall: {maple_recall:.3f}, Precision: {maple_precision:.3f}")
    
    # –£—Ä–æ–≤–µ–Ω—å 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤
    non_maple_mask_train = y_train != list(class_names).index('–∫–ª–µ–Ω')
    non_maple_mask_test = y_test != list(class_names).index('–∫–ª–µ–Ω')
    
    if np.sum(non_maple_mask_train) > 0 and np.sum(non_maple_mask_test) > 0:
        X_train_non_maple = X_train[non_maple_mask_train]
        y_train_non_maple = y_train[non_maple_mask_train]
        X_test_non_maple = X_test[non_maple_mask_test]
        y_test_non_maple = y_test[non_maple_mask_test]
        
        # –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤
        other_classifier = RandomForestClassifier(
            n_estimators=200, max_depth=20, random_state=42
        )
        other_classifier.fit(X_train_non_maple, y_train_non_maple)
        
        other_pred = other_classifier.predict(X_test_non_maple)
        other_accuracy = np.mean(y_test_non_maple == other_pred)
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤: {other_accuracy:.3f}")
    
    return maple_classifier, maple_recall, maple_precision

def analyze_maple_data_distribution(X_train, y_train, X_test, y_test, class_names):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–ª–µ–Ω–∞"""
    print("\nüîç –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –ö–õ–ï–ù–ê:")
    maple_idx = list(class_names).index('–∫–ª–µ–Ω')
    
    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    maple_train_mask = y_train == maple_idx
    maple_train_data = X_train[maple_train_mask]
    
    print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∫–ª–µ–Ω–∞: {np.sum(maple_train_mask)}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–ª–µ–Ω–∞ (—Ç—Ä–µ–Ω): {np.mean(maple_train_data, axis=0)[:5]}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (—Ç—Ä–µ–Ω): {np.std(maple_train_data, axis=0)[:5]}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    maple_test_mask = y_test == maple_idx
    maple_test_data = X_test[maple_test_mask]
    
    print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∫–ª–µ–Ω–∞: {np.sum(maple_test_mask)}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–ª–µ–Ω–∞ (—Ç–µ—Å—Ç): {np.mean(maple_test_data, axis=0)[:5]}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (—Ç–µ—Å—Ç): {np.std(maple_test_data, axis=0)[:5]}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
    non_maple_train = X_train[~maple_train_mask]
    non_maple_test = X_test[~maple_test_mask]
    
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (—Ç—Ä–µ–Ω): {np.mean(non_maple_train, axis=0)[:5]}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (—Ç–µ—Å—Ç): {np.mean(non_maple_test, axis=0)[:5]}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–∏–º–æ—Å—Ç–∏
    maple_train_mean = np.mean(maple_train_data, axis=0)
    maple_test_mean = np.mean(maple_test_data, axis=0)
    correlation = np.corrcoef(maple_train_mean, maple_test_mean)[0, 1]
    
    print(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º–∏ –∏ —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–µ–Ω–∞: {correlation:.3f}")
    
    if correlation < 0.5:
        print("‚ö†Ô∏è  –ü–†–û–ë–õ–ï–ú–ê: –ù–∏–∑–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –≤–µ—Å–µ–Ω–Ω–∏–º–∏ –∏ –ª–µ—Ç–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–µ–Ω–∞!")
    
    return maple_train_data, maple_test_data

def create_enhanced_ensemble_solution(X_train, y_train, X_test, y_test, class_names):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω"""
    print("\nüî∏ –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –£—Å–∏–ª–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å –¥–ª—è –∫–ª–µ–Ω–∞")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∫–ª–µ–Ω–∞
    maple_train_data, maple_test_data = analyze_maple_data_distribution(
        X_train, y_train, X_test, y_test, class_names
    )
    
    models = []
    predictions = []
    maple_idx = list(class_names).index('–∫–ª–µ–Ω')
    
    # –ú–æ–¥–µ–ª—å 1: Random Forest —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω
    sample_weights = np.ones(len(y_train))
    sample_weights[y_train == maple_idx] = 20.0  # –û–ß–ï–ù–¨ –≤—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è –∫–ª–µ–Ω–∞
    
    rf_extreme = RandomForestClassifier(
        n_estimators=500, 
        max_depth=30, 
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42,
        class_weight={maple_idx: 20.0}  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
    )
    rf_extreme.fit(X_train, y_train, sample_weight=sample_weights)
    pred1 = rf_extreme.predict(X_test)
    predictions.append(pred1)
    models.append(('RF Extreme Maple', rf_extreme))
    
    # –ú–æ–¥–µ–ª—å 2: –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–ª–µ–Ω–∞
    # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –∑–∞–¥–∞—á—É: –∫–ª–µ–Ω vs –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
    y_binary_train = (y_train == maple_idx).astype(int)
    y_binary_test = (y_test == maple_idx).astype(int)
    
    maple_detector = RandomForestClassifier(
        n_estimators=300,
        max_depth=25,
        random_state=43,
        class_weight={1: 30.0, 0: 1.0}  # –°–∏–ª—å–Ω—ã–π —Ñ–æ–∫—É—Å –Ω–∞ –∫–ª–µ–Ω
    )
    maple_detector.fit(X_train, y_binary_train)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–ª–µ–Ω–∞
    maple_proba = maple_detector.predict_proba(X_test)[:, 1]
    
    # –ú–æ–¥–µ–ª—å 3: –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    rf_balanced = RandomForestClassifier(
        n_estimators=300, max_depth=20, random_state=44,
        class_weight='balanced'
    )
    rf_balanced.fit(X_train, y_train)
    pred3 = rf_balanced.predict(X_test)
    predictions.append(pred3)
    models.append(('RF Balanced', rf_balanced))
    
    # –ú–æ–¥–µ–ª—å 4: –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω
    nn_model = keras.Sequential([
        layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(len(class_names), activation='softmax')
    ])
    
    nn_model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–ª–µ–Ω–∞
    class_weights = {i: 1.0 for i in range(len(class_names))}
    class_weights[maple_idx] = 15.0
    
    nn_model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=16,
        class_weight=class_weights,
        verbose=0
    )
    
    pred4 = np.argmax(nn_model.predict(X_test, verbose=0), axis=1)
    predictions.append(pred4)
    models.append(('NN Enhanced', nn_model))
    
    # –ì–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è –∫–ª–µ–Ω–∞
    ensemble_pred = []
    for i in range(len(X_test)):
        # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–µ–Ω–∞ —É–≤–µ—Ä–µ–Ω (>0.3), –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if maple_proba[i] > 0.3:
            ensemble_pred.append(maple_idx)
        else:
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            votes = [predictions[j][i] for j in range(len(predictions))]
            unique, counts = np.unique(votes, return_counts=True)
            ensemble_pred.append(unique[np.argmax(counts)])
    
    ensemble_pred = np.array(ensemble_pred)
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:")
    for name, _ in models:
        idx = [i for i, (n, _) in enumerate(models) if n == name][0]
        acc = np.mean(y_test == predictions[idx])
        maple_acc = np.mean(y_test[y_test == maple_idx] == predictions[idx][y_test == maple_idx]) if np.sum(y_test == maple_idx) > 0 else 0
        print(f"  {name}: –æ–±—â–∞—è {acc:.3f}, –∫–ª–µ–Ω {maple_acc:.3f}")
    
    # –û—Ü–µ–Ω–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∫–ª–µ–Ω–∞
    maple_detector_acc = np.mean(y_binary_test == (maple_proba > 0.5))
    maple_recall = np.mean((y_binary_test == 1) & (maple_proba > 0.3)) / np.sum(y_binary_test == 1) if np.sum(y_binary_test == 1) > 0 else 0
    print(f"  –î–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–µ–Ω–∞: —Ç–æ—á–Ω–æ—Å—Ç—å {maple_detector_acc:.3f}, recall –∫–ª–µ–Ω–∞ {maple_recall:.3f}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
    ensemble_acc = np.mean(y_test == ensemble_pred)
    maple_ensemble_acc = np.mean(y_test[y_test == maple_idx] == ensemble_pred[y_test == maple_idx]) if np.sum(y_test == maple_idx) > 0 else 0
    print(f"–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å: –æ–±—â–∞—è {ensemble_acc:.3f}, –∫–ª–µ–Ω {maple_ensemble_acc:.3f}")
    
    return ensemble_pred, maple_ensemble_acc

def analyze_final_solution(y_test, y_pred, class_names, title):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ"""
    print(f"\n{'='*60}")
    print(f"–ê–ù–ê–õ–ò–ó –†–ï–®–ï–ù–ò–Ø: {title}")
    print("="*60)
    
    accuracy = np.mean(y_test == y_pred)
    print(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report = classification_report(y_test, y_pred, target_names=class_names, digits=4)
    print("\n–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    print(report)
    
    # –§–æ–∫—É—Å –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤–∏–¥—ã
    cm = confusion_matrix(y_test, y_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ü–†–û–ë–õ–ï–ú–ù–´–ú –í–ò–î–ê–ú:")
    for problem_species in ['–∫–ª–µ–Ω', '–¥—É–±']:
        if problem_species in class_names:
            idx = list(class_names).index(problem_species)
            correct = cm_normalized[idx][idx]
            total = cm[idx].sum()
            
            print(f"{problem_species.upper()}: {correct:.3f} ({correct*100:.1f}%) –∏–∑ {total} –æ–±—Ä–∞–∑—Ü–æ–≤")
            
            if problem_species == '–∫–ª–µ–Ω':
                if correct > 0.5:
                    print("   üéâ –£–°–ü–ï–•! –ö–ª–µ–Ω —Ç–µ–ø–µ—Ä—å —Ö–æ—Ä–æ—à–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç—Å—è!")
                elif correct > 0.3:
                    print("   ‚ö° –ü–†–û–ì–†–ï–°–°! –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ!")
                elif correct > 0.1:
                    print("   üìà –£–ª—É—á—à–µ–Ω–∏–µ –µ—Å—Ç—å, –Ω–æ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ")
                else:
                    print("   ‚ùå –í—Å–µ –µ—â–µ –ø—Ä–æ–±–ª–µ–º—ã...")
    
    return accuracy, cm_normalized

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üî• –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –ö–õ–ï–ù–ê")
    print("="*60)
    print("–°—Ç—Ä–∞—Ç–µ–≥–∏–∏:")
    print("1. –í–æ–∑–≤—Ä–∞—Ç –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∫–ª–µ–Ω–∞ (–±–µ–∑ –∫–ª–µ–Ω_–∞–º)")
    print("2. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    print("3. –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–π –ø–æ–¥—Ö–æ–¥")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ë–ï–ó –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–ª–µ–Ω–∞)
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ)...")
    train_data, train_labels = load_spring_data()
    test_data, test_labels = load_summer_data_original_only()
    
    print(f"–í–µ—Å–µ–Ω–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(train_data)}")
    print(f"–õ–µ—Ç–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(test_data)}")
    print(f"–õ–µ—Ç–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã –∫–ª–µ–Ω–∞: {test_labels.count('–∫–ª–µ–Ω')}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    all_spectra = train_data + test_data
    min_length = min(len(spectrum) for spectrum in all_spectra)
    
    train_data_trimmed = [spectrum[:min_length] for spectrum in train_data]
    test_data_trimmed = [spectrum[:min_length] for spectrum in test_data]
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω...")
    X_train = extract_enhanced_maple_features(train_data_trimmed)
    X_test = extract_enhanced_maple_features(test_data_trimmed)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(train_labels)
    y_test = label_encoder.transform(test_labels)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"–§–æ—Ä–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_scaled.shape}")
    
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("\nüìä –ë–ê–ó–û–í–ê–Ø –ú–û–î–ï–õ–¨ (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è):")
    rf_baseline = RandomForestClassifier(n_estimators=200, random_state=42)
    rf_baseline.fit(X_train_scaled, y_train)
    baseline_pred = rf_baseline.predict(X_test_scaled)
    
    baseline_acc, _ = analyze_final_solution(y_test, baseline_pred, label_encoder.classes_, "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    maple_classifier, maple_recall, maple_precision = create_hierarchical_classifier(
        X_train_scaled, y_train, X_test_scaled, y_test, label_encoder.classes_
    )
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –£—Å–∏–ª–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–π –ø–æ–¥—Ö–æ–¥
    ensemble_pred, maple_ensemble_acc = create_enhanced_ensemble_solution(
        X_train_scaled, y_train, X_test_scaled, y_test, label_encoder.classes_
    )
    
    ensemble_acc, _ = analyze_final_solution(y_test, ensemble_pred, label_encoder.classes_, "–ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å")
    
    # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print("\n" + "="*60)
    print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ü–û–î–•–û–î–û–í")
    print("="*60)
    print(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (—É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏): {baseline_acc:.3f}")
    print(f"–£—Å–∏–ª–µ–Ω–Ω–∞—è –∞–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å:         {ensemble_acc:.3f}")
    print(f"–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ (recall –∫–ª–µ–Ω–∞):  {maple_recall:.3f}")
    print(f"–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å (—Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–µ–Ω–∞):  {maple_ensemble_acc:.3f}")
    
    print("\nüîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –í –°–ö–†–ò–ü–¢–ï:")
    print("‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞ (46 –≤–º–µ—Å—Ç–æ 24)")
    print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∫–∞–Ω–∞–ª—ã: 172-186, 258-287")
    print("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ –ø–∏–∫–∏: 179, 180, 181, 258, 276, 286")
    print("‚úÖ –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–ª–µ–Ω–∞ (20x –≤–º–µ—Å—Ç–æ 5x)")
    print("‚úÖ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–µ–Ω–∞ —Å –ø–æ—Ä–æ–≥–æ–º 0.3")
    print("‚úÖ –ì–∏–±—Ä–∏–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
    print("‚úÖ –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    best_approach = "–£—Å–∏–ª–µ–Ω–Ω–∞—è –∞–Ω—Å–∞–º–±–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å" if ensemble_acc > baseline_acc else "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"
    print(f"\nüèÜ –õ—É—á—à–∏–π –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_approach}")
    
    improvement = ensemble_acc - baseline_acc
    if improvement > 0:
        print(f"üìà –£–ª—É—á—à–µ–Ω–∏–µ –æ–±—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏: +{improvement:.3f} ({improvement*100:.1f}%)")
    
    if maple_ensemble_acc > 0.5:
        print("üéâ –ü–†–û–†–´–í –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∫–ª–µ–Ω–∞ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç!")
    elif maple_ensemble_acc > 0.3:
        print("‚ö° –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ü–†–û–ì–†–ï–°–° –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∫–ª–µ–Ω–∞!")
    elif maple_ensemble_acc > 0.1:
        print("üìà –ï—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∫–ª–µ–Ω–∞!")
    else:
        print("‚ùå –ö–ª–µ–Ω –æ—Å—Ç–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω—ã–º –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ù–µ–æ–±—Ö–æ–¥–∏–º —Å–±–æ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–ª–µ–Ω–∞")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
    print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    import joblib
    joblib.dump(scaler, 'enhanced_solution_scaler.pkl')
    joblib.dump(label_encoder, 'enhanced_solution_label_encoder.pkl')
    joblib.dump(rf_baseline, 'enhanced_baseline_model.pkl')
    joblib.dump(maple_classifier, 'enhanced_maple_detector.pkl')
    
    print("\nüìÅ –°–û–•–†–ê–ù–ï–ù–´ –§–ê–ô–õ–´:")
    print("- enhanced_solution_scaler.pkl")
    print("- enhanced_solution_label_encoder.pkl") 
    print("- enhanced_baseline_model.pkl")
    print("- enhanced_maple_detector.pkl")
    
    print("\nüéØ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –†–ï–®–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("üìä –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

if __name__ == "__main__":
    main() 