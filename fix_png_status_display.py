#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –°–¢–ê–¢–£–°–ê –í PNG –ú–ê–¢–†–ò–¶–ê–•

–ü—Ä–æ–±–ª–µ–º–∞: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (0.903‚Üí0.509) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ "‚ùå –ü–†–û–ë–õ–ï–ú–ê"
–†–µ—à–µ–Ω–∏–µ: –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import confusion_matrix, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# –†—É—Å—Å–∫–∏–µ —à—Ä–∏—Ñ—Ç—ã –¥–ª—è matplotlib
plt.rcParams['font.family'] = 'Arial Unicode MS'
plt.rcParams['axes.unicode_minus'] = False

def load_spring_data_7_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö 7 –≤–∏–¥–æ–≤ (–≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥)"""
    base_path = "–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    for i, tree_type in enumerate(tree_types):
        tree_path = os.path.join(base_path, tree_type)
        if not os.path.exists(tree_path):
            continue
            
        files = [f for f in os.listdir(tree_path) if f.endswith('.xlsx')]
        print(f"   {tree_type}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        for file in files:
            try:
                file_path = os.path.join(tree_path, file)
                df = pd.read_excel(file_path)
                
                if df.shape[1] >= 2:
                    spectrum = df.iloc[:, 1].values
                    spectrum = spectrum[~np.isnan(spectrum)]
                    
                    if len(spectrum) > 100:
                        all_data.append(spectrum)
                        all_labels.append(i)
            except Exception as e:
                continue
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_data)} —Å–ø–µ–∫—Ç—Ä–æ–≤, {len(set(all_labels))} –∫–ª–∞—Å—Å–æ–≤")
    return np.array(all_data), np.array(all_labels), tree_types

def preprocess_data_properly(X, y):
    """–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    
    min_length = min(len(spectrum) for spectrum in X)
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    X_processed = np.array([spectrum[:min_length] for spectrum in X])
    scaler = RobustScaler()
    X_processed = scaler.fit_transform(X_processed)
    
    print(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X_processed.shape}")
    print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {X_processed.std():.6f}")
    
    return X_processed, y, scaler

def add_gaussian_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —à—É–º–∞ –∫ –¥–∞–Ω–Ω—ã–º"""
    if noise_level == 0:
        return X
    
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

def create_robust_classifier():
    """–°–æ–∑–¥–∞–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
    return RandomForestClassifier(
        n_estimators=1000,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        bootstrap=True,
        oob_score=True,
        random_state=42,
        n_jobs=-1
    )

def evaluate_with_noise_levels(model, X_test, y_test, noise_levels, tree_types):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞"""
    results = []
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞:")
    
    for noise_level in noise_levels:
        X_test_noisy = add_gaussian_noise(X_test, noise_level)
        predictions = model.predict(X_test_noisy)
        probabilities = model.predict_proba(X_test_noisy)
        
        accuracy = accuracy_score(y_test, predictions)
        max_probabilities = np.max(probabilities, axis=1)
        mean_max_prob = np.mean(max_probabilities)
        std_max_prob = np.std(max_probabilities)
        
        cm = confusion_matrix(y_test, predictions)
        unique_probs = len(np.unique(np.round(max_probabilities, 4)))
        uniqueness_ratio = unique_probs / len(max_probabilities) * 100
        
        results.append({
            'noise_level': noise_level,
            'noise_percent': noise_level * 100,
            'accuracy': accuracy,
            'mean_max_probability': mean_max_prob,
            'std_max_probability': std_max_prob,
            'unique_probs': unique_probs,
            'total_samples': len(max_probabilities),
            'uniqueness_ratio': uniqueness_ratio,
            'min_prob': np.min(max_probabilities),
            'max_prob': np.max(max_probabilities),
            'confusion_matrix': cm
        })
        
        print(f"   –®—É–º {noise_level*100:3.0f}%: —Ç–æ—á–Ω–æ—Å—Ç—å={accuracy:.3f}, "
              f"—Å—Ä–µ–¥–Ω—è—è_–º–∞–∫—Å_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={mean_max_prob:.3f}, "
              f"—É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å={uniqueness_ratio:.1f}%")
    
    return results

def create_corrected_confusion_matrices_png(results, tree_types, save_path="confusion_matrices_CORRECTED.png"):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ PNG —Å –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞
    
    –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞
    """
    n_matrices = len(results)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫: –ü–†–ê–í–ò–õ–¨–ù–û–ï –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ (7 –≤–∏–¥–æ–≤)', 
                 fontsize=16, fontweight='bold')
    
    axes_flat = axes.flatten()
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–¥–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    baseline_prob = results[0]['mean_max_probability']  # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–µ–∑ —à—É–º–∞
    prob_threshold = baseline_prob * 0.8  # –ü–æ—Ä–æ–≥ —Å–Ω–∏–∂–µ–Ω–∏—è (20% –æ—Ç –±–∞–∑–æ–≤–æ–π)
    
    for i, result in enumerate(results):
        if i >= 6:
            break
            
        ax = axes_flat[i]
        cm = result['confusion_matrix']
        noise_level = result['noise_percent']
        accuracy = result['accuracy']
        mean_prob = result['mean_max_probability']
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        sns.heatmap(cm_normalized, 
                   annot=True, 
                   fmt='.3f',
                   cmap='Blues',
                   ax=ax,
                   xticklabels=tree_types,
                   yticklabels=tree_types,
                   cbar_kws={'shrink': 0.8})
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –°–¢–ê–¢–£–°–ê
        if i == 0:
            # –î–ª—è –Ω—É–ª–µ–≤–æ–≥–æ —à—É–º–∞ - –≤—Å–µ–≥–¥–∞ –∑–µ–ª–µ–Ω—ã–π (–±–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è)
            color = 'green'
            status = '‚úÖ –ë–ê–ó–û–í–ê–Ø –õ–ò–ù–ò–Ø'
        elif mean_prob <= baseline_prob:
            # –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç –∏–ª–∏ —Ä–∞–≤–Ω–∞ –±–∞–∑–æ–≤–æ–π - —Ö–æ—Ä–æ—à–æ
            color = 'green'
            status = '‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û'
        elif mean_prob > baseline_prob * 1.1:
            # –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç –±–æ–ª—å—à–µ —á–µ–º –Ω–∞ 10% - –ø—Ä–æ–±–ª–µ–º–∞
            color = 'red'
            status = '‚ùå –ü–†–û–ë–õ–ï–ú–ê'
        else:
            # –ù–µ–±–æ–ª—å—à–∏–µ –∫–æ–ª–µ–±–∞–Ω–∏—è - –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
            color = 'orange'
            status = '‚ö†Ô∏è –°–¢–ê–ë–ò–õ–¨–ù–û'
            
        ax.set_title(f'–®—É–º: {noise_level:.0f}% | –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}\n'
                    f'–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {mean_prob:.3f} {status}',
                    fontsize=12, color=color, fontweight='bold')
        ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"üìä –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï PNG –º–∞—Ç—Ä–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")
    plt.show()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - —Å–æ–∑–¥–∞–Ω–∏–µ PNG —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞"""
    print("üéØ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –°–¢–ê–¢–£–°–ê –í PNG –ú–ê–¢–†–ò–¶–ê–•\n")
    print("=" * 60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y, tree_types = load_spring_data_7_species()
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    X_processed, y_processed, scaler = preprocess_data_properly(X, y)
    
    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed
    )
    
    print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û–±—É—á–µ–Ω–∏–µ: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   –¢–µ—Å—Ç: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–æ–±–∞—Å—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
    model = create_robust_classifier()
    
    print("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.fit(X_train, y_train)
    print(f"   OOB Score: {model.oob_score_:.3f}")
    
    # 5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º
    noise_levels = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]
    results = evaluate_with_noise_levels(model, X_test, y_test, noise_levels, tree_types)
    
    # 6. –°–æ–∑–¥–∞–Ω–∏–µ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–• PNG —Ñ–∞–π–ª–æ–≤
    print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ PNG —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Å—Ç–∞—Ç—É—Å–∞...")
    create_corrected_confusion_matrices_png(results, tree_types, "confusion_matrices_STATUS_FIXED.png")
    
    # 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    print("\n" + "="*60)
    print("üîç –ü–†–û–í–ï–†–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –°–¢–ê–¢–£–°–ê:")
    print("="*60)
    
    baseline_prob = results[0]['mean_max_probability']
    final_prob = results[-1]['mean_max_probability']
    
    print(f"\nüìä –í–ï–†–û–Ø–¢–ù–û–°–¢–ò:")
    for result in results:
        noise = result['noise_percent']
        prob = result['mean_max_probability']
        change = prob - baseline_prob
        print(f"   –®—É–º {noise:3.0f}%: {prob:.3f} (–∏–∑–º–µ–Ω–µ–Ω–∏–µ: {change:+.3f})")
    
    if final_prob < baseline_prob:
        print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ü–ê–î–ê–Æ–¢ ({baseline_prob:.3f} ‚Üí {final_prob:.3f})")
        print("‚úÖ –°–¢–ê–¢–£–°: –¢–µ–ø–µ—Ä—å –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –∑–µ–ª–µ–Ω—ã–º —Ü–≤–µ—Ç–æ–º!")
    else:
        print(f"\n‚ùå –ü–†–û–ë–õ–ï–ú–ê: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ù–ï –ø–∞–¥–∞—é—Ç ({baseline_prob:.3f} ‚Üí {final_prob:.3f})")
    
    print(f"\nüìÅ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: confusion_matrices_STATUS_FIXED.png")

if __name__ == "__main__":
    main()