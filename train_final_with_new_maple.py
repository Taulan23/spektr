import os
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def load_spring_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–µ–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    for tree_type in tree_types:
        folder_path = os.path.join(base_path, tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(excel_files)} –≤–µ—Å–µ–Ω–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type}")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception as e:
                    continue
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} –≤–µ—Å–µ–Ω–Ω–∏—Ö —Å–ø–µ–∫—Ç—Ä–æ–≤")
    return all_data, all_labels

def load_summer_data_with_new_maple():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ + –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–ª–µ–Ω–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –ª–µ—Ç–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
    
    for tree_type in tree_types:
        if tree_type == '–∫–ª–µ–Ω':
            # –î–ª—è –∫–ª–µ–Ω–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ª–µ—Ç–Ω–∏–µ + –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–ª–µ–Ω–∞
            folder_path = os.path.join('.', tree_type)
            if os.path.exists(folder_path):
                excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
                print(f"–ù–∞–π–¥–µ–Ω–æ {len(excel_files)} –ª–µ—Ç–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type} (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ)")
                
                for file_path in excel_files:
                    try:
                        df = pd.read_excel(file_path)
                        if df.shape[1] >= 2:
                            spectrum_data = df.iloc[:, 1].values
                            if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                                spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                                if len(spectrum_data) > 10:
                                    all_data.append(spectrum_data)
                                    all_labels.append(tree_type)
                    except Exception as e:
                        continue
            
            # –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–ª–µ–Ω–∞ –∏–∑ –ø–∞–ø–∫–∏ –∫–ª–µ–Ω_–∞–º
            new_maple_path = "–∫–ª–µ–Ω_–∞–º"
            if os.path.exists(new_maple_path):
                excel_files = glob.glob(os.path.join(new_maple_path, '*.xlsx'))
                print(f"–ù–∞–π–¥–µ–Ω–æ {len(excel_files)} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type} (–∫–ª–µ–Ω_–∞–º)")
                
                for file_path in excel_files:
                    try:
                        df = pd.read_excel(file_path)
                        if df.shape[1] >= 2:
                            spectrum_data = df.iloc[:, 1].values
                            if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                                spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                                if len(spectrum_data) > 10:
                                    all_data.append(spectrum_data)
                                    all_labels.append(tree_type)
                    except Exception as e:
                        continue
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤ - –æ–±—ã—á–Ω—ã–µ –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
            folder_path = os.path.join('.', tree_type)
            if os.path.exists(folder_path):
                excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
                print(f"–ù–∞–π–¥–µ–Ω–æ {len(excel_files)} –ª–µ—Ç–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type}")
                
                for file_path in excel_files:
                    try:
                        df = pd.read_excel(file_path)
                        if df.shape[1] >= 2:
                            spectrum_data = df.iloc[:, 1].values
                            if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                                spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                                if len(spectrum_data) > 10:
                                    all_data.append(spectrum_data)
                                    all_labels.append(tree_type)
                    except Exception as e:
                        continue
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} –ª–µ—Ç–Ω–∏—Ö —Å–ø–µ–∫—Ç—Ä–æ–≤ (–≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–ª–µ–Ω–∞)")
    return all_data, all_labels

def extract_enhanced_features_v2(spectra):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω"""
    features = []
    
    # –ö–ª—é—á–µ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –¥—É–±–∞ –∏ –∫–ª–µ–Ω–∞ (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞)
    oak_channels = list(range(151, 161))  # 151-160
    maple_channels = list(range(172, 182)) + list(range(179, 186)) + [258, 276, 286]  # –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
    
    for spectrum in spectra:
        spectrum = np.array(spectrum)
        feature_vector = []
        
        # 1. –ò—Å—Ö–æ–¥–Ω—ã–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–∞—è –≤—ã–±–æ—Ä–∫–∞)
        feature_vector.extend(spectrum[::8])  # –∫–∞–∂–¥—ã–π 8-–π –∫–∞–Ω–∞–ª (–±—ã–ª–æ 10)
        
        # 2. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_vector.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.min(spectrum),
            np.max(spectrum),
            np.percentile(spectrum, 10),
            np.percentile(spectrum, 25),
            np.percentile(spectrum, 75),
            np.percentile(spectrum, 90),
            np.ptp(spectrum),  # —Ä–∞–∑–º–∞—Ö
            np.var(spectrum),  # –¥–∏—Å–ø–µ—Ä—Å–∏—è
        ])
        
        # 3. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        derivative = np.diff(spectrum)
        feature_vector.extend([
            np.mean(derivative),
            np.std(derivative),
            np.max(np.abs(derivative)),
            np.sum(derivative > 0),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏—Ö —Ç–æ—á–µ–∫
            np.sum(derivative < 0),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–±—ã–≤–∞—é—â–∏—Ö —Ç–æ—á–µ–∫
        ])
        
        # 4. –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥—É–±–∞
        if len(spectrum) > max(oak_channels):
            oak_region = spectrum[oak_channels]
            feature_vector.extend([
                np.mean(oak_region),
                np.std(oak_region),
                np.max(oak_region),
                np.min(oak_region),
                np.median(oak_region),
                np.ptp(oak_region)
            ])
        else:
            feature_vector.extend([0, 0, 0, 0, 0, 0])
        
        # 5. –£–°–ò–õ–ï–ù–ù–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞ (–∫–ª—é—á–µ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ!)
        valid_maple_channels = [ch for ch in maple_channels if ch < len(spectrum)]
        if valid_maple_channels:
            maple_region = spectrum[valid_maple_channels]
            feature_vector.extend([
                np.mean(maple_region),
                np.std(maple_region),
                np.max(maple_region),
                np.min(maple_region),
                np.median(maple_region),
                np.ptp(maple_region),
                np.percentile(maple_region, 25),
                np.percentile(maple_region, 75),
                np.var(maple_region),
                np.sum(maple_region > np.mean(spectrum)),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤—ã—à–µ –æ–±—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
            ])
        else:
            feature_vector.extend([0] * 10)
        
        # 6. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
        normalized_spectrum = spectrum / np.sum(spectrum) if np.sum(spectrum) > 0 else spectrum
        channels = np.arange(len(spectrum))
        
        # –¶–µ–Ω—Ç—Ä–æ–∏–¥ (—Å—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞)
        centroid = np.sum(channels * normalized_spectrum) if np.sum(normalized_spectrum) > 0 else 0
        feature_vector.append(centroid)
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞
        if np.sum(normalized_spectrum) > 0:
            spread = np.sqrt(np.sum(((channels - centroid) ** 2) * normalized_spectrum))
        else:
            spread = 0
        feature_vector.append(spread)
        
        # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è –∏ –∫—É—Ä—Ç–æ–∑–∏—Å
        if np.std(spectrum) > 0:
            skewness = np.mean(((spectrum - np.mean(spectrum)) / np.std(spectrum)) ** 3)
            kurtosis = np.mean(((spectrum - np.mean(spectrum)) / np.std(spectrum)) ** 4) - 3
        else:
            skewness = 0
            kurtosis = 0
        feature_vector.extend([skewness, kurtosis])
        
        # 7. –≠–Ω–µ—Ä–≥–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö (–±–æ–ª—å—à–µ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤)
        n_bands = 8  # –±—ã–ª–æ 5
        band_size = len(spectrum) // n_bands
        for i in range(n_bands):
            start_idx = i * band_size
            end_idx = min((i + 1) * band_size, len(spectrum))
            band_energy = np.sum(spectrum[start_idx:end_idx] ** 2)
            feature_vector.append(band_energy)
        
        features.append(feature_vector)
    
    return np.array(features)

def create_maple_focused_model(input_shape, num_classes):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ—Å–æ–±—ã–º —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∫–ª–µ–Ω–∞"""
    
    # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    inputs = layers.Input(shape=(input_shape,))
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –≤–µ—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    
    x = layers.Dense(512, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É—Ç–∏
    
    # –ü—É—Ç—å –¥–ª—è –æ–±—â–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    general_path = layers.Dense(256, activation='relu', name='general_path')(x)
    general_path = layers.BatchNormalization()(general_path)
    general_path = layers.Dropout(0.3)(general_path)
    general_path = layers.Dense(128, activation='relu')(general_path)
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è –∫–ª–µ–Ω–∞ (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ!)
    maple_path = layers.Dense(128, activation='relu', name='maple_path')(x)
    maple_path = layers.BatchNormalization()(maple_path)
    maple_path = layers.Dropout(0.2)(maple_path)
    maple_path = layers.Dense(64, activation='relu')(maple_path)
    
    # –ü—É—Ç—å –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≤–∏–¥–æ–≤ (–¥—É–±)
    oak_path = layers.Dense(64, activation='relu', name='oak_path')(x)
    oak_path = layers.Dropout(0.2)(oak_path)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—É—Ç–µ–π
    combined = layers.Concatenate()([general_path, maple_path, oak_path])
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏ —Å attention
    attention = layers.Dense(combined.shape[-1], activation='sigmoid')(combined)
    attended = layers.Multiply()([combined, attention])
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    output = layers.Dense(128, activation='relu')(attended)
    output = layers.Dropout(0.2)(output)
    output = layers.Dense(num_classes, activation='softmax')(output)
    
    model = keras.Model(inputs=inputs, outputs=output)
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # –º–µ–Ω—å—à–µ learning rate
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def analyze_final_results(y_test, y_pred, class_names, model_name="Final Model"):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print(f"\n" + "="*60)
    print(f"–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í - {model_name}")
    print("="*60)
    
    # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    accuracy = np.mean(y_test == y_pred)
    print(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)
    print("\n–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    print("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º):")
    print("–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞")
    print("-" * 60)
    
    for i, class_name in enumerate(class_names):
        row_sum = np.sum(cm_normalized[i])
        accuracy_class = cm_normalized[i][i]
        print(f"{class_name:>8}: —Ç–æ—á–Ω–æ—Å—Ç—å {accuracy_class:.3f} ({accuracy_class*100:.1f}%) | —Å—É–º–º–∞: {row_sum:.3f}")
    
    # –û—Å–æ–±—ã–π —Ñ–æ–∫—É—Å –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤–∏–¥—ã
    print("\nüéØ –§–û–ö–£–° –ù–ê –ü–†–û–ë–õ–ï–ú–ù–´–ï –í–ò–î–´:")
    problematic = ['–¥—É–±', '–∫–ª–µ–Ω']
    for species in problematic:
        if species in class_names:
            idx = list(class_names).index(species)
            correct = cm_normalized[idx][idx]
            total_samples = cm[idx].sum()
            
            if species == '–∫–ª–µ–Ω':
                print(f"üçÅ {species.upper()}: {correct:.3f} ({correct*100:.1f}%) –∏–∑ {total_samples} –æ–±—Ä–∞–∑—Ü–æ–≤")
                if correct > 0.5:
                    print("   ‚úÖ –ü–†–û–†–´–í! –ö–ª–µ–Ω —Ç–µ–ø–µ—Ä—å —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç—Å—è!")
                elif correct > 0.2:
                    print("   ‚ö° –ü–†–û–ì–†–ï–°–°! –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ!")
                else:
                    print("   ‚ùå –í—Å–µ –µ—â–µ –ø—Ä–æ–±–ª–µ–º—ã...")
            else:
                print(f"üå≥ {species.upper()}: {correct:.3f} ({correct*100:.1f}%) –∏–∑ {total_samples} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å'})
    
    plt.title(f'{model_name}\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}', fontsize=14)
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(f'final_confusion_matrix_{model_name.lower().replace(" ", "_")}.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    return report, cm, cm_normalized

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("–§–ò–ù–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ –° –ù–û–í–´–ú–ò –î–ê–ù–ù–´–ú–ò –ö–õ–ï–ù–ê")
    print("="*60)
    print("üçÅ –í–∫–ª—é—á–µ–Ω—ã –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–ª–µ–Ω–∞ –∏–∑ –ø–∞–ø–∫–∏ '–∫–ª–µ–Ω_–∞–º'")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_data, train_labels = load_spring_data()
    test_data, test_labels = load_summer_data_with_new_maple()
    
    if len(train_data) == 0 or len(test_data) == 0:
        print("–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        return
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–µ–Ω–∞–º
    maple_count = test_labels.count('–∫–ª–µ–Ω')
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–µ–Ω—É –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {maple_count} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    all_spectra = train_data + test_data
    min_length = min(len(spectrum) for spectrum in all_spectra)
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    train_data_trimmed = [spectrum[:min_length] for spectrum in train_data]
    test_data_trimmed = [spectrum[:min_length] for spectrum in test_data]
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    X_train_features = extract_enhanced_features_v2(train_data_trimmed)
    X_test_features = extract_enhanced_features_v2(test_data_trimmed)
    
    print(f"–§–æ—Ä–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_features.shape}")
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(train_labels)
    y_test = label_encoder.transform(test_labels)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_features)
    X_test_scaled = scaler.transform(X_test_features)
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_scaled.shape}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_scaled.shape}")
    
    # Random Forest –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("\n1. Random Forest (–±–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è)...")
    rf = RandomForestClassifier(
        n_estimators=300,
        max_depth=25,
        min_samples_split=3,
        min_samples_leaf=1,
        random_state=42,
        n_jobs=-1
    )
    rf.fit(X_train_scaled, y_train)
    rf_pred = rf.predict(X_test_scaled)
    
    analyze_final_results(y_test, rf_pred, label_encoder.classes_, "Random Forest")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
    print("\n2. –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª–µ–Ω...")
    final_model = create_maple_focused_model(X_train_scaled.shape[1], len(label_encoder.classes_))
    
    print("\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
    final_model.summary()
    
    # Callback'–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_accuracy', patience=30, restore_best_weights=True, verbose=1
    )
    
    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.7, patience=15, min_lr=1e-7, verbose=1
    )
    
    model_checkpoint = keras.callbacks.ModelCheckpoint(
        'best_final_maple_model.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    history = final_model.fit(
        X_train_scaled, y_train,
        batch_size=32,
        epochs=200,
        validation_data=(X_test_scaled, y_test),
        callbacks=[early_stopping, reduce_lr, model_checkpoint],
        verbose=1
    )
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    final_loss, final_accuracy = final_model.evaluate(X_test_scaled, y_test, verbose=0)
    final_pred = np.argmax(final_model.predict(X_test_scaled, verbose=0), axis=1)
    
    analyze_final_results(y_test, final_pred, label_encoder.classes_, "Final Neural Network")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    rf_accuracy = np.mean(y_test == rf_pred)
    
    print("\n" + "="*60)
    print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï")
    print("="*60)
    print(f"Random Forest:     {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)")
    print(f"Final NN Model:    {final_accuracy:.4f} ({final_accuracy*100:.2f}%)")
    
    improvement = final_accuracy - rf_accuracy
    if improvement > 0:
        print(f"–£–ª—É—á—à–µ–Ω–∏–µ NN:      +{improvement:.4f} (+{improvement*100:.2f}%)")
    else:
        print(f"–†–∞–∑–Ω–∏—Ü–∞:           {improvement:.4f} ({improvement*100:.2f}%)")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    import joblib
    joblib.dump(scaler, 'final_scaler.pkl')
    joblib.dump(label_encoder, 'final_label_encoder.pkl') 
    joblib.dump(rf, 'final_rf_model.pkl')
    final_model.save('final_neural_network.keras')
    
    print("\nüéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print("–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print("- best_final_maple_model.keras")
    print("- final_neural_network.keras")
    print("- final_rf_model.pkl")
    print("- final_scaler.pkl")
    print("- final_label_encoder.pkl")

if __name__ == "__main__":
    main() 