import os
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

def load_spring_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join(base_path, tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(excel_files)} –≤–µ—Å–µ–Ω–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type}")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def load_summer_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join('.', tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(excel_files)} –ª–µ—Ç–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è {tree_type}")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def extract_ultimate_features(spectra):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–∏–¥–æ–≤"""
    features = []
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≤–∏–¥–æ–≤
    maple_channels_1 = list(range(170, 190))   # –ö–ª–µ–Ω –æ–±–ª–∞—Å—Ç—å 1
    maple_channels_2 = list(range(250, 290))   # –ö–ª–µ–Ω –æ–±–ª–∞—Å—Ç—å 2
    maple_key_peaks = [179, 180, 181, 258, 276, 286]  # –ö–ª—é—á–µ–≤—ã–µ –ø–∏–∫–∏ –∫–ª–µ–Ω–∞
    oak_channels = list(range(149, 165))       # –î—É–± —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
    
    for spectrum in spectra:
        spectrum = np.array(spectrum)
        feature_vector = []
        
        # 1. –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        feature_vector.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.max(spectrum),
            np.min(spectrum),
            np.ptp(spectrum),
            np.var(spectrum),
            np.sqrt(np.mean(spectrum**2)),  # RMS
            np.mean(np.abs(spectrum)),      # MAD
        ])
        
        # 2. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
        for p in [5, 10, 15, 25, 35, 50, 65, 75, 85, 90, 95]:
            feature_vector.append(np.percentile(spectrum, p))
        
        # 3. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤—Å–µ—Ö –ø–æ—Ä—è–¥–∫–æ–≤
        for order in [1, 2, 3]:
            if order == 1:
                derivative = np.diff(spectrum)
            elif order == 2:
                derivative = np.diff(np.diff(spectrum))
            else:
                derivative = np.diff(np.diff(np.diff(spectrum)))
            
            if len(derivative) > 0:
                feature_vector.extend([
                    np.mean(derivative),
                    np.std(derivative),
                    np.max(np.abs(derivative)),
                    np.min(derivative),
                    np.max(derivative),
                ])
            else:
                feature_vector.extend([0] * 5)
        
        # 4. –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞ - –æ–±–ª–∞—Å—Ç—å 1
        valid_maple_1 = [ch for ch in maple_channels_1 if ch < len(spectrum)]
        if valid_maple_1:
            maple_region1 = spectrum[valid_maple_1]
            avg_spectrum = np.mean(spectrum)
            max_spectrum = np.max(spectrum)
            feature_vector.extend([
                np.mean(maple_region1),
                np.std(maple_region1),
                np.max(maple_region1),
                np.min(maple_region1),
                np.ptp(maple_region1),
                np.median(maple_region1),
                np.var(maple_region1),
                np.mean(maple_region1) / avg_spectrum if avg_spectrum > 0 else 0,
                np.std(maple_region1) / avg_spectrum if avg_spectrum > 0 else 0,
                np.max(maple_region1) / max_spectrum if max_spectrum > 0 else 0,
                np.sum(maple_region1 > np.mean(maple_region1)),
                np.sum(maple_region1 > np.percentile(spectrum, 75)),
                np.sum(maple_region1 < np.percentile(spectrum, 25)),
                np.corrcoef(maple_region1, np.arange(len(maple_region1)))[0,1] if len(maple_region1) > 1 else 0,
                np.sum(np.diff(maple_region1) > 0),  # –í–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏–µ —É—á–∞—Å—Ç–∫–∏
            ])
        else:
            feature_vector.extend([0] * 15)
        
        # 5. –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–µ–Ω–∞ - –æ–±–ª–∞—Å—Ç—å 2
        valid_maple_2 = [ch for ch in maple_channels_2 if ch < len(spectrum)]
        if valid_maple_2:
            maple_region2 = spectrum[valid_maple_2]
            feature_vector.extend([
                np.mean(maple_region2),
                np.std(maple_region2),
                np.max(maple_region2),
                np.min(maple_region2),
                np.ptp(maple_region2),
                np.mean(maple_region2) / np.mean(spectrum) if np.mean(spectrum) > 0 else 0,
                len(maple_region2[maple_region2 > np.percentile(spectrum, 80)]),
                len(maple_region2[maple_region2 < np.percentile(spectrum, 20)]),
                np.trapz(maple_region2),  # –ò–Ω—Ç–µ–≥—Ä–∞–ª –æ–±–ª–∞—Å—Ç–∏
                np.sum(np.diff(maple_region2) < 0),  # –£–±—ã–≤–∞—é—â–∏–µ —É—á–∞—Å—Ç–∫–∏
            ])
        else:
            feature_vector.extend([0] * 10)
        
        # 6. –ö–ª—é—á–µ–≤—ã–µ –ø–∏–∫–∏ –∫–ª–µ–Ω–∞
        valid_peaks = [ch for ch in maple_key_peaks if ch < len(spectrum)]
        if valid_peaks:
            maple_peaks = spectrum[valid_peaks]
            feature_vector.extend([
                np.mean(maple_peaks),
                np.max(maple_peaks),
                np.min(maple_peaks),
                np.sum(maple_peaks),
                np.std(maple_peaks),
                np.median(maple_peaks),
                np.mean(maple_peaks) / np.mean(spectrum) if np.mean(spectrum) > 0 else 0,
            ])
        else:
            feature_vector.extend([0] * 7)
        
        # 7. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥—É–±–∞
        valid_oak = [ch for ch in oak_channels if ch < len(spectrum)]
        if valid_oak:
            oak_region = spectrum[valid_oak]
            feature_vector.extend([
                np.mean(oak_region),
                np.std(oak_region),
                np.max(oak_region),
                np.min(oak_region),
                np.ptp(oak_region),
                np.mean(oak_region) / np.mean(spectrum) if np.mean(spectrum) > 0 else 0,
                np.sum(oak_region > np.mean(oak_region)),
                np.trapz(oak_region),
                np.var(oak_region),
                np.corrcoef(oak_region, np.arange(len(oak_region)))[0,1] if len(oak_region) > 1 else 0,
            ])
        else:
            feature_vector.extend([0] * 10)
        
        # 8. –≠–Ω–µ—Ä–≥–∏—è –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
        n_bands = 10
        band_size = len(spectrum) // n_bands
        for i in range(n_bands):
            start_idx = i * band_size
            end_idx = min((i + 1) * band_size, len(spectrum))
            if start_idx < len(spectrum):
                band = spectrum[start_idx:end_idx]
                band_energy = np.sum(band ** 2)
                band_mean = np.mean(band)
                feature_vector.extend([band_energy, band_mean])
            else:
                feature_vector.extend([0, 0])
        
        # 9. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        normalized_spectrum = spectrum / np.sum(spectrum) if np.sum(spectrum) > 0 else spectrum
        channels = np.arange(len(spectrum))
        if np.sum(normalized_spectrum) > 0:
            centroid = np.sum(channels * normalized_spectrum)
            spread = np.sqrt(np.sum(((channels - centroid) ** 2) * normalized_spectrum))
            skewness = np.sum(((channels - centroid) ** 3) * normalized_spectrum) / (spread ** 3) if spread > 0 else 0
            kurtosis = np.sum(((channels - centroid) ** 4) * normalized_spectrum) / (spread ** 4) if spread > 0 else 0
            feature_vector.extend([centroid, spread, skewness, kurtosis])
        else:
            feature_vector.extend([0, 0, 0, 0])
        
        # 10. –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏ —Å–ø–µ–∫—Ç—Ä–∞
        quarter = len(spectrum) // 4
        q1 = np.mean(spectrum[:quarter])
        q2 = np.mean(spectrum[quarter:2*quarter])
        q3 = np.mean(spectrum[2*quarter:3*quarter])
        q4 = np.mean(spectrum[3*quarter:])
        
        feature_vector.extend([
            q1/q2 if q2 > 0 else 0,
            q2/q3 if q3 > 0 else 0,
            q3/q4 if q4 > 0 else 0,
            q1/q4 if q4 > 0 else 0,
            (q1+q4)/(q2+q3) if (q2+q3) > 0 else 0,
        ])
        
        # 11. –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å—Ç—Ä–µ–º—É–º—ã
        from scipy.signal import find_peaks
        try:
            peaks, _ = find_peaks(spectrum, height=np.percentile(spectrum, 60))
            valleys, _ = find_peaks(-spectrum, height=-np.percentile(spectrum, 40))
            feature_vector.extend([
                len(peaks),
                len(valleys),
                np.mean(spectrum[peaks]) if len(peaks) > 0 else 0,
                np.mean(spectrum[valleys]) if len(valleys) > 0 else 0,
            ])
        except:
            feature_vector.extend([0, 0, 0, 0])
        
        features.append(feature_vector)
    
    return np.array(features)

def create_seasonal_adaptation_model(input_shape, num_classes):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —Å–µ–∑–æ–Ω–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –¥–ª—è –∫–ª–µ–Ω–∞"""
    
    # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    inputs = layers.Input(shape=(input_shape,))
    
    # –ë–∞–∑–æ–≤–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    x = layers.Dense(512, activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    
    x = layers.Dense(256, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    
    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –≤–∏–¥–∞
    
    # –í–µ—Ç–≤—å –¥–ª—è –∫–ª–µ–Ω–∞ (—Å–µ–∑–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è)
    maple_branch = layers.Dense(128, activation='relu', name='maple_specialization')(x)
    maple_branch = layers.BatchNormalization()(maple_branch)
    maple_branch = layers.Dropout(0.3)(maple_branch)
    maple_branch = layers.Dense(64, activation='relu')(maple_branch)
    maple_branch = layers.Dropout(0.2)(maple_branch)
    
    # –í–µ—Ç–≤—å –¥–ª—è –¥—É–±–∞
    oak_branch = layers.Dense(64, activation='relu', name='oak_specialization')(x)
    oak_branch = layers.BatchNormalization()(oak_branch)
    oak_branch = layers.Dropout(0.2)(oak_branch)
    
    # –û–±—â–∞—è –≤–µ—Ç–≤—å –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–¥–æ–≤
    general_branch = layers.Dense(256, activation='relu', name='general_branch')(x)
    general_branch = layers.BatchNormalization()(general_branch)
    general_branch = layers.Dropout(0.3)(general_branch)
    general_branch = layers.Dense(128, activation='relu')(general_branch)
    general_branch = layers.Dropout(0.2)(general_branch)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Ç–≤–µ–π
    combined = layers.Concatenate()([maple_branch, oak_branch, general_branch])
    
    # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ñ–æ–∫—É—Å–∞ –Ω–∞ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    attention = layers.Dense(combined.shape[-1], activation='sigmoid', name='attention')(combined)
    attended = layers.Multiply()([combined, attention])
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
    output = layers.Dense(256, activation='relu')(attended)
    output = layers.BatchNormalization()(output)
    output = layers.Dropout(0.3)(output)
    
    output = layers.Dense(128, activation='relu')(output)
    output = layers.Dropout(0.2)(output)
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    predictions = layers.Dense(num_classes, activation='softmax', name='predictions')(output)
    
    model = keras.Model(inputs=inputs, outputs=predictions)
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º learning rate
    optimizer = keras.optimizers.Adam(
        learning_rate=0.001,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07
    )
    
    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def create_mega_ensemble(X_train, y_train, X_test, y_test, class_names):
    """–°–æ–∑–¥–∞–µ—Ç –º–µ–≥–∞-–∞–Ω—Å–∞–º–±–ª—å –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π"""
    print("\nüöÄ –°–û–ó–î–ê–ù–ò–ï –ú–ï–ì–ê-–ê–ù–°–ê–ú–ë–õ–Ø –î–õ–Ø –í–°–ï–• –í–ò–î–û–í")
    
    models = []
    predictions = []
    maple_idx = list(class_names).index('–∫–ª–µ–Ω')
    oak_idx = list(class_names).index('–¥—É–±')
    
    # 1. –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π Random Forest –¥–ª—è –∫–ª–µ–Ω–∞
    print("–û–±—É—á–µ–Ω–∏–µ Extreme Random Forest...")
    rf_extreme = ExtraTreesClassifier(
        n_estimators=1000,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42,
        n_jobs=-1,
        class_weight={maple_idx: 50.0, oak_idx: 10.0}  # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
    )
    
    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤
    sample_weights = np.ones(len(y_train))
    sample_weights[y_train == maple_idx] = 100.0
    sample_weights[y_train == oak_idx] = 20.0
    
    rf_extreme.fit(X_train, y_train, sample_weight=sample_weights)
    pred1 = rf_extreme.predict(X_test)
    predictions.append(pred1)
    models.append(('Extreme RF', rf_extreme))
    
    # 2. Gradient Boosting —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –≤–∏–¥—ã
    print("–û–±—É—á–µ–Ω–∏–µ Gradient Boosting...")
    gb_model = GradientBoostingClassifier(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=10,
        random_state=42
    )
    gb_model.fit(X_train, y_train, sample_weight=sample_weights)
    pred2 = gb_model.predict(X_test)
    predictions.append(pred2)
    models.append(('Gradient Boosting', gb_model))
    
    # 3. SVM —Å RBF —è–¥—Ä–æ–º
    print("–û–±—É—á–µ–Ω–∏–µ SVM...")
    svm_model = SVC(
        C=100.0,
        gamma='scale',
        kernel='rbf',
        class_weight={maple_idx: 100.0, oak_idx: 20.0},
        probability=True,
        random_state=42
    )
    svm_model.fit(X_train, y_train, sample_weight=sample_weights)
    pred3 = svm_model.predict(X_test)
    predictions.append(pred3)
    models.append(('SVM RBF', svm_model))
    
    # 4. –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å —Å–µ–∑–æ–Ω–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
    print("–û–±—É—á–µ–Ω–∏–µ Seasonal Adaptation Neural Network...")
    nn_model = create_seasonal_adaptation_model(X_train.shape[1], len(class_names))
    
    # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = {i: 1.0 for i in range(len(class_names))}
    class_weights[maple_idx] = 100.0
    class_weights[oak_idx] = 20.0
    
    # Callbacks
    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=20, restore_best_weights=True
    )
    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.7, patience=10, min_lr=1e-6
    )
    
    history = nn_model.fit(
        X_train, y_train,
        epochs=200,
        batch_size=8,  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        validation_split=0.2,
        class_weight=class_weights,
        callbacks=[early_stopping, reduce_lr],
        verbose=0
    )
    
    pred4 = np.argmax(nn_model.predict(X_test, verbose=0), axis=1)
    predictions.append(pred4)
    models.append(('Seasonal NN', nn_model))
    
    # 5. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –≤–∏–¥–∞
    print("–û–±—É—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤...")
    
    # –î–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–µ–Ω–∞
    maple_detector = ExtraTreesClassifier(
        n_estimators=500,
        max_depth=20,
        random_state=43,
        class_weight={1: 200.0, 0: 1.0},
        n_jobs=-1
    )
    y_maple_binary = (y_train == maple_idx).astype(int)
    maple_weights = np.ones(len(y_train))
    maple_weights[y_train == maple_idx] = 200.0
    maple_detector.fit(X_train, y_maple_binary, sample_weight=maple_weights)
    maple_proba = maple_detector.predict_proba(X_test)[:, 1]
    
    # –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–∞
    oak_detector = ExtraTreesClassifier(
        n_estimators=500,
        max_depth=20,
        random_state=44,
        class_weight={1: 50.0, 0: 1.0},
        n_jobs=-1
    )
    y_oak_binary = (y_train == oak_idx).astype(int)
    oak_weights = np.ones(len(y_train))
    oak_weights[y_train == oak_idx] = 50.0
    oak_detector.fit(X_train, y_oak_binary, sample_weight=oak_weights)
    oak_proba = oak_detector.predict_proba(X_test)[:, 1]
    
    # –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    print("–°–æ–∑–¥–∞–Ω–∏–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    
    predictions_array = np.array(predictions)
    final_predictions = []
    
    for i in range(len(X_test)):
        # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–µ–Ω–∞ –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω (–ø–æ—Ä–æ–≥ 0.2), —Ñ–æ—Ä—Å–∏—Ä—É–µ–º –∫–ª–µ–Ω
        if maple_proba[i] > 0.2:
            final_predictions.append(maple_idx)
        # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–∞ —É–≤–µ—Ä–µ–Ω (–ø–æ—Ä–æ–≥ 0.3), —Ñ–æ—Ä—Å–∏—Ä—É–µ–º –¥—É–±
        elif oak_proba[i] > 0.3:
            final_predictions.append(oak_idx)
        else:
            # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            votes = predictions_array[:, i]
            
            # –ü–æ–¥—Å—á–µ—Ç –≥–æ–ª–æ—Å–æ–≤ —Å –≤–µ—Å–∞–º–∏
            vote_weights = {
                0: 3.0,  # Extreme RF
                1: 2.0,  # Gradient Boosting  
                2: 2.0,  # SVM
                3: 4.0,  # Seasonal NN (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å)
            }
            
            weighted_votes = {}
            for vote_idx, vote in enumerate(votes):
                weight = vote_weights.get(vote_idx, 1.0)
                if vote in weighted_votes:
                    weighted_votes[vote] += weight
                else:
                    weighted_votes[vote] = weight
            
            # –í—ã–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –≤–µ—Å–æ–º
            best_class = max(weighted_votes, key=weighted_votes.get)
            final_predictions.append(best_class)
    
    final_predictions = np.array(final_predictions)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¢–î–ï–õ–¨–ù–´–• –ú–û–î–ï–õ–ï–ô:")
    for name, _ in models:
        idx = [i for i, (n, _) in enumerate(models) if n == name][0]
        acc = np.mean(y_test == predictions_array[idx])
        maple_acc = np.mean(y_test[y_test == maple_idx] == predictions_array[idx][y_test == maple_idx]) if np.sum(y_test == maple_idx) > 0 else 0
        oak_acc = np.mean(y_test[y_test == oak_idx] == predictions_array[idx][y_test == oak_idx]) if np.sum(y_test == oak_idx) > 0 else 0
        print(f"  {name}: –æ–±—â–∞—è {acc:.3f}, –∫–ª–µ–Ω {maple_acc:.3f}, –¥—É–± {oak_acc:.3f}")
    
    # –î–µ—Ç–µ–∫—Ç–æ—Ä—ã
    maple_recall = np.sum((y_test == maple_idx) & (maple_proba > 0.2)) / np.sum(y_test == maple_idx) if np.sum(y_test == maple_idx) > 0 else 0
    oak_recall = np.sum((y_test == oak_idx) & (oak_proba > 0.3)) / np.sum(y_test == oak_idx) if np.sum(y_test == oak_idx) > 0 else 0
    print(f"  –î–µ—Ç–µ–∫—Ç–æ—Ä –∫–ª–µ–Ω–∞: recall {maple_recall:.3f} (–ø–æ—Ä–æ–≥ 0.2)")
    print(f"  –î–µ—Ç–µ–∫—Ç–æ—Ä –¥—É–±–∞: recall {oak_recall:.3f} (–ø–æ—Ä–æ–≥ 0.3)")
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    final_acc = np.mean(y_test == final_predictions)
    final_maple_acc = np.mean(y_test[y_test == maple_idx] == final_predictions[y_test == maple_idx]) if np.sum(y_test == maple_idx) > 0 else 0
    final_oak_acc = np.mean(y_test[y_test == oak_idx] == final_predictions[y_test == oak_idx]) if np.sum(y_test == oak_idx) > 0 else 0
    
    print(f"\nüéØ –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ê–ù–°–ê–ú–ë–õ–¨:")
    print(f"  –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_acc:.3f}")
    print(f"  –ö–ª–µ–Ω: {final_maple_acc:.3f}")
    print(f"  –î—É–±: {final_oak_acc:.3f}")
    
    return final_predictions, final_maple_acc, final_oak_acc, models

def analyze_ultimate_solution(y_test, y_pred, class_names, title):
    """–û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ—à–µ–Ω–∏—è"""
    print(f"\n{'='*70}")
    print(f"üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó: {title}")
    print("="*70)
    
    accuracy = np.mean(y_test == y_pred)
    print(f"üéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report = classification_report(y_test, y_pred, target_names=class_names, digits=4)
    print("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    print(report)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –í–°–ï–ú –í–ò–î–ê–ú:")
    success_count = 0
    for i, species in enumerate(class_names):
        correct = cm_normalized[i][i]
        total = cm[i].sum()
        
        if correct > 0.5:
            status = "üéâ –û–¢–õ–ò–ß–ù–û"
            success_count += 1
        elif correct > 0.3:
            status = "‚ö° –•–û–†–û–®–û"
            success_count += 1
        elif correct > 0.1:
            status = "üìà –ü–†–ò–ï–ú–õ–ï–ú–û"
        else:
            status = "‚ùå –ü–†–û–ë–õ–ï–ú–ê"
        
        print(f"  {species.upper()}: {correct:.3f} ({correct*100:.1f}%) –∏–∑ {total} –æ–±—Ä–∞–∑—Ü–æ–≤ - {status}")
    
    print(f"\n‚úÖ –£–°–ü–ï–®–ù–û –†–ê–°–ü–û–ó–ù–ê–ï–¢–°–Ø: {success_count}/7 –≤–∏–¥–æ–≤")
    
    if success_count == 7:
        print("üèÜ –ü–û–õ–ù–´–ô –£–°–ü–ï–•! –í–°–ï –í–ò–î–´ –†–ê–°–ü–û–ó–ù–ê–Æ–¢–°–Ø!")
    elif success_count >= 6:
        print("‚ö° –ü–û–ß–¢–ò –ò–î–ï–ê–õ–¨–ù–û! –û—Å—Ç–∞–ª–æ—Å—å –¥–æ–≤–µ—Å—Ç–∏ 1-2 –≤–∏–¥–∞")
    elif success_count >= 5:
        print("üìà –•–û–†–û–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤–∏–¥–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    else:
        print("‚ùå –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –†–ê–ë–û–¢–ê")
    
    return accuracy, cm_normalized, success_count

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –í–°–ï–• –≤–∏–¥–æ–≤"""
    print("üî•üî•üî• –ê–ì–†–ï–°–°–ò–í–ù–û–ï –†–ï–®–ï–ù–ò–ï –î–õ–Ø –í–°–ï–• 7 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í üî•üî•üî•")
    print("="*80)
    print("üéØ –¶–ï–õ–¨: 100% —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –≤–∏–¥–æ–≤ –≤–∫–ª—é—á–∞—è –∫–ª–µ–Ω –∏ –¥—É–±")
    print("üöÄ –ú–ï–¢–û–î–´: –ú–µ–≥–∞-–∞–Ω—Å–∞–º–±–ª—å + –°–µ–∑–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è + –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞")
    print("="*80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüì• –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•...")
    train_data, train_labels = load_spring_data()
    test_data, test_labels = load_summer_data()
    
    print(f"‚úÖ –í–µ—Å–µ–Ω–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(train_data)}")
    print(f"‚úÖ –õ–µ—Ç–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(test_data)}")
    
    for species in ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']:
        spring_count = train_labels.count(species)
        summer_count = test_labels.count(species)
        print(f"  {species}: –≤–µ—Å–Ω–∞ {spring_count}, –ª–µ—Ç–æ {summer_count}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("\nüîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê...")
    all_spectra = train_data + test_data
    min_length = min(len(spectrum) for spectrum in all_spectra)
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    train_data_trimmed = [spectrum[:min_length] for spectrum in train_data]
    test_data_trimmed = [spectrum[:min_length] for spectrum in test_data]
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüß† –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ì–û –ö–û–õ–ò–ß–ï–°–¢–í–ê –ü–†–ò–ó–ù–ê–ö–û–í...")
    X_train = extract_ultimate_features(train_data_trimmed)
    X_test = extract_ultimate_features(test_data_trimmed)
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {X_train.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤!")
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(train_labels)
    y_test = label_encoder.transform(test_labels)
    
    # –†–æ–±–∞—Å—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ª—É—á—à–µ –¥–ª—è –≤—ã–±—Ä–æ—Å–æ–≤)
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–≥–∞-–∞–Ω—Å–∞–º–±–ª—è
    final_pred, maple_acc, oak_acc, models = create_mega_ensemble(
        X_train_scaled, y_train, X_test_scaled, y_test, label_encoder.classes_
    )
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    accuracy, cm_norm, success_count = analyze_ultimate_solution(
        y_test, final_pred, label_encoder.classes_, "–ú–ï–ì–ê-–ê–ù–°–ê–ú–ë–õ–¨ –î–õ–Ø –í–°–ï–• –í–ò–î–û–í"
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô...")
    import joblib
    
    joblib.dump(scaler, 'ultimate_scaler.pkl')
    joblib.dump(label_encoder, 'ultimate_label_encoder.pkl')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for i, (name, model) in enumerate(models):
        if 'NN' in name:
            model.save(f'ultimate_model_{i}_{name.replace(" ", "_")}.keras')
        else:
            joblib.dump(model, f'ultimate_model_{i}_{name.replace(" ", "_")}.pkl')
    
    print("\nüéâ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("="*50)
    print(f"üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)")
    print(f"üçÅ –ö–ª–µ–Ω: {maple_acc:.3f} ({maple_acc*100:.1f}%)")
    print(f"üå≥ –î—É–±: {oak_acc:.3f} ({oak_acc*100:.1f}%)")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –≤–∏–¥–æ–≤: {success_count}/7")
    
    if success_count == 7:
        print("\nüèÜüèÜüèÜ –ú–ò–°–°–ò–Ø –í–´–ü–û–õ–ù–ï–ù–ê! –í–°–ï –í–ò–î–´ –†–ê–°–ü–û–ó–ù–ê–Æ–¢–°–Ø! üèÜüèÜüèÜ")
    elif success_count >= 6:
        print("\n‚ö°‚ö°‚ö° –ü–û–ß–¢–ò –ò–î–ï–ê–õ–¨–ù–û! –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢! ‚ö°‚ö°‚ö°")
    else:
        print(f"\nüìà –•–û–†–û–®–ò–ô –ü–†–û–ì–†–ï–°–°! {success_count} –∏–∑ 7 –≤–∏–¥–æ–≤ —Ä–∞–±–æ—Ç–∞—é—Ç")
    
    print("\nüéØ –†–ï–®–ï–ù–ò–ï –î–õ–Ø –í–°–ï–• –í–ò–î–û–í –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main() 