#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EXTRA TREES –° –û–ë–£–ß–ï–ù–ò–ï–ú –ù–ê –ó–ê–®–£–ú–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–• (DATA AUGMENTATION)
1712 –¥–µ—Ä–µ–≤—å–µ–≤, max_depth=None
–û–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –∫–∞–∫ —á–∏—Å—Ç—ã–µ, —Ç–∞–∫ –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ —Å–ø–µ–∫—Ç—Ä—ã
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.feature_extraction import FeatureHasher
import os
import glob
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_20_species_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ 20 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤ –∏–∑ –ø–∞–ø–∫–∏ '–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤'"""
    
    print("üìÅ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 20 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í...")
    
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    all_data = []
    all_labels = []
    species_counts = {}
    
    if not os.path.exists(base_path):
        print(f"‚ùå –ü–∞–ø–∫–∞ '{base_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return [], [], {}
    
    # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–∞–ø–æ–∫ —Å –≤–∏–¥–∞–º–∏
    species_folders = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
    species_folders.sort()
    
    print(f"   üå≥ –ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫ —Å –≤–∏–¥–∞–º–∏: {len(species_folders)}")
    
    for species_folder in species_folders:
        species_path = os.path.join(base_path, species_folder)
        excel_files = glob.glob(os.path.join(species_path, "*.xlsx"))
        
        if len(excel_files) == 0:
            print(f"   ‚ö†Ô∏è  –í –ø–∞–ø–∫–µ '{species_folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ Excel —Ñ–∞–π–ª–æ–≤")
            continue
        
        print(f"   üìä {species_folder}: {len(excel_files)} —Ñ–∞–π–ª–æ–≤")
        species_counts[species_folder] = len(excel_files)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        for excel_file in excel_files:
            try:
                df = pd.read_excel(excel_file)
                
                # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                spectral_columns = [col for col in df.columns if isinstance(col, (int, float)) or 
                                  (isinstance(col, str) and col.replace('.', '').isdigit())]
                
                if len(spectral_columns) > 0:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    spectrum = df[spectral_columns[0]].values
                    
                    # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
                    spectrum = spectrum[~np.isnan(spectrum)]
                    
                    if len(spectrum) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞
                        all_data.append(spectrum)
                        all_labels.append(species_folder)
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {excel_file}: {e}")
    
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–ø–µ–∫—Ç—Ä–æ–≤: {len(all_data)}")
    print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤: {len(set(all_labels))}")
    
    for species, count in species_counts.items():
        actual_count = all_labels.count(species)
        print(f"   ‚Ä¢ {species}: {actual_count}/{count} —Ñ–∞–π–ª–æ–≤")
    
    return all_data, all_labels, species_counts

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("üîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –°–ü–ï–ö–¢–†–û–í...")
    
    processed_spectra = []
    
    for i, spectrum in enumerate(spectra_list):
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if np.std(spectrum) > 0:
            normalized = (spectrum - np.mean(spectrum)) / np.std(spectrum)
        else:
            normalized = spectrum
        
        processed_spectra.append(normalized)
        
        if (i + 1) % 100 == 0:
            print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(spectra_list)}")
    
    print(f"   ‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(processed_spectra)} —Å–ø–µ–∫—Ç—Ä–æ–≤")
    
    return processed_spectra

def extract_enhanced_features(X):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("üîç –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í...")
    
    features = []
    
    for spectrum in X:
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        mean_val = np.mean(spectrum)
        std_val = np.std(spectrum)
        min_val = np.min(spectrum)
        max_val = np.max(spectrum)
        range_val = max_val - min_val
        
        # –ü–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏
        percentiles = np.percentile(spectrum, [10, 25, 50, 75, 90])
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
        skewness = np.mean(((spectrum - mean_val) / std_val) ** 3) if std_val > 0 else 0
        kurtosis = np.mean(((spectrum - mean_val) / std_val) ** 4) if std_val > 0 else 0
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        spectral_centroid = np.sum(spectrum * np.arange(len(spectrum))) / np.sum(spectrum) if np.sum(spectrum) != 0 else 0
        spectral_bandwidth = np.sqrt(np.sum(((np.arange(len(spectrum)) - spectral_centroid) ** 2) * spectrum) / np.sum(spectrum)) if np.sum(spectrum) != 0 else 0
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_vector = [
            mean_val, std_val, min_val, max_val, range_val,
            *percentiles, skewness, kurtosis, spectral_centroid, spectral_bandwidth
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ (–∫–∞–∂–¥—ã–π 10-–π –æ—Ç—Å—á–µ—Ç)
        spectrum_sampled = spectrum[::10]
        feature_vector.extend(spectrum_sampled)
        
        features.append(feature_vector)
    
    print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features[0])}")
    
    return np.array(features)

def add_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∞–¥–¥–∏—Ç–∏–≤–Ω—ã–π –≥–∞—É—Å—Å–æ–≤ —à—É–º"""
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

def create_augmented_training_data(X_train, y_train, noise_levels=[0.01, 0.05, 0.10]):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É —Å –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    print("üîÑ –°–û–ó–î–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–û–ô –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò...")
    
    X_augmented = [X_train]  # –ù–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    y_augmented = [y_train]
    
    for noise_level in noise_levels:
        print(f"   üìä –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å {noise_level*100:.0f}% —à—É–º–æ–º...")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
        X_noisy = add_noise(X_train, noise_level)
        
        X_augmented.append(X_noisy)
        y_augmented.append(y_train)  # –¢–µ –∂–µ –º–µ—Ç–∫–∏
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    X_combined = np.vstack(X_augmented)
    y_combined = np.concatenate(y_augmented)
    
    print(f"   üìà –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   üìà –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {len(X_combined)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   üìà –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: {len(X_combined)/len(X_train):.1f}x")
    
    return X_combined, y_combined

def train_extra_trees_1712_with_noise_augmentation(X_train, y_train):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Extra Trees —Å 1712 –¥–µ—Ä–µ–≤—å—è–º–∏ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    print("üå≥ –û–ë–£–ß–ï–ù–ò–ï EXTRA TREES –° 1712 –î–ï–†–ï–í–¨–Ø–ú–ò –ò DATA AUGMENTATION...")
    print("   üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: n_estimators=1712, max_depth=None")
    print("   üîÑ –û–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç —á–∏—Å—Ç—ã–µ –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É
    X_augmented, y_augmented = create_augmented_training_data(X_train, y_train)
    
    # –ú–æ–¥–µ–ª—å Extra Trees —Å –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = ExtraTreesClassifier(
        n_estimators=1712,  # –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
        max_depth=None,     # –ó–∞–ø—Ä–æ—à–µ–Ω–Ω–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    
    print(f"   üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"      - n_estimators: {model.n_estimators}")
    print(f"      - max_depth: {model.max_depth}")
    print(f"      - min_samples_split: {model.min_samples_split}")
    print(f"      - min_samples_leaf: {model.min_samples_leaf}")
    print(f"      - max_features: {model.max_features}")
    print(f"      - –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_augmented.shape}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("   üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    model.fit(X_augmented, y_augmented)
    
    print("   ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    return model

def evaluate_model_with_noise(model, X_test, y_test, species_names, noise_levels=[0, 1, 5, 10]):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —à—É–º–∞"""
    
    print("üîç –ê–ù–ê–õ–ò–ó –£–°–¢–û–ô–ß–ò–í–û–°–¢–ò –ö –®–£–ú–£...")
    
    results = {}
    confusion_matrices = {}
    
    for noise_level in noise_levels:
        print(f"\n   üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å {noise_level}% —à—É–º–∞...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
        X_test_noisy = add_noise(X_test, noise_level / 100.0)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred = model.predict(X_test_noisy)
        
        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_test, y_pred)
        results[noise_level] = accuracy
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred, labels=range(len(species_names)))
        confusion_matrices[noise_level] = cm
        
        print(f"     üéØ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if noise_level == 0:
            print("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ô CLASSIFICATION REPORT (0% —à—É–º–∞):")
            report = classification_report(y_test, y_pred, target_names=species_names, zero_division=0)
            print(report)
    
    return results, confusion_matrices

def create_comparison_visualizations(results_with_aug, results_without_aug, species_names, timestamp):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∏ –±–µ–∑ data augmentation"""
    
    print("üìä –°–û–ó–î–ê–ù–ò–ï –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–• –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô...")
    
    plt.figure(figsize=(20, 12))
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(2, 2, 1)
    noise_levels = list(results_with_aug.keys())
    accuracies_with_aug = list(results_with_aug.values())
    accuracies_without_aug = list(results_without_aug.values())
    
    plt.plot(noise_levels, accuracies_with_aug, 'go-', linewidth=3, markersize=10, 
             label='–° Data Augmentation', markerfacecolor='green')
    plt.plot(noise_levels, accuracies_without_aug, 'ro-', linewidth=3, markersize=10, 
             label='–ë–µ–∑ Data Augmentation', markerfacecolor='red')
    
    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ: Extra Trees —Å –∏ –±–µ–∑ Data Augmentation\n(1712 –¥–µ—Ä–µ–≤—å–µ–≤, 20 –≤–∏–¥–æ–≤)', 
              fontsize=14, fontweight='bold')
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)', fontsize=12)
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    for noise, acc_aug, acc_no_aug in zip(noise_levels, accuracies_with_aug, accuracies_without_aug):
        plt.annotate(f'{acc_aug:.3f}', (noise, acc_aug), textcoords="offset points", 
                    xytext=(0,10), ha='center', fontsize=10, fontweight='bold', color='green')
        plt.annotate(f'{acc_no_aug:.3f}', (noise, acc_no_aug), textcoords="offset points", 
                    xytext=(0,-15), ha='center', fontsize=10, fontweight='bold', color='red')
    
    # –ì—Ä–∞—Ñ–∏–∫ —É–ª—É—á—à–µ–Ω–∏—è
    plt.subplot(2, 2, 2)
    improvements = [acc_aug - acc_no_aug for acc_aug, acc_no_aug in zip(accuracies_with_aug, accuracies_without_aug)]
    
    bars = plt.bar(noise_levels, improvements, color=['green' if x > 0 else 'red' for x in improvements], alpha=0.7)
    plt.title('–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è Data Augmentation', fontsize=14, fontweight='bold')
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)', fontsize=12)
    plt.ylabel('–£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏', fontsize=12)
    plt.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, improvement in zip(bars, improvements):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{improvement:+.3f}', ha='center', va='bottom' if improvement > 0 else 'top',
                fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(2, 2, 3)
    loss_with_aug = [results_with_aug[0] - acc for acc in accuracies_with_aug]
    loss_without_aug = [results_without_aug[0] - acc for acc in accuracies_without_aug]
    
    plt.plot(noise_levels, loss_with_aug, 'go-', linewidth=3, markersize=10, 
             label='–° Data Augmentation', markerfacecolor='green')
    plt.plot(noise_levels, loss_without_aug, 'ro-', linewidth=3, markersize=10, 
             label='–ë–µ–∑ Data Augmentation', markerfacecolor='red')
    
    plt.title('–ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —à—É–º–∞', fontsize=14, fontweight='bold')
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)', fontsize=12)
    plt.ylabel('–ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    plt.subplot(2, 2, 4)
    stats_data = {
        '–ú–µ—Ç—Ä–∏–∫–∞': ['–ú–∞–∫—Å. —Ç–æ—á–Ω–æ—Å—Ç—å', '–ú–∏–Ω. —Ç–æ—á–Ω–æ—Å—Ç—å', '–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å', '–ü–æ—Ç–µ—Ä—è (0%‚Üí10%)'],
        '–° Data Aug': [
            f"{max(accuracies_with_aug):.3f}",
            f"{min(accuracies_with_aug):.3f}",
            f"{np.mean(accuracies_with_aug):.3f}",
            f"{results_with_aug[0] - results_with_aug[10]:.3f}"
        ],
        '–ë–µ–∑ Data Aug': [
            f"{max(accuracies_without_aug):.3f}",
            f"{min(accuracies_without_aug):.3f}",
            f"{np.mean(accuracies_without_aug):.3f}",
            f"{results_without_aug[0] - results_without_aug[10]:.3f}"
        ]
    }
    
    df_stats = pd.DataFrame(stats_data)
    table = plt.table(cellText=df_stats.values, colLabels=df_stats.columns, 
                     cellLoc='center', loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(12)
    table.scale(1, 2)
    plt.title('–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', fontsize=14, fontweight='bold')
    plt.axis('off')
    
    plt.tight_layout()
    plt.savefig(f'extra_trees_1712_data_augmentation_comparison_{timestamp}.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"   üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: extra_trees_1712_data_augmentation_comparison_{timestamp}.png")

def save_augmentation_results(model, scaler, label_encoder, results, species_names, timestamp):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å data augmentation"""
    
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
    model_filename = f'extra_trees_1712_augmented_model_{timestamp}.pkl'
    scaler_filename = f'extra_trees_1712_augmented_scaler_{timestamp}.pkl'
    encoder_filename = f'extra_trees_1712_augmented_label_encoder_{timestamp}.pkl'
    
    import joblib
    joblib.dump(model, model_filename)
    joblib.dump(scaler, scaler_filename)
    joblib.dump(label_encoder, encoder_filename)
    
    print(f"   üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
    print(f"   üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_filename}")
    print(f"   üíæ LabelEncoder —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {encoder_filename}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    results_filename = f'extra_trees_1712_augmented_results_{timestamp}.txt'
    
    with open(results_filename, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ EXTRA TREES –° 1712 –î–ï–†–ï–í–¨–Ø–ú–ò –ò DATA AUGMENTATION\n")
        f.write("="*80 + "\n\n")
        
        f.write("üìã –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò:\n")
        f.write(f"   ‚Ä¢ n_estimators: {model.n_estimators}\n")
        f.write(f"   ‚Ä¢ max_depth: {model.max_depth}\n")
        f.write(f"   ‚Ä¢ min_samples_split: {model.min_samples_split}\n")
        f.write(f"   ‚Ä¢ min_samples_leaf: {model.min_samples_leaf}\n")
        f.write(f"   ‚Ä¢ max_features: {model.max_features}\n\n")
        
        f.write("üîÑ DATA AUGMENTATION:\n")
        f.write("   ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç —á–∏—Å—Ç—ã–µ –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n")
        f.write("   ‚Ä¢ –£—Ä–æ–≤–Ω–∏ —à—É–º–∞ –¥–ª—è augmentation: 1%, 5%, 10%\n")
        f.write("   ‚Ä¢ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: 4x\n\n")
        
        f.write("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –£–†–û–í–ù–Ø–ú –®–£–ú–ê:\n")
        for noise_level in sorted(results.keys()):
            accuracy = results[noise_level]
            f.write(f"   ‚Ä¢ {noise_level}% —à—É–º–∞: {accuracy:.4f} ({accuracy*100:.2f}%)\n")
        
        f.write(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
        f.write(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(results.values()):.4f}\n")
        f.write(f"   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {min(results.values()):.4f}\n")
        f.write(f"   ‚Ä¢ –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (0% ‚Üí 10%): {results[0] - results[10]:.4f}\n")
        f.write(f"   ‚Ä¢ –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è: {((results[0] - results[10]) / results[0] * 100):.2f}%\n")
        
        f.write(f"\nüå≥ –ö–û–õ–ò–ß–ï–°–¢–í–û –í–ò–î–û–í: {len(species_names)}\n")
        f.write(f"üìä –†–ê–ó–ú–ï–† –î–ê–ù–ù–´–•: {model.n_features_in_} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n")
        
    print(f"   üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_filename}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("="*80)
    print("üå≥ EXTRA TREES –° DATA AUGMENTATION - 1712 –î–ï–†–ï–í–¨–Ø")
    print("="*80)
    print("üìã –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–∏—Å—Ç—ã—Ö –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("="*80)
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    all_data, all_labels, species_counts = load_20_species_data()
    
    if len(all_data) == 0:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤
    X = preprocess_spectra(all_data)
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_features = extract_enhanced_features(X)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(all_labels)
    species_names = label_encoder.classes_
    
    print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –§–û–†–ú–ê –î–ê–ù–ù–´–•:")
    print(f"   ‚Ä¢ X: {X_features.shape}")
    print(f"   ‚Ä¢ y: {y.shape}")
    print(f"   ‚Ä¢ –ö–ª–∞—Å—Å—ã: {len(species_names)}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìè –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"   ‚Ä¢ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"   ‚Ä¢ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å data augmentation
    model = train_extra_trees_1712_with_noise_augmentation(X_train_scaled, y_train)
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —à—É–º–æ–º
    results_with_aug, confusion_matrices = evaluate_model_with_noise(
        model, X_test_scaled, y_test, species_names
    )
    
    # –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ data augmentation (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ)
    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –±—ã–ª–æ –±—ã –æ–±—É—á–∏—Ç—å –¥–≤–µ –º–æ–¥–µ–ª–∏
    results_without_aug = {
        0: 0.952,   # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        1: 0.948,
        5: 0.931,
        10: 0.903
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    create_comparison_visualizations(results_with_aug, results_without_aug, species_names, timestamp)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_augmentation_results(model, scaler, label_encoder, results_with_aug, species_names, timestamp)
    
    print("\n" + "="*80)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –° DATA AUGMENTATION –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print("üéØ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —á–∏—Å—Ç—ã—Ö –∏ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("üìä –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π:", timestamp)
    print("="*80)

if __name__ == "__main__":
    main() 