import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier
import warnings
warnings.filterwarnings('ignore')

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ TensorFlow
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    TF_AVAILABLE = True
    print("TensorFlow –¥–æ—Å—Ç—É–ø–µ–Ω!")
except ImportError:
    print("‚ö†Ô∏è TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏–º—É–ª—è—Ü–∏—é.")
    TF_AVAILABLE = False

def load_spectral_data_enhanced():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_spectra = []
    all_labels = []
    
    print("üåø –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)...")
    print("="*70)
    
    for tree_type in tree_types:
        folder_path = os.path.join('.', tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            print(f"üìÅ {tree_type}: {len(excel_files)} —Ñ–∞–π–ª–æ–≤")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    
                    if df.shape[1] >= 2:
                        # –ë–µ—Ä–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤—Ç–æ—Ä–æ–π —Å—Ç–æ–ª–±–µ—Ü)
                        spectrum = df.iloc[:, 1].values
                        
                        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç NaN
                        spectrum = spectrum[~np.isnan(spectrum)]
                        
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
                        if len(spectrum) >= 100:
                            # –£–¥–∞–ª—è–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã (–∑–∞ 3 —Å–∏–≥–º—ã)
                            mean_val = np.mean(spectrum)
                            std_val = np.std(spectrum)
                            mask = np.abs(spectrum - mean_val) <= 3 * std_val
                            spectrum = spectrum[mask]
                            
                            if len(spectrum) >= 100:
                                all_spectra.append(spectrum)
                                all_labels.append(tree_type)
                            
                except Exception as e:
                    continue
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_spectra)} —Å–ø–µ–∫—Ç—Ä–æ–≤ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    return all_spectra, all_labels, tree_types

def create_optimized_1d_alexnet_tensorflow(input_shape, num_classes):
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é 1D-AlexNet —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    model = keras.Sequential([
        # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫ - —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        layers.Conv1D(filters=128, kernel_size=11, strides=2, activation='relu', 
                     input_shape=input_shape, padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(pool_size=3, strides=2),
        layers.Dropout(0.2),
        
        # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv1D(filters=256, kernel_size=5, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(pool_size=3, strides=2),
        layers.Dropout(0.3),
        
        # –¢—Ä–µ—Ç–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv1D(filters=384, kernel_size=3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        
        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv1D(filters=384, kernel_size=3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        
        # –ü—è—Ç—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(pool_size=3, strides=2),
        layers.Dropout(0.4),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ - —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        layers.Flatten(),
        layers.Dense(4096, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        
        layers.Dense(2048, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        
        layers.Dense(1024, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

def create_optimized_simulation_model(X_train, y_train, X_val, y_val):
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏–º—É–ª—è—Ü–∏—é 1D-AlexNet —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏ 1D-AlexNet...")
    
    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –≤—ã–±–µ—Ä–µ–º –ª—É—á—à—É—é
    models = {
        'Deep_MLP': MLPClassifier(
            hidden_layer_sizes=(1024, 512, 256, 128, 64),  # –ì–ª—É–±–∂–µ
            activation='relu',
            solver='adam',
            max_iter=2000,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö
            random_state=42,
            early_stopping=True,
            validation_fraction=0.15,
            learning_rate_init=0.001,
            batch_size=32,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞—Ç—á
            alpha=0.0001,  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            beta_1=0.9,
            beta_2=0.999
        ),
        'Wide_MLP': MLPClassifier(
            hidden_layer_sizes=(2048, 1024, 512, 256),  # –®–∏—Ä–µ
            activation='relu',
            solver='adam',
            max_iter=1500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.15,
            learning_rate_init=0.0005,
            batch_size=16,  # –ú–µ–Ω—å—à–∏–π –±–∞—Ç—á –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            alpha=0.0001
        ),
        'Gradient_Boost': GradientBoostingClassifier(
            n_estimators=500,  # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
            learning_rate=0.1,
            max_depth=8,
            random_state=42,
            subsample=0.8,
            max_features='sqrt'
        )
    }
    
    best_model = None
    best_accuracy = 0
    best_name = ""
    
    for name, model in models.items():
        print(f"  üîß –û–±—É—á–µ–Ω–∏–µ {name}...")
        
        model.fit(X_train, y_train)
        val_accuracy = model.score(X_val, y_val)
        
        print(f"    –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_accuracy:.4f}")
        
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_model = model
            best_name = name
    
    print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_name} —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {best_accuracy:.4f}")
    return best_model

def enhanced_data_augmentation(X, y, augment_factor=2):
    """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print(f"üîÑ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (—Ñ–∞–∫—Ç–æ—Ä {augment_factor})...")
    
    augmented_X = [X]
    augmented_y = [y]
    
    for i in range(augment_factor):
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
        noise_level = 0.01 * (i + 1)
        X_noisy = X + np.random.normal(0, noise_level, X.shape)
        
        # –ù–µ–±–æ–ª—å—à–æ–π —Å–¥–≤–∏–≥
        shift = np.random.randint(-2, 3, X.shape[0])
        X_shifted = np.array([np.roll(spectrum, s) for spectrum, s in zip(X, shift)])
        
        augmented_X.extend([X_noisy, X_shifted])
        augmented_y.extend([y, y])
    
    X_augmented = np.vstack(augmented_X)
    y_augmented = np.hstack(augmented_y)
    
    print(f"  üìä –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {X_augmented.shape}")
    return X_augmented, y_augmented

def test_optimized_noise_robustness(model, X_test, y_test, tree_types, noise_levels, n_realizations=1000):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å 1000 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏"""
    print("\n" + "="*70)
    print("üéØ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –®–£–ú–û–ú (1000 –†–ï–ê–õ–ò–ó–ê–¶–ò–ô)")
    print("="*70)
    
    results = {}
    target_results = {
        '–±–µ—Ä–µ–∑–∞': [0.944, 0.939, 0.919],
        '–¥—É–±': [0.783, 0.820, 0.827],
        '–∫–ª–µ–Ω': [0.818, 0.821, 0.830],
        '–ª–∏–ø–∞': [0.931, 0.875, 0.791],
        '–æ—Å–∏–Ω–∞': [0.821, 0.751, 0.640],
        '–µ–ª—å': [0.914, 0.908, 0.881],
        '—Å–æ—Å–Ω–∞': [0.854, 0.832, 0.792]
    }
    
    noise_map = {0.01: 0, 0.05: 1, 0.1: 2}  # –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    for noise_level in noise_levels:
        print(f"\nüîä –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {noise_level * 100:.1f}%")
        print("-" * 50)
        
        accuracies = []
        class_correct = np.zeros(len(tree_types))
        class_total = np.zeros(len(tree_types))
        
        # 1000 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π
        for realization in range(n_realizations):
            if realization % 200 == 0:
                print(f"  –†–µ–∞–ª–∏–∑–∞—Ü–∏—è {realization + 1}/1000...")
            
            # –ì–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º —Å –Ω—É–ª–µ–≤—ã–º —Å—Ä–µ–¥–Ω–∏–º
            if noise_level > 0:
                noise = np.random.normal(0, noise_level, X_test.shape).astype(np.float32)
                X_test_noisy = X_test + noise
            else:
                X_test_noisy = X_test
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            y_pred = model.predict(X_test_noisy)
            accuracy = accuracy_score(y_test, y_pred)
            accuracies.append(accuracy)
            
            # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π –ø–æ –∫–ª–∞—Å—Å–∞–º
            for i in range(len(tree_types)):
                mask = (y_test == i)
                class_total[i] += np.sum(mask)
                class_correct[i] += np.sum((y_pred == i) & mask)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –ø–µ—Ä–≤–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            if realization == 0:
                first_pred = y_pred
                first_true = y_test
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        mean_accuracy = np.mean(accuracies)
        std_accuracy = np.std(accuracies)
        class_accuracies = class_correct / class_total
        
        print(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ü–µ–ª–µ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        if noise_level in noise_map:
            target_idx = noise_map[noise_level]
            print(f"\nüéØ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ü–µ–ª–µ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (—à—É–º {noise_level*100}%):")
            total_diff = 0
            for i, tree in enumerate(tree_types):
                target_val = target_results[tree][target_idx]
                our_val = class_accuracies[i]
                diff = abs(target_val - our_val)
                total_diff += diff
                status = "‚úÖ" if diff < 0.05 else "‚ö†Ô∏è" if diff < 0.1 else "‚ùå"
                print(f"  {tree}: {our_val:.3f} (—Ü–µ–ª—å: {target_val:.3f}) {status}")
            
            avg_diff = total_diff / len(tree_types)
            print(f"  –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞: {avg_diff:.3f}")
        
        # –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        print(f"\nüìã –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(first_true, first_pred, target_names=tree_types, digits=3))
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –∏ FPR
        cm = confusion_matrix(first_true, first_pred)
        print("\nüö® –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ª–æ–∂–Ω–æ–π —Ç—Ä–µ–≤–æ–≥–∏ (FPR):")
        for i, tree in enumerate(tree_types):
            FP = cm.sum(axis=0)[i] - cm[i, i]
            TN = cm.sum() - cm.sum(axis=0)[i] - cm.sum(axis=1)[i] + cm[i, i]
            FPR = FP / (FP + TN) if (FP + TN) != 0 else 0
            print(f"  {tree}: {FPR:.3f}")
        
        results[noise_level] = {
            'mean_accuracy': mean_accuracy,
            'std_accuracy': std_accuracy,
            'class_accuracies': class_accuracies,
            'confusion_matrix': cm
        }
    
    return results

def plot_comparison_with_target(results, tree_types):
    """–°—Ç—Ä–æ–∏—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å —Ü–µ–ª–µ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
    target_results = {
        '–±–µ—Ä–µ–∑–∞': [0.944, 0.939, 0.919],
        '–¥—É–±': [0.783, 0.820, 0.827],
        '–∫–ª–µ–Ω': [0.818, 0.821, 0.830],
        '–ª–∏–ø–∞': [0.931, 0.875, 0.791],
        '–æ—Å–∏–Ω–∞': [0.821, 0.751, 0.640],
        '–µ–ª—å': [0.914, 0.908, 0.881],
        '—Å–æ—Å–Ω–∞': [0.854, 0.832, 0.792]
    }
    
    noise_levels_target = [0.01, 0.05, 0.1]
    noise_levels_our = [0.01, 0.05, 0.1]
    
    plt.figure(figsize=(20, 15))
    
    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
    for i, tree in enumerate(tree_types):
        plt.subplot(3, 3, i + 1)
        
        # –¶–µ–ª–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        target_vals = target_results[tree]
        plt.plot([n*100 for n in noise_levels_target], target_vals, 
                'o-', label='–°—Ç–∞—Ç—å—è (—Ü–µ–ª—å)', linewidth=2, markersize=8)
        
        # –ù–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        our_vals = [results[noise]['class_accuracies'][i] for noise in noise_levels_our]
        plt.plot([n*100 for n in noise_levels_our], our_vals, 
                's-', label='–ù–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', linewidth=2, markersize=8)
        
        plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')
        plt.title(f'{tree.upper()}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0.5, 1.0)
    
    # –û–±—â–∏–π –≥—Ä–∞—Ñ–∏–∫
    plt.subplot(3, 3, 8)
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    target_means = [np.mean([target_results[tree][i] for tree in tree_types]) 
                   for i in range(3)]
    our_means = [np.mean([results[noise]['class_accuracies'] for noise in noise_levels_our])]
    
    plt.plot([n*100 for n in noise_levels_target], target_means, 
            'o-', label='–°—Ç–∞—Ç—å—è (—Å—Ä–µ–¥–Ω–µ–µ)', linewidth=3, markersize=10)
    
    overall_accuracies = [results[noise]['mean_accuracy'] for noise in noise_levels_our]
    plt.plot([n*100 for n in noise_levels_our], overall_accuracies, 
            's-', label='–ù–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ–±—â–∏–µ)', linewidth=3, markersize=10)
    
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.title('–û–ë–©–ï–ï –°–†–ê–í–ù–ï–ù–ò–ï')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('optimized_1d_alexnet_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üå≤ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –†–ê–°–¢–ò–¢–ï–õ–¨–ù–û–°–¢–ò (1D-AlexNet)")
    print("=" * 70)
    print("üéØ –¶–µ–ª—å: –¥–æ—Å—Ç–∏—á—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –Ω–∞—É—á–Ω–æ–π —Å—Ç–∞—Ç—å–∏")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    spectra, labels, tree_types = load_spectral_data_enhanced()
    
    if len(spectra) == 0:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    lengths = [len(s) for s in spectra]
    target_length = min(lengths)
    print(f"üìè –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {target_length}")
    
    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã
    X = np.array([spectrum[:target_length] for spectrum in spectra], dtype=np.float32)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(labels)
    
    print(f"üìä –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"üéØ –ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.67, random_state=42, stratify=y_temp
    )
    
    print(f"üìè –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –û–±—É—á–∞—é—â–∞—è: {X_train.shape}")
    print(f"  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è: {X_val.shape}")
    print(f"  –¢–µ—Å—Ç–æ–≤–∞—è: {X_test.shape}")
    
    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    X_train_aug, y_train_aug = enhanced_data_augmentation(X_train, y_train, augment_factor=1)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_aug)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    if TF_AVAILABLE:
        print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π 1D-AlexNet –≤ TensorFlow...")
        model = create_optimized_1d_alexnet_tensorflow((target_length, 1), len(tree_types))
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–º–ø–∏–ª—è—Ü–∏–∏
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # –ö–æ–ª–ª–±—ç–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        callbacks = [
            keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10),
            keras.callbacks.ModelCheckpoint('best_1d_alexnet.h5', save_best_only=True)
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —ç–ø–æ—Ö
        history = model.fit(
            X_train_scaled.reshape(-1, target_length, 1),
            y_train_aug,
            validation_data=(X_val_scaled.reshape(-1, target_length, 1), y_val),
            epochs=200,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö
            batch_size=16,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞—Ç—á
            callbacks=callbacks,
            verbose=1
        )
        
    else:
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏...")
        model = create_optimized_simulation_model(X_train_scaled, y_train_aug, X_val_scaled, y_val)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    noise_levels = [0.01, 0.05, 0.1]  # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–∞—Ç—å–µ
    
    results = test_optimized_noise_robustness(
        model, X_test_scaled, y_test, tree_types, noise_levels, n_realizations=1000
    )
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plot_comparison_with_target(results, tree_types)
    
    print("\n" + "="*70)
    print("‚úÖ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω—ã —Å —Ü–µ–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏")
    print("üìä –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: optimized_1d_alexnet_comparison.png")
    print("="*70)

if __name__ == "__main__":
    main() 