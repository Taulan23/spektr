#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
1D ALEXNET –î–õ–Ø 20 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Alexnet –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten
from tensorflow.keras.utils import to_categorical
import warnings
from datetime import datetime
import time

warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_20_species_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö 20 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤"""
    
    print("üå≤ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 20 –í–ò–î–û–í...")
    
    spring_folder = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ –≤–∏–¥–æ–≤
    all_folders = [d for d in os.listdir(spring_folder) 
                   if os.path.isdir(os.path.join(spring_folder, d))]
    
    print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫: {len(all_folders)}")
    for folder in sorted(all_folders):
        print(f"   - {folder}")
    
    spring_data = []
    spring_labels = []
    
    for species in sorted(all_folders):
        folder_path = os.path.join(spring_folder, species)
        
        # –î–ª—è –∫–ª–µ–Ω_–∞–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–æ–∂–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
        if species == "–∫–ª–µ–Ω_–∞–º":
            subfolder_path = os.path.join(folder_path, species)
            if os.path.exists(subfolder_path):
                folder_path = subfolder_path
        
        files = glob.glob(os.path.join(folder_path, "*.xlsx"))
        print(f"   {species}: {len(files)} —Ñ–∞–π–ª–æ–≤ (–ø—É—Ç—å: {folder_path})")
        
        species_count = 0
        for file in files:
            try:
                df = pd.read_excel(file)
                if not df.empty and len(df.columns) >= 2:
                    spectrum = df.iloc[:, 1].values
                    if len(spectrum) > 100:
                        spring_data.append(spectrum)
                        spring_labels.append(species)
                        species_count += 1
            except Exception as e:
                continue
        
        if species_count == 0:
            print(f"   ‚ö†Ô∏è  {species}: –ù–ï–¢ –î–ê–ù–ù–´–• - –∏—Å–∫–ª—é—á–∞–µ–º –∏–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    
    # –£–¥–∞–ª—è–µ–º –≤–∏–¥—ã –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö
    unique_labels = sorted(list(set(spring_labels)))
    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(spring_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ {len(unique_labels)} –≤–∏–¥–∞–º")
    print(f"   –í–∏–¥—ã —Å –¥–∞–Ω–Ω—ã–º–∏: {unique_labels}")
    
    return spring_data, spring_labels

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤ –¥–ª—è CNN"""
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    min_length = min(len(spectrum) for spectrum in spectra_list)
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Å–ø–µ–∫—Ç—Ä—ã –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –æ—á–∏—â–∞–µ–º –æ—Ç NaN
    processed_spectra = []
    for spectrum in spectra_list:
        spectrum_clean = spectrum[~np.isnan(spectrum)]
        if len(spectrum_clean) >= min_length:
            processed_spectra.append(spectrum_clean[:min_length])
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤
    X = np.array(processed_spectra)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞ –¥–ª—è CNN
    X = X.reshape(X.shape[0], X.shape[1], 1)
    
    print(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CNN: {X.shape}")
    return X

def create_1d_alexnet(input_shape, num_classes, species_names):
    """–°–æ–∑–¥–∞–µ—Ç 1D –∞–¥–∞–ø—Ç–∞—Ü–∏—é Alexnet –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    print(f"üß† –°–û–ó–î–ê–ù–ò–ï 1D ALEXNET...")
    print(f"   –í—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {input_shape}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}")
    
    inputs = Input(shape=input_shape, name='spectrum_input')
    
    # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫ (–∞–Ω–∞–ª–æ–≥ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ Alexnet)
    x = Conv1D(96, 11, strides=4, activation='relu', name='conv1')(inputs)
    x = BatchNormalization(name='bn1')(x)
    x = MaxPooling1D(3, strides=2, name='pool1')(x)
    
    # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
    x = Conv1D(256, 5, padding='same', activation='relu', name='conv2')(x)
    x = BatchNormalization(name='bn2')(x)
    x = MaxPooling1D(3, strides=2, name='pool2')(x)
    
    # –¢—Ä–µ—Ç–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
    x = Conv1D(384, 3, padding='same', activation='relu', name='conv3')(x)
    x = BatchNormalization(name='bn3')(x)
    
    # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
    x = Conv1D(384, 3, padding='same', activation='relu', name='conv4')(x)
    x = BatchNormalization(name='bn4')(x)
    
    # –ü—è—Ç—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
    x = Conv1D(256, 3, padding='same', activation='relu', name='conv5')(x)
    x = BatchNormalization(name='bn5')(x)
    x = MaxPooling1D(3, strides=2, name='pool5')(x)
    
    # –ü–µ—Ä–µ—Ö–æ–¥ –∫ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º —Å–ª–æ—è–º
    x = GlobalAveragePooling1D(name='global_pool')(x)
    
    # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (FC layers)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dropout(0.5, name='dropout1')(x)
    x = BatchNormalization(name='bn_fc1')(x)
    
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dropout(0.5, name='dropout2')(x)
    x = BatchNormalization(name='bn_fc2')(x)
    
    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ç–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø –≤–∏–¥–æ–≤
    print("   –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ç–æ–∫...")
    
    # –í–µ—Ç–∫–∞ –¥–ª—è —Ö–≤–æ–π–Ω—ã—Ö
    conifer_species = ['–µ–ª—å', '–µ–ª—å_–≥–æ–ª—É–±–∞—è', '–ª–∏—Å—Ç–≤–µ–Ω–Ω–∏—Ü–∞', '—Å–æ—Å–Ω–∞', '—Ç—É—è']
    conifer_branch = Dense(1024, activation='relu', name='conifer_branch')(x)
    conifer_branch = Dropout(0.3)(conifer_branch)
    conifer_branch = BatchNormalization()(conifer_branch)
    
    # –í–µ—Ç–∫–∞ –¥–ª—è –ª–∏—Å—Ç–≤–µ–Ω–Ω—ã—Ö
    deciduous_species = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–∫–ª–µ–Ω', '–∫–ª–µ–Ω_–∞–º', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—è—Å–µ–Ω—å', '–∫–∞—à—Ç–∞–Ω', '–æ—Ä–µ—Ö']
    deciduous_branch = Dense(1024, activation='relu', name='deciduous_branch')(x)
    deciduous_branch = Dropout(0.3)(deciduous_branch)
    deciduous_branch = BatchNormalization()(deciduous_branch)
    
    # –í–µ—Ç–∫–∞ –¥–ª—è –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤ –∏ –æ—Å–æ–±—ã—Ö –≤–∏–¥–æ–≤
    special_species = ['—Å–∏—Ä–µ–Ω—å', '—á–µ—Ä–µ–º—É—Ö–∞', '—Ä—è–±–∏–Ω–∞', '—Ç–æ–ø–æ–ª—å_—á–µ—Ä–Ω—ã–π', '—Ç–æ–ø–æ–ª—å_–±–∞–ª—å–∑–∞–º–∏—á–µ—Å–∫–∏–π', '–∏–≤–∞']
    special_branch = Dense(1024, activation='relu', name='special_branch')(x)
    special_branch = Dropout(0.3)(special_branch)
    special_branch = BatchNormalization()(special_branch)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Ç–∫–∏
    from tensorflow.keras.layers import Concatenate
    combined = Concatenate(name='combine_branches')([conifer_branch, deciduous_branch, special_branch])
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
    x = Dense(2048, activation='relu', name='fc_final1')(combined)
    x = Dropout(0.4)(x)
    x = BatchNormalization()(x)
    
    x = Dense(1024, activation='relu', name='fc_final2')(x)
    x = Dropout(0.3)(x)
    x = BatchNormalization()(x)
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    outputs = Dense(num_classes, activation='softmax', name='classification')(x)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = Model(inputs=inputs, outputs=outputs, name='Alexnet1D_20Species')
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º
    model.compile(
        optimizer=Adam(learning_rate=0.0001, weight_decay=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # –í—ã–≤–æ–¥–∏–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
    print("\nüìã –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò:")
    model.summary()
    
    return model

def create_data_augmentation(X, y, augmentation_factor=2):
    """–°–æ–∑–¥–∞–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    print(f"üîÑ –°–û–ó–î–ê–ù–ò–ï –ê–£–ì–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–• –î–ê–ù–ù–´–• (—Ñ–∞–∫—Ç–æ—Ä: {augmentation_factor})...")
    
    X_aug = []
    y_aug = []
    
    for i in range(len(X)):
        spectrum = X[i].flatten()
        label = y[i]
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–µ–∫—Ç—Ä
        X_aug.append(X[i])
        y_aug.append(label)
        
        for _ in range(augmentation_factor):
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            noise_level = 0.02
            noisy_spectrum = spectrum + np.random.normal(0, noise_level * np.std(spectrum), spectrum.shape)
            
            # –ù–µ–±–æ–ª—å—à–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            scale_factor = np.random.uniform(0.95, 1.05)
            scaled_spectrum = noisy_spectrum * scale_factor
            
            # –ù–µ–±–æ–ª—å—à–æ–π —Å–¥–≤–∏–≥
            shift = np.random.uniform(-0.01, 0.01) * np.mean(spectrum)
            shifted_spectrum = scaled_spectrum + shift
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
            X_aug.append(shifted_spectrum.reshape(-1, 1))
            y_aug.append(label)
    
    X_augmented = np.array(X_aug)
    y_augmented = np.array(y_aug)
    
    print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X)}")
    print(f"   –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {len(X_augmented)}")
    
    return X_augmented, y_augmented

def analyze_results(model, X_test, y_test, species_names, history):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    
    print("\nüìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_proba = model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_proba, axis=1)
    y_true = np.argmax(y_test, axis=1)
    
    # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    accuracy = accuracy_score(y_true, y_pred)
    print(f"\nüéØ –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report = classification_report(y_true, y_pred, target_names=species_names, output_dict=True, zero_division=0)
    
    print(f"\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –í–ò–î–ê–ú:")
    for species in species_names:
        if species in report:
            precision = report[species]['precision']
            recall = report[species]['recall']
            f1 = report[species]['f1-score']
            print(f"   {species:25} P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    
    # 1. –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    ax1 = axes[0, 0]
    ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è', fontsize=14, fontweight='bold')
    ax1.set_xlabel('–≠–ø–æ—Ö–∞')
    ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. –ü–æ—Ç–µ—Ä–∏
    ax2 = axes[0, 1]
    ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)
    ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    ax2.set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å', fontsize=14, fontweight='bold')
    ax2.set_xlabel('–≠–ø–æ—Ö–∞')
    ax2.set_ylabel('–ü–æ—Ç–µ—Ä–∏')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Confusion Matrix
    ax3 = axes[1, 0]
    cm = confusion_matrix(y_true, y_pred)
    cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)
    
    im = ax3.imshow(cm_normalized, cmap='Blues', aspect='auto')
    ax3.set_xticks(range(len(species_names)))
    ax3.set_yticks(range(len(species_names)))
    ax3.set_xticklabels(species_names, rotation=45, ha='right')
    ax3.set_yticklabels(species_names)
    ax3.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Predicted')
    ax3.set_ylabel('True')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∏
    for i in range(len(species_names)):
        for j in range(len(species_names)):
            value = cm_normalized[i, j]
            color = 'white' if value > 0.5 else 'black'
            ax3.text(j, i, f'{value:.2f}', ha='center', va='center', 
                    color=color, fontweight='bold', fontsize=8)
    
    plt.colorbar(im, ax=ax3, shrink=0.8)
    
    # 4. –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–∏–¥–∞–º
    ax4 = axes[1, 1]
    species_accuracy = []
    for i, species in enumerate(species_names):
        mask = y_true == i
        if np.sum(mask) > 0:
            acc = accuracy_score(y_true[mask], y_pred[mask])
            species_accuracy.append(acc)
        else:
            species_accuracy.append(0)
    
    bars = ax4.bar(range(len(species_names)), species_accuracy, 
                   color=['green' if acc > 0.8 else 'orange' if acc > 0.5 else 'red' for acc in species_accuracy],
                   alpha=0.8)
    ax4.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–∏–¥–∞–º', fontsize=14, fontweight='bold')
    ax4.set_xlabel('–í–∏–¥—ã')
    ax4.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    ax4.set_xticks(range(len(species_names)))
    ax4.set_xticklabels(species_names, rotation=45, ha='right')
    ax4.set_ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, bar in enumerate(bars):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{species_accuracy[i]:.2f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('alexnet_20_species_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return accuracy, report

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üå≤" * 25)
    print("üöÄ 1D ALEXNET –î–õ–Ø 20 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í")
    print("üå≤" * 25)
    
    start_time = time.time()
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    spring_data, spring_labels = load_20_species_data()
    
    if len(spring_data) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("\nüîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•...")
    X = preprocess_spectra(spring_data)
    
    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(spring_labels)
    species_names = label_encoder.classes_
    y_categorical = to_categorical(y_encoded)
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
    print(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X)}")
    print(f"   –§–æ—Ä–º–∞ —Å–ø–µ–∫—Ç—Ä–∞: {X.shape[1:]}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤: {len(species_names)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∏–¥–∞–º
    unique, counts = np.unique(y_encoded, return_counts=True)
    for i, (species, count) in enumerate(zip(species_names, counts)):
        print(f"   {species:25} {count:3d} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_categorical, 
        test_size=0.2, 
        random_state=42, 
        stratify=y_encoded
    )
    
    print(f"\nüìà –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}")
    
    # 5. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    X_train_aug, y_train_aug = create_data_augmentation(X_train, y_train, augmentation_factor=1)
    
    # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    print(f"\nüî¢ –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•...")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –∫–∞–Ω–∞–ª–∞–º
    scaler = StandardScaler()
    X_train_scaled = X_train_aug.copy()
    X_test_scaled = X_test.copy()
    
    for i in range(X_train_aug.shape[0]):
        X_train_scaled[i, :, 0] = scaler.fit_transform(X_train_aug[i, :, 0].reshape(-1, 1)).flatten()
    
    for i in range(X_test.shape[0]):
        X_test_scaled[i, :, 0] = scaler.fit_transform(X_test[i, :, 0].reshape(-1, 1)).flatten()
    
    # 7. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = create_1d_alexnet(X_train_scaled.shape[1:], len(species_names), species_names)
    
    # 8. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    callbacks = [
        EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7, verbose=1),
        ModelCheckpoint('best_alexnet_20_species.keras', monitor='val_accuracy', save_best_only=True, verbose=1)
    ]
    
    # 9. –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüéØ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø...")
    print(f"   –≠–ø–æ—Ö–∏: 100")
    print(f"   Batch size: 32")
    print(f"   –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_train_scaled)}")
    
    history = model.fit(
        X_train_scaled, y_train_aug,
        batch_size=32,
        epochs=100,
        validation_data=(X_test_scaled, y_test),
        callbacks=callbacks,
        verbose=1
    )
    
    # 10. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    accuracy, report = analyze_results(model, X_test_scaled, y_test, species_names, history)
    
    # 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    model.save(f'alexnet_20_species_final_{timestamp}.keras')
    
    import joblib
    joblib.dump(label_encoder, f'alexnet_20_species_label_encoder_{timestamp}.pkl')
    joblib.dump(scaler, f'alexnet_20_species_scaler_{timestamp}.pkl')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report_text = f"""
üèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ 1D ALEXNET –î–õ–Ø 20 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í
==============================================

‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {time.time() - start_time:.1f} —Å–µ–∫—É–Ω–¥
üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)
üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤: {len(species_names)}
üî¢ –û–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_train_scaled)}
üß™ –¢–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_test_scaled)}

üìã –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û –í–ò–î–ê–ú:
{chr(10).join([f"   {species:25} P={report[species]['precision']:.3f}, R={report[species]['recall']:.3f}, F1={report[species]['f1-score']:.3f}" 
               for species in species_names if species in report])}

üèÜ –°–¢–ê–¢–£–°: {'–£–°–ü–ï–•' if accuracy > 0.7 else '–¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–ô'}

üìÅ –§–ê–ô–õ–´:
   - alexnet_20_species_final_{timestamp}.keras
   - alexnet_20_species_label_encoder_{timestamp}.pkl
   - alexnet_20_species_scaler_{timestamp}.pkl
   - alexnet_20_species_results.png
    """
    
    with open(f'alexnet_20_species_report_{timestamp}.txt', 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    # 12. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    total_time = time.time() - start_time
    
    print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï 1D ALEXNET –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"üèÜ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}")
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å timestamp: {timestamp}")
    print(f"üéØ –°—Ç–∞—Ç—É—Å: {'–£–°–ü–ï–•' if accuracy > 0.7 else '–¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–ô'}")
    
    return model, accuracy, report

if __name__ == "__main__":
    main() 