import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
import pickle
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import random
warnings.filterwarnings('ignore')

# –§–∏–∫—Å–∏—Ä—É–µ–º random seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
torch.cuda.manual_seed_all(RANDOM_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
print(f"üé≤ Random seed –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω: {RANDOM_SEED}")

def load_spectral_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è 1D-AlexNet"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_spectra = []
    all_labels = []
    
    print("üåø –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    print("="*60)
    
    for tree_type in tree_types:
        folder_path = os.path.join('.', tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            print(f"üìÅ {tree_type}: {len(excel_files)} —Ñ–∞–π–ª–æ–≤")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    
                    if df.shape[1] >= 2:
                        # –ë–µ—Ä–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤—Ç–æ—Ä–æ–π —Å—Ç–æ–ª–±–µ—Ü)
                        spectrum = df.iloc[:, 1].values
                        
                        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç NaN
                        spectrum = spectrum[~np.isnan(spectrum)]
                        
                        if len(spectrum) >= 100:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            all_spectra.append(spectrum)
                            all_labels.append(tree_type)
                            
                except Exception as e:
                    print(f"‚ùóÔ∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    continue
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_spectra)} —Å–ø–µ–∫—Ç—Ä–æ–≤ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    return all_spectra, all_labels, tree_types

def preprocess_spectra_for_1d_alexnet(spectra, labels, target_length=None):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä—ã –¥–ª—è 1D-AlexNet —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏.
    """
    print("\nüîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤ –¥–ª—è 1D-AlexNet...")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –¥–ª–∏–Ω—É —Å–ø–µ–∫—Ç—Ä–∞
    if target_length is None:
        lengths = [len(s) for s in spectra]
        unique_lengths, counts = np.unique(lengths, return_counts=True)
        target_length = unique_lengths[np.argmax(counts)]
        print(f"üìè –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {target_length}")
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω: {dict(zip(unique_lengths, counts))}")
    else:
        print(f"üìè –ó–∞–¥–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {target_length}")
    
    processed_spectra = []
    processed_labels = []
    
    for i, spectrum in enumerate(spectra):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ø–µ–∫—Ç—Ä—ã
        if len(spectrum) < 50:
            continue
            
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º, –µ—Å–ª–∏ –¥–ª–∏–Ω–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
        if len(spectrum) != target_length:
            processed_spectrum = np.interp(
                np.linspace(0, len(spectrum) - 1, target_length),
                np.arange(len(spectrum)),
                spectrum
            )
        else:
            processed_spectrum = spectrum
            
        processed_spectra.append(processed_spectrum)
        processed_labels.append(labels[i])
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤
    X = np.array(processed_spectra, dtype=np.float32)
    
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(processed_labels)
    
    print(f"üìä –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(np.unique(y))}")
    print(f"üè∑Ô∏è –ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    
    return X, y, label_encoder, target_length

class AlexNet1D(nn.Module):
    """
    –¢–æ—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è 1D AlexNet —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∏—Å–ª–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ
    (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã —Å–ø–µ–∫—Ç—Ä–∞)
    """
    
    def __init__(self, input_length=300, num_classes=7):
        super(AlexNet1D, self).__init__()
        
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã)
        self.conv1 = nn.Conv1d(1, 10, kernel_size=25, stride=4, padding=2)  # 10 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞
        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2)                   # —Ä–∞–∑–º–µ—Ä 3, stride 2
        
        self.conv2 = nn.Conv1d(10, 20, kernel_size=15, stride=1, padding=2)  # 20 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞  
        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2)                   # —Ä–∞–∑–º–µ—Ä 3, stride 2
        
        self.conv3 = nn.Conv1d(20, 50, kernel_size=2, stride=1, padding=1)   # 50 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä 2, stride 1
        self.conv4 = nn.Conv1d(50, 50, kernel_size=2, stride=1, padding=1)   # 50 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä 2, stride 1
        self.conv5 = nn.Conv1d(50, 25, kernel_size=2, stride=1, padding=1)   # 25 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä 2, stride 1
        
        self.pool3 = nn.MaxPool1d(kernel_size=3, stride=2)                   # —Ä–∞–∑–º–µ—Ä 3, stride 2
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤
        self._calculate_fc_input_size(input_length)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ
        self.fc1 = nn.Linear(self.fc_input_size, 200)  # 200 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc2 = nn.Linear(200, 200)                 # 200 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc3 = nn.Linear(200, num_classes)         # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        
        # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        self.dropout = nn.Dropout(0.5)
        
    def _calculate_fc_input_size(self, input_length):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è"""
        # –ü—Ä–æ–≤–æ–¥–∏–º dummy forward pass –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
        with torch.no_grad():
            dummy_input = torch.randn(1, 1, input_length)
            x = self._conv_forward(dummy_input)
            self.fc_input_size = x.numel()
    
    def _conv_forward(self, x):
        """–ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏"""
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = F.relu(self.conv5(x))
        
        x = self.pool3(x)
        
        return x
    
    def forward(self, x):
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        x = self._conv_forward(x)
        
        # Flatten –¥–ª—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
        x = x.view(x.size(0), -1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        
        x = self.fc3(x)
        
        return x

def train_model(model, train_loader, val_loader, epochs=400, learning_rate=0.001, momentum=0.3):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏
    """
    print(f"\nüöÄ –û–ë–£–ß–ï–ù–ò–ï 1D AlexNet")
    print("="*60)
    print(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   - –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: RMSprop")
    print(f"   - Learning Rate: {learning_rate}")
    print(f"   - Momentum: {momentum}")
    print(f"   - –≠–ø–æ—Ö–∏: {epochs}")
    print("="*60)
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä RMSprop —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç–∞—Ç—å–µ
    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=momentum)
    criterion = nn.CrossEntropyLoss()
    
    train_losses = []
    val_accuracies = []
    best_val_acc = 0.0
    
    for epoch in range(epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()
        
        val_acc = 100 * val_correct / val_total
        avg_train_loss = train_loss / len(train_loader)
        
        train_losses.append(avg_train_loss)
        val_accuracies.append(val_acc)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_alexnet1d_multiplicative_model.pth')
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö
        if (epoch + 1) % 50 == 0:
            print(f"–≠–ø–æ—Ö–∞ [{epoch + 1}/{epochs}], "
                  f"Train Loss: {avg_train_loss:.4f}, "
                  f"Val Acc: {val_acc:.2f}%, "
                  f"Best Val Acc: {best_val_acc:.2f}%")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    model.load_state_dict(torch.load('best_alexnet1d_multiplicative_model.pth'))
    
    return model, train_losses, val_accuracies, best_val_acc

def test_with_multiplicative_gaussian_noise(model, X_test, y_test, tree_types, noise_levels, n_realizations=1000):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ú –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–º —à—É–º–æ–º
    –ö–∞–∂–¥—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ (1 + –¥–µ–ª—å—Ç–∞)
    –≥–¥–µ –¥–µ–ª—å—Ç–∞ ~ N(0, –ø—Ä–æ—Ü–µ–Ω—Ç_—à—É–º–∞)
    """
    print("\n" + "="*80)
    print("üé≤ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ú –ì–ê–£–°–°–û–í–°–ö–ò–ú –®–£–ú–û–ú")
    print("üìã –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø:")
    print("   - –ö–∞–∂–¥—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ (1 + –¥–µ–ª—å—Ç–∞)")
    print("   - –î–µ–ª—å—Ç–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –ø–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –∑–∞–∫–æ–Ω—É —Å–æ —Å—Ä–µ–¥–Ω–∏–º = 0")
    print("   - –°–ö–û –¥–µ–ª—å—Ç—ã = –ø—Ä–æ—Ü–µ–Ω—Ç —à—É–º–∞")
    print("   - X_noisy = X * (1 + delta), –≥–¥–µ delta ~ N(0, œÉ)")
    print("   - 1000 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —à—É–º–∞")
    print("="*80)
    
    model.eval()
    results = {}
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ PyTorch —Ç–µ–Ω–∑–æ—Ä—ã
    X_test_tensor = torch.FloatTensor(X_test).unsqueeze(1).to(device)  # –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª
    
    for noise_level in noise_levels:
        print(f"\nüîä –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {noise_level * 100:.1f}%")
        print("-" * 50)
        
        accuracies = []
        all_confusion_matrices = []
        
        # 1000 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π —à—É–º–∞
        for realization in range(n_realizations):
            if realization % 100 == 0:
                print(f"  –†–µ–∞–ª–∏–∑–∞—Ü–∏—è {realization + 1}/{n_realizations}...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ô –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º
            if noise_level > 0:
                # delta ~ N(0, noise_level)
                delta = torch.normal(0, noise_level, X_test_tensor.shape).to(device)
                # X_noisy = X * (1 + delta)
                X_test_noisy = X_test_tensor * (1 + delta)
            else:
                X_test_noisy = X_test_tensor
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = model(X_test_noisy)
                _, predicted = torch.max(outputs, 1)
                predicted = predicted.cpu().numpy()
            
            accuracy = accuracy_score(y_test, predicted)
            accuracies.append(accuracy)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            cm = confusion_matrix(y_test, predicted, labels=range(len(tree_types)))
            all_confusion_matrices.append(cm)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        mean_accuracy = np.mean(accuracies)
        std_accuracy = np.std(accuracies)
        
        print(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {mean_accuracy:.4f} ¬± {std_accuracy:.4f}")
        print(f"üìà –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {np.min(accuracies):.4f}")
        print(f"üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {np.max(accuracies):.4f}")
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –ø–æ –≤—Å–µ–º —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º
        mean_confusion_matrix = np.mean(all_confusion_matrices, axis=0)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
        mean_class_accuracies = []
        std_class_accuracies = []
        for i in range(len(tree_types)):
            class_accuracies_all_realizations = []
            for cm in all_confusion_matrices:
                if cm.sum(axis=1)[i] > 0:  # –∏–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                    class_acc = cm[i, i] / cm.sum(axis=1)[i]
                    class_accuracies_all_realizations.append(class_acc)
            
            if class_accuracies_all_realizations:
                mean_class_acc = np.mean(class_accuracies_all_realizations)
                std_class_acc = np.std(class_accuracies_all_realizations)
                mean_class_accuracies.append(mean_class_acc)
                std_class_accuracies.append(std_class_acc)
            else:
                mean_class_accuracies.append(0.0)
                std_class_accuracies.append(0.0)
        
        # –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        print(f"\nüìã –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ {n_realizations} —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º (—à—É–º {noise_level * 100:.1f}%):")
        print(f"\n‚úÖ –°—Ä–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        for i, tree in enumerate(tree_types):
            print(f"  {tree}: {mean_class_accuracies[i]:.4f} ¬± {std_class_accuracies[i]:.4f}")
        
        # –°—Ä–µ–¥–Ω—è—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        print(f"\nüìä –°—Ä–µ–¥–Ω—è—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–æ–∫—Ä—É–≥–ª–µ–Ω–Ω–∞—è):")
        print(np.round(mean_confusion_matrix).astype(int))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results[noise_level] = {
            'mean_accuracy': mean_accuracy,
            'std_accuracy': std_accuracy,
            'min_accuracy': np.min(accuracies),
            'max_accuracy': np.max(accuracies),
            'class_accuracies': mean_class_accuracies,
            'std_class_accuracies': std_class_accuracies,
            'confusion_matrix': np.round(mean_confusion_matrix).astype(int),
            'all_accuracies': accuracies,
            'mean_confusion_matrix': mean_confusion_matrix
        }
    
    return results

def print_confusion_matrix_table(results, tree_types):
    """
    –í—ã–≤–æ–¥–∏—Ç –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —à—É–º–∞
    —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º–∞—Ç—É –∏–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    print("\n" + "="*100)
    print("üìä –ú–ê–¢–†–ò–¶–´ –û–®–ò–ë–û–ö –í –¢–ê–ë–õ–ò–ß–ù–û–ú –§–û–†–ú–ê–¢–ï (–ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ô –®–£–ú)")
    print("üìã –ö–∞–∂–¥—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ (1 + –¥–µ–ª—å—Ç–∞)")
    print("üìã –î–µ–ª—å—Ç–∞ ~ N(0, œÉ) –≥–¥–µ œÉ = –ø—Ä–æ—Ü–µ–Ω—Ç —à—É–º–∞")
    print("="*100)
    
    # –ú–∞—Ç—Ä–∏—Ü—ã –¥–ª—è —à—É–º–∞ 1%, 5%, 10%
    target_noise_levels = [0.01, 0.05, 0.10]
    
    for noise_level in target_noise_levels:
        if noise_level in results:
            print(f"\nüîä –¢–ê–ë–õ–ò–¶–ê 3 - –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö –î–õ–Ø –®–£–ú–ê {noise_level*100:.0f}%")
            print("–í —Ç–∞–±–ª–∏—Ü–µ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ –ø–æ–∫–∞–∑–∞–Ω–∞ —Å–≤–æ–¥–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞")
            print("—Å—Ä–µ–¥–Ω–∏—Ö (–ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º —à—É–º–∞) –∑–Ω–∞—á–µ–Ω–∏–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
            print("–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ä–æ–¥ –¥–µ—Ä–µ–≤—å–µ–≤")
            print("-" * 80)
            
            cm = results[noise_level]['confusion_matrix']
            total_samples_per_class = cm.sum(axis=1)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            normalized_cm = np.zeros_like(cm, dtype=float)
            for i in range(len(tree_types)):
                if total_samples_per_class[i] > 0:
                    normalized_cm[i] = cm[i] / total_samples_per_class[i]
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
            header = "–ø–æ—Ä–æ–¥–∞  "
            for tree in tree_types:
                short_name = tree[:6]  # –°–æ–∫—Ä–∞—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                header += f"| {short_name:>8}"
            print(header)
            print("-" * len(header))
            
            # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
            for i, tree in enumerate(tree_types):
                short_name = tree[:6]
                row = f"{short_name:<8}"
                for j in range(len(tree_types)):
                    if normalized_cm[i,j] > 0:
                        row += f"| {normalized_cm[i,j]:>8.2f}"
                    else:
                        row += f"| {'0':>8}"
                print(row)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ä–µ–¥–Ω–∏—Ö —Ç–æ—á–Ω–æ—Å—Ç—è—Ö
            print(f"\n‚úÖ –°—Ä–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
            for i, tree in enumerate(tree_types):
                prob = results[noise_level]['class_accuracies'][i]
                std_prob = results[noise_level].get('std_class_accuracies', [0]*len(tree_types))[i]
                print(f"   {tree}: {prob:.3f} ¬± {std_prob:.3f}")
                
            print(f"\nüìà –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results[noise_level]['mean_accuracy']:.4f} ¬± {results[noise_level]['std_accuracy']:.4f}")
            print("=" * 80)

def save_results_to_file(results, tree_types, best_val_acc, input_length):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    with open('results_analysis_multiplicative_noise.txt', 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –†–ê–°–¢–ò–¢–ï–õ–¨–ù–û–°–¢–ò 1D-AlexNet\n")
        f.write("–ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ô –ì–ê–£–°–°–û–í–°–ö–ò–ô –®–£–ú\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ï–¢–ò (—Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ):\n")
        f.write("- Conv1d: 10 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ 25, stride 4, padding 2\n")
        f.write("- MaxPool1d: —Ä–∞–∑–º–µ—Ä 3, stride 2\n")
        f.write("- Conv1d: 20 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ 15, stride 1, padding 2\n")
        f.write("- MaxPool1d: —Ä–∞–∑–º–µ—Ä 3, stride 2\n")
        f.write("- Conv1d: 50 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ 2, stride 1, padding 1\n")
        f.write("- Conv1d: 50 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ 2, stride 1, padding 1\n")
        f.write("- Conv1d: 25 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ 2, stride 1, padding 1\n")
        f.write("- MaxPool1d: —Ä–∞–∑–º–µ—Ä 3, stride 2\n")
        f.write("- Linear: 200 –Ω–µ–π—Ä–æ–Ω–æ–≤\n")
        f.write("- Linear: 200 –Ω–µ–π—Ä–æ–Ω–æ–≤\n")
        f.write("- Linear: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤\n\n")
        
        f.write("–ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:\n")
        f.write("- –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: RMSprop\n")
        f.write("- Learning Rate: 0.001\n")
        f.write("- Momentum: 0.3\n")
        f.write("- –≠–ø–æ—Ö–∏: 400\n")
        f.write("- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π —à—É–º–∞: 1000\n")
        f.write("- –¢–∏–ø —à—É–º–∞: –ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ô (X * (1 + delta))\n")
        f.write("- delta ~ N(0, œÉ) –≥–¥–µ œÉ = –ø—Ä–æ—Ü–µ–Ω—Ç —à—É–º–∞\n")
        f.write(f"- –î–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {input_length} —Ç–æ—á–µ–∫\n\n")
        
        f.write(f"–õ–£–ß–®–ê–Ø –¢–û–ß–ù–û–°–¢–¨ –ù–ê –í–ê–õ–ò–î–ê–¶–ò–ò: {best_val_acc:.4f}\n\n")
        
        for noise_level, result in results.items():
            f.write(f"–£–†–û–í–ï–ù–¨ –®–£–ú–ê: {noise_level * 100:.1f}%\n")
            f.write("-" * 50 + "\n")
            f.write(f"–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {result['mean_accuracy']:.4f} ¬± {result['std_accuracy']:.4f}\n")
            f.write(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {result['min_accuracy']:.4f}\n")
            f.write(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {result['max_accuracy']:.4f}\n\n")
            
            f.write("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:\n")
            for i, tree in enumerate(tree_types):
                std_acc = result.get('std_class_accuracies', [0]*len(tree_types))[i]
                f.write(f"  {tree}: {result['class_accuracies'][i]:.4f} ¬± {std_acc:.4f}\n")
            f.write("\n")
            
            f.write("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:\n")
            f.write(str(result['confusion_matrix']) + "\n\n")
        
        f.write("=" * 80 + "\n")
        f.write("–ö–õ–Æ–ß–ï–í–´–ï –û–¢–õ–ò–ß–ò–Ø –û–¢ –ê–î–î–ò–¢–ò–í–ù–û–ì–û –®–£–ú–ê:\n")
        f.write("1. –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–π —à—É–º: X_noisy = X * (1 + delta)\n")
        f.write("2. –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–π —à—É–º: X_noisy = X + noise\n")
        f.write("3. –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–π —à—É–º –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\n")
        f.write("4. –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–π —à—É–º –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é —Å–æ—Å—Ç–∞–≤–ª—è—é—â—É—é\n")
        f.write("5. –ü—Ä–∏ –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω–æ–º —à—É–º–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ñ–æ—Ä–º–∞ —Å–ø–µ–∫—Ç—Ä–∞\n")
        f.write("=" * 80 + "\n")

def plot_noise_analysis(results, tree_types):
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω–æ–º—É —à—É–º—É"""
    noise_levels = list(results.keys())
    mean_accuracies = [results[noise]['mean_accuracy'] for noise in noise_levels]
    std_accuracies = [results[noise]['std_accuracy'] for noise in noise_levels]
    
    # –ì—Ä–∞—Ñ–∏–∫ –æ–±—â–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 2, 1)
    plt.errorbar([n*100 for n in noise_levels], mean_accuracies, yerr=std_accuracies, 
                marker='o', capsize=5, capthick=2, linewidth=2)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')
    plt.title('–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å 1D-AlexNet –∫ –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω–æ–º—É —à—É–º—É')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    plt.subplot(2, 2, 2)
    for i, tree in enumerate(tree_types):
        class_accs = [results[noise]['class_accuracies'][i] for noise in noise_levels]
        plt.plot([n*100 for n in noise_levels], class_accs, marker='o', label=tree)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º')
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –≤–∏–¥–∞–º —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ç–æ—á–Ω–æ—Å—Ç–µ–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —à—É–º–∞
    plt.subplot(2, 2, 3)
    max_noise = max(noise_levels)
    accuracies = results[max_noise]['all_accuracies']
    plt.hist(accuracies, bins=50, alpha=0.7, edgecolor='black')
    plt.xlabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ (–º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–π —à—É–º {max_noise*100}%)')
    plt.grid(True, alpha=0.3)
    
    # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    plt.subplot(2, 2, 4)
    cm = results[0.0]['confusion_matrix']  # –ë–µ–∑ —à—É–º–∞
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–±–µ–∑ —à—É–º–∞)')
    plt.colorbar()
    tick_marks = np.arange(len(tree_types))
    plt.xticks(tick_marks, tree_types, rotation=45)
    plt.yticks(tick_marks, tree_types)
    
    plt.tight_layout()
    plt.savefig('1d_alexnet_multiplicative_noise_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ 1D-AlexNet —Å –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–º —à—É–º–æ–º"""
    print("üå≤ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –†–ê–°–¢–ò–¢–ï–õ–¨–ù–û–°–¢–ò –° 1D-AlexNet (–ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ô –®–£–ú)")
    print("=" * 80)
    print("üìÑ –¢–û–ß–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ò–ó –ü–†–ò–°–õ–ê–ù–ù–û–ô –°–•–ï–ú–´")
    print("üéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: RMSprop, Rate=0.001, Moment=0.3, Epochs=400")
    print("üîä –®–£–ú: X_noisy = X * (1 + delta), –≥–¥–µ delta ~ N(0, œÉ)")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    spectra, labels, tree_types = load_spectral_data()

    if len(spectra) == 0:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Å–ø–µ–∫—Ç—Ä–∞)
    X, y, label_encoder, input_length = preprocess_spectra_for_1d_alexnet(spectra, labels)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 50/50
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)
    print(f"\nüìè –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –∏ label_encoder
    with open('scaler_multiplicative.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    with open('label_encoder_multiplicative.pkl', 'wb') as f:
        pickle.dump(label_encoder, f)

    # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è PyTorch
    X_train_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(1)  # –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª
    y_train_tensor = torch.LongTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_test_scaled).unsqueeze(1)
    y_val_tensor = torch.LongTensor(y_test)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = AlexNet1D(input_length=input_length, num_classes=len(tree_types))
    model.to(device)
    
    print(f"\nüèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
    print(model)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nüìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"üìä –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, train_losses, val_accuracies, best_val_acc = train_model(
        model, train_loader, val_loader, 
        epochs=400, learning_rate=0.001, momentum=0.3
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–º —à—É–º–æ–º (0%, 1%, 5%, 10%)
    noise_levels = [0.0, 0.01, 0.05, 0.1]
    
    results = test_with_multiplicative_gaussian_noise(
        model, X_test_scaled, y_test, tree_types, noise_levels, n_realizations=1000
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_results_to_file(results, tree_types, best_val_acc, input_length)
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plot_noise_analysis(results, tree_types)
    
    # –í—ã–≤–æ–¥ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    print_confusion_matrix_table(results, tree_types)
    
    print("\n" + "="*80)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –° –ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ú –®–£–ú–û–ú –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–µ–π —à—É–º–∞:")
    print(f"   - –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_acc:.4f}")
    print(f"   - –î–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {input_length} —Ç–æ—á–µ–∫")
    print(f"   - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")
    print("   - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: —Ç–æ—á–Ω–æ –ø–æ –ø—Ä–∏—Å–ª–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ")
    print("   - RMSprop, Rate=0.001, Moment=0.3, Epochs=400")
    print("   - 1000 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —à—É–º–∞")
    print("   - –ú–£–õ–¨–¢–ò–ü–õ–ò–ö–ê–¢–ò–í–ù–´–ô —à—É–º: X * (1 + delta)")
    print("üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print("   - results_analysis_multiplicative_noise.txt (–¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)")
    print("   - best_alexnet1d_multiplicative_model.pth (–ª—É—á—à–∞—è –º–æ–¥–µ–ª—å)")
    print("   - scaler_multiplicative.pkl, label_encoder_multiplicative.pkl")
    print("   - 1d_alexnet_multiplicative_noise_analysis.png (–≥—Ä–∞—Ñ–∏–∫–∏)")
    print("="*80)

if __name__ == "__main__":
    main() 