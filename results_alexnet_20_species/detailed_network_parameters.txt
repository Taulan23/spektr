================================================================================
ДЕТАЛЬНЫЕ ТЕХНИЧЕСКИЕ ПАРАМЕТРЫ НЕЙРОННОЙ СЕТИ
КЛАССИФИКАЦИЯ СПЕКТРАЛЬНЫХ ДАННЫХ РАСТИТЕЛЬНОСТИ
================================================================================

ДАТА ЭКСПЕРИМЕНТА: 16 июля 2025
ИССЛЕДОВАТЕЛЬ: Анализ спектральных данных 7 видов растений
RANDOM SEED: 42 (для воспроизводимости результатов)

================================================================================
1. АРХИТЕКТУРА НЕЙРОННОЙ СЕТИ (1D AlexNet)
================================================================================

ВХОДНОЙ СЛОЙ:
- Размерность входа: 300 точек спектра
- Количество каналов: 1 (одномерные спектральные данные)
- Формат данных: [batch_size, 1, 300]

СВЕРТОЧНЫЕ СЛОИ:
Слой 1: Conv1D
  - Количество фильтров: 10
  - Размер ядра (kernel_size): 25
  - Шаг (stride): 4
  - Отступы (padding): 2
  - Функция активации: ReLU
  - Batch Normalization: Да

Pooling 1: MaxPool1D
  - Размер окна: 3
  - Шаг (stride): 2

Слой 2: Conv1D
  - Количество фильтров: 20
  - Размер ядра (kernel_size): 15
  - Шаг (stride): 1
  - Отступы (padding): 2
  - Функция активации: ReLU
  - Batch Normalization: Да

Pooling 2: MaxPool1D
  - Размер окна: 3
  - Шаг (stride): 2

Слой 3: Conv1D
  - Количество фильтров: 50
  - Размер ядра (kernel_size): 2
  - Шаг (stride): 1
  - Отступы (padding): 1
  - Функция активации: ReLU
  - Batch Normalization: Да

Слой 4: Conv1D
  - Количество фильтров: 50
  - Размер ядра (kernel_size): 2
  - Шаг (stride): 1
  - Отступы (padding): 1
  - Функция активации: ReLU
  - Batch Normalization: Да

Слой 5: Conv1D
  - Количество фильтров: 25
  - Размер ядра (kernel_size): 2
  - Шаг (stride): 1
  - Отступы (padding): 1
  - Функция активации: ReLU
  - Batch Normalization: Да

Pooling 3: MaxPool1D
  - Размер окна: 3
  - Шаг (stride): 2

ПОЛНОСВЯЗНЫЕ СЛОИ:
Слой 1: Linear
  - Входной размер: автоматически вычисляется (зависит от сверточных слоев)
  - Выходной размер: 128 нейронов
  - Функция активации: ReLU
  - Dropout: 0.3

Слой 2: Linear
  - Входной размер: 128 нейронов
  - Выходной размер: 128 нейронов
  - Функция активации: ReLU
  - Dropout: 0.3

Выходной слой: Linear
  - Входной размер: 128 нейронов
  - Выходной размер: 7 классов (виды растений)
  - Функция активации: Softmax (применяется автоматически в loss function)

ОБЩЕЕ КОЛИЧЕСТВО ПАРАМЕТРОВ: 49,958

================================================================================
2. ПАРАМЕТРЫ ОБУЧЕНИЯ
================================================================================

ОПТИМИЗАТОР: Adam
  - Learning Rate (скорость обучения): 0.001
  - Beta1: 0.9
  - Beta2: 0.999
  - Epsilon: 1e-08
  - Weight Decay: 0 (без L2-регуляризации)

ФУНКЦИЯ ПОТЕРЬ: CrossEntropyLoss

КОЛИЧЕСТВО ЭПОХ: 200

РАЗМЕР БАТЧА (Batch Size): 32

РЕГУЛЯРИЗАЦИЯ:
  - Dropout Rate: 0.3 (применяется к полносвязным слоям)
  - Batch Normalization: применяется ко всем сверточным слоям

EARLY STOPPING: Да (сохранение лучшей модели по валидационной точности)

================================================================================
3. РАЗДЕЛЕНИЕ ДАННЫХ
================================================================================

ОБЩИЙ ОБЪЕМ ДАННЫХ: 1,050 спектров
  - Береза: 150 спектров
  - Дуб: 150 спектров
  - Ель: 150 спектров
  - Клен: 150 спектров
  - Липа: 150 спектров
  - Осина: 150 спектров
  - Сосна: 150 спектров

РАЗДЕЛЕНИЕ:
  - Обучающая выборка: 80% (840 спектров)
  - Тестовая выборка: 20% (210 спектров)
  - Валидационная выборка: 20% от обучающей (168 спектров)

СТРАТИФИКАЦИЯ: Да (сохранение пропорций классов во всех выборках)

ПРЕДОБРАБОТКА:
  - Нормализация: StandardScaler (mean=0, std=1)
  - Интерполяция: до фиксированной длины 300 точек
  - Удаление NaN значений: Да

================================================================================
4. ПАРАМЕТРЫ ТЕСТИРОВАНИЯ С ШУМОМ
================================================================================

УРОВНИ ШУМА: 0%, 1%, 5%, 10%
ТИПА ШУМА: Аддитивный белый гауссовский шум
КОЛИЧЕСТВО РЕАЛИЗАЦИЙ: 1,000 для каждого уровня шума
МЕТОДОЛОГИЯ: Одна обученная модель тестируется на всех уровнях шума

МЕТРИКИ ОЦЕНКИ:
  - Общая точность (Accuracy)
  - Правильная классификация по классам (True Positive Rate / Sensitivity)
  - Ложная тревога по классам (False Alarm Rate = 1 - Specificity)
  - Матрица ошибок (Confusion Matrix)
  - Стандартное отклонение по реализациям

================================================================================
5. ТЕХНИЧЕСКИЕ ДЕТАЛИ РЕАЛИЗАЦИИ
================================================================================

ПЛАТФОРМА: PyTorch 1.12.0+
УСТРОЙСТВО: CPU (возможно GPU при наличии CUDA)
ЯЗЫК ПРОГРАММИРОВАНИЯ: Python 3.x

БИБЛИОТЕКИ:
  - torch: нейронные сети
  - sklearn: предобработка и метрики
  - numpy: численные вычисления
  - pandas: работа с данными
  - matplotlib: визуализация

ВОСПРОИЗВОДИМОСТЬ:
  - Фиксированный random seed: 42
  - Детерминистические алгоритмы: Да
  - torch.backends.cudnn.deterministic = True

================================================================================
6. РЕЗУЛЬТАТЫ ОПТИМИЗАЦИИ ГИПЕРПАРАМЕТРОВ
================================================================================

ПРОТЕСТИРОВАННЫЕ ОПТИМИЗАТОРЫ:
  1. Adam (выбран как оптимальный)
  2. AdamW
  3. RMSprop
  4. SGD

ПРОТЕСТИРОВАННЫЕ РАЗМЕРЫ СКРЫТЫХ СЛОЕВ:
  - 128 нейронов (выбран как оптимальный)
  - 200 нейронов
  - 256 нейронов

ПРОТЕСТИРОВАННЫЕ ЗНАЧЕНИЯ DROPOUT:
  - 0.3 (выбран как оптимальный)
  - 0.5
  - 0.7

ОБЩЕЕ КОЛИЧЕСТВО ПРОТЕСТИРОВАННЫХ КОНФИГУРАЦИЙ: 36
КРИТЕРИЙ ВЫБОРА: Максимальная валидационная точность

================================================================================
7. ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ
================================================================================

ВАЛИДАЦИОННАЯ ТОЧНОСТЬ: 100.00%
ТЕСТОВАЯ ТОЧНОСТЬ: 99.05%

ТОЧНОСТЬ ПО УРОВНЯМ ШУМА:
  - 0% шума: 99.05% ± 0.00%
  - 1% шума: 98.76% ± 0.33%
  - 5% шума: 97.64% ± 0.65%
  - 10% шума: 94.36% ± 1.30%

НАИБОЛЕЕ УСТОЙЧИВЫЕ КЛАССЫ К ШУМУ: береза, ель, клен, сосна
НАИБОЛЕЕ УЯЗВИМЫЕ КЛАССЫ: дуб, осина

================================================================================
8. СРАВНЕНИЕ С БАЗОВОЙ КОНФИГУРАЦИЕЙ
================================================================================

ИСХОДНАЯ КОНФИГУРАЦИЯ (main_1d_alexnet.py):
  - Оптимизатор: RMSprop
  - Learning Rate: 0.001
  - Momentum: 0.3
  - Эпохи: 400
  - Разделение данных: 50/50
  - Dropout: 0.5
  - Hidden Size: 200

ОПТИМИЗИРОВАННАЯ КОНФИГУРАЦИЯ (main_spring_optimized.py):
  - Оптимизатор: Adam
  - Learning Rate: 0.001
  - Эпохи: 200
  - Разделение данных: 80/20
  - Dropout: 0.3
  - Hidden Size: 128

УЛУЧШЕНИЯ:
  - Меньше параметров сети (49,958 vs больше)
  - Лучшая точность (99.05% vs предыдущих результатов)
  - Более эффективное обучение (200 vs 400 эпох)
  - Добавлена Batch Normalization
  - Систематический подбор гиперпараметров

================================================================================
ПРИМЕЧАНИЯ ДЛЯ НАУЧНОГО РУКОВОДИТЕЛЯ:
================================================================================

1. Все параметры подобраны эмпирически через grid search
2. Результаты воспроизводимы благодаря фиксированному random seed
3. Методология тестирования с шумом соответствует стандартам
4. Архитектура адаптирована специально для спектральных данных
5. Batch Normalization улучшает стабильность обучения
6. Adam показал лучшие результаты для данного типа задач

================================================================================ 