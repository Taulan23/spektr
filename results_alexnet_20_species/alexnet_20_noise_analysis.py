#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –®–£–ú–ê –ù–ê 1D ALEXNET –î–õ–Ø 20 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í
–°–æ–∑–¥–∞–Ω–∏–µ confusion –º–∞—Ç—Ä–∏—Ü –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —à—É–º–∞
"""

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import tensorflow as tf
from tensorflow import keras
import joblib
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_20_species_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö 20 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤"""
    
    print("üå≤ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 20 –í–ò–î–û–í...")
    
    spring_folder = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ –≤–∏–¥–æ–≤
    all_folders = [d for d in os.listdir(spring_folder) 
                   if os.path.isdir(os.path.join(spring_folder, d))]
    
    spring_data = []
    spring_labels = []
    
    for species in sorted(all_folders):
        folder_path = os.path.join(spring_folder, species)
        
        # –î–ª—è –∫–ª–µ–Ω_–∞–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–æ–∂–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
        if species == "–∫–ª–µ–Ω_–∞–º":
            subfolder_path = os.path.join(folder_path, species)
            if os.path.exists(subfolder_path):
                folder_path = subfolder_path
        
        files = glob.glob(os.path.join(folder_path, "*.xlsx"))
        print(f"   {species}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        for file in files:
            try:
                df = pd.read_excel(file)
                if not df.empty and len(df.columns) >= 2:
                    spectrum = df.iloc[:, 1].values
                    if len(spectrum) > 100:
                        spring_data.append(spectrum)
                        spring_labels.append(species)
            except Exception as e:
                continue
    
    unique_labels = sorted(list(set(spring_labels)))
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(spring_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ {len(unique_labels)} –≤–∏–¥–∞–º")
    
    return spring_data, spring_labels

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤ –¥–ª—è CNN"""
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    min_length = min(len(spectrum) for spectrum in spectra_list)
    
    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Å–ø–µ–∫—Ç—Ä—ã –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –∏ –æ—á–∏—â–∞–µ–º –æ—Ç NaN
    processed_spectra = []
    for spectrum in spectra_list:
        spectrum_clean = spectrum[~np.isnan(spectrum)]
        if len(spectrum_clean) >= min_length:
            processed_spectra.append(spectrum_clean[:min_length])
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤
    X = np.array(processed_spectra)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–Ω–∞–ª–∞ –¥–ª—è CNN
    X = X.reshape(X.shape[0], X.shape[1], 1)
    
    return X

def add_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª—è–µ—Ç Gaussian —à—É–º –∫ —Å–ø–µ–∫—Ç—Ä–∞–º"""
    if noise_level == 0:
        return X
    
    X_noisy = X.copy()
    for i in range(X.shape[0]):
        spectrum = X[i, :, 0]
        noise = np.random.normal(0, noise_level * np.std(spectrum), spectrum.shape)
        X_noisy[i, :, 0] = spectrum + noise
    
    return X_noisy

def load_saved_models():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏"""
    
    print("üìÅ –ó–ê–ì–†–£–ó–ö–ê –°–û–•–†–ê–ù–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô...")
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–π–ª—ã
    model_files = glob.glob("alexnet_20_species_final_*.keras")
    encoder_files = glob.glob("alexnet_20_species_label_encoder_*.pkl")
    scaler_files = glob.glob("alexnet_20_species_scaler_*.pkl")
    
    if not model_files or not encoder_files or not scaler_files:
        raise Exception("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏!")
    
    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–π–ª—ã
    model_file = sorted(model_files)[-1]
    encoder_file = sorted(encoder_files)[-1]
    scaler_file = sorted(scaler_files)[-1]
    
    print(f"   –ú–æ–¥–µ–ª—å: {model_file}")
    print(f"   Encoder: {encoder_file}")
    print(f"   Scaler: {scaler_file}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º
    model = keras.models.load_model(model_file)
    label_encoder = joblib.load(encoder_file)
    scaler = joblib.load(scaler_file)
    
    return model, label_encoder, scaler

def create_noise_confusion_matrices(model, X_test, y_test, species_names, noise_levels):
    """–°–æ–∑–¥–∞–µ—Ç confusion –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —à—É–º–∞"""
    
    print("üîä –ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –®–£–ú–ê...")
    
    results = {}
    confusion_matrices = {}
    
    for noise_level in noise_levels:
        print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º {noise_level*100:.0f}%...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
        X_noisy = add_noise(X_test, noise_level)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_noisy, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        y_true = np.argmax(y_test, axis=1)
        
        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_true, y_pred)
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)
        
        results[noise_level] = {
            'accuracy': accuracy,
            'confusion_matrix': cm,
            'confusion_matrix_normalized': cm_normalized
        }
        
        print(f"     –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    return results

def plot_noise_confusion_matrices(results, species_names, noise_levels):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é confusion –º–∞—Ç—Ä–∏—Ü –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —à—É–º–∞"""
    
    print("üìä –°–û–ó–î–ê–ù–ò–ï –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò...")
    
    n_levels = len(noise_levels)
    fig, axes = plt.subplots(2, 3, figsize=(24, 16))
    fig.suptitle('üîä Confusion Matrices –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —à—É–º–∞\n1D Alexnet - 20 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤', 
                 fontsize=20, fontweight='bold', y=0.98)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ subplot'—ã
    if n_levels < 6:
        for i in range(n_levels, 6):
            fig.delaxes(axes.flatten()[i])
    
    for idx, noise_level in enumerate(noise_levels):
        if idx >= 6:  # –ú–∞–∫—Å–∏–º—É–º 6 –≥—Ä–∞—Ñ–∏–∫–æ–≤
            break
            
        ax = axes.flatten()[idx]
        cm_norm = results[noise_level]['confusion_matrix_normalized']
        accuracy = results[noise_level]['accuracy']
        
        # –°–æ–∑–¥–∞–µ–º heatmap
        im = ax.imshow(cm_norm, cmap='Blues', aspect='auto', vmin=0, vmax=1)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Å–µ–π
        ax.set_xticks(range(len(species_names)))
        ax.set_yticks(range(len(species_names)))
        ax.set_xticklabels(species_names, rotation=45, ha='right', fontsize=8)
        ax.set_yticklabels(species_names, fontsize=8)
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é
        ax.set_title(f'–®—É–º {noise_level*100:.0f}%\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)', 
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xlabel('Predicted', fontsize=12)
        ax.set_ylabel('True', fontsize=12)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏)
        for i in range(len(species_names)):
            value = cm_norm[i, i]
            color = 'white' if value > 0.5 else 'black'
            ax.text(i, i, f'{value:.2f}', ha='center', va='center', 
                   color=color, fontweight='bold', fontsize=10)
        
        # Colorbar
        cbar = plt.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å', fontsize=10)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.93)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'alexnet_20_noise_confusion_matrices_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    return filename

def create_accuracy_degradation_plot(results, noise_levels):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç —à—É–º–∞"""
    
    print("üìà –°–û–ó–î–ê–ù–ò–ï –ì–†–ê–§–ò–ö–ê –î–ï–ì–†–ê–î–ê–¶–ò–ò –¢–û–ß–ù–û–°–¢–ò...")
    
    accuracies = [results[noise]['accuracy'] for noise in noise_levels]
    noise_percentages = [noise * 100 for noise in noise_levels]
    
    plt.figure(figsize=(12, 8))
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
    plt.plot(noise_percentages, accuracies, 'o-', linewidth=3, markersize=8, 
             color='red', markerfacecolor='darkred', markeredgecolor='white', markeredgewidth=2)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫–∏ —Å –ø–æ–¥–ø–∏—Å—è–º–∏
    for i, (noise, acc) in enumerate(zip(noise_percentages, accuracies)):
        plt.annotate(f'{acc:.3f}', (noise, acc), textcoords="offset points", 
                    xytext=(0,15), ha='center', fontweight='bold', fontsize=12)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    plt.title('üîä –í–ª–∏—è–Ω–∏–µ —à—É–º–∞ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å 1D Alexnet (20 –≤–∏–¥–æ–≤)', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)', fontsize=14)
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1.05)
    
    # –¶–≤–µ—Ç–æ–≤—ã–µ –∑–æ–Ω—ã
    plt.axhspan(0.9, 1.0, alpha=0.2, color='green', label='–û—Ç–ª–∏—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (>90%)')
    plt.axhspan(0.7, 0.9, alpha=0.2, color='yellow', label='–•–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (70-90%)')
    plt.axhspan(0.0, 0.7, alpha=0.2, color='red', label='–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (<70%)')
    
    plt.legend(loc='lower right')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'alexnet_20_accuracy_degradation_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    return filename

def create_detailed_report(results, species_names, noise_levels):
    """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç"""
    
    print("üìã –°–û–ó–î–ê–ù–ò–ï –î–ï–¢–ê–õ–¨–ù–û–ì–û –û–¢–ß–ï–¢–ê...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    report_lines = [
        "üîä –ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –®–£–ú–ê –ù–ê 1D ALEXNET (20 –í–ò–î–û–í)",
        "=" * 60,
        "",
        f"‚è∞ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"üå≤ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤: {len(species_names)}",
        f"üî¢ –¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ —É—Ä–æ–≤–Ω–∏ —à—É–º–∞: {[f'{n*100:.0f}%' for n in noise_levels]}",
        "",
        "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –£–†–û–í–ù–Ø–ú –®–£–ú–ê:",
        "-" * 40,
    ]
    
    for noise_level in noise_levels:
        accuracy = results[noise_level]['accuracy']
        report_lines.extend([
            f"",
            f"üîä –®—É–º {noise_level*100:.0f}%:",
            f"   –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({accuracy*100:.1f}%)",
            f"   –°—Ç–∞—Ç—É—Å: {'–û–¢–õ–ò–ß–ù–û' if accuracy > 0.9 else '–•–û–†–û–®–û' if accuracy > 0.7 else '–ü–õ–û–•–û'}",
        ])
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤–∏–¥–∞–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —à—É–º–∞
    report_lines.extend([
        "",
        "üìã –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û –í–ò–î–ê–ú:",
        "-" * 40,
    ])
    
    for species_idx, species in enumerate(species_names):
        report_lines.append(f"\n{species}:")
        for noise_level in noise_levels:
            cm_norm = results[noise_level]['confusion_matrix_normalized']
            species_accuracy = cm_norm[species_idx, species_idx]
            report_lines.append(f"   {noise_level*100:2.0f}% —à—É–º–∞: {species_accuracy:.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –≤–∏–¥–æ–≤
    report_lines.extend([
        "",
        "üõ°Ô∏è  –ê–ù–ê–õ–ò–ó –£–°–¢–û–ô–ß–ò–í–û–°–¢–ò –ö –®–£–ú–£:",
        "-" * 40,
    ])
    
    for species_idx, species in enumerate(species_names):
        accuracies = [results[noise]['confusion_matrix_normalized'][species_idx, species_idx] 
                     for noise in noise_levels]
        degradation = accuracies[0] - accuracies[-1]  # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É 0% –∏ max —à—É–º–æ–º
        
        if degradation < 0.1:
            status = "–û–ß–ï–ù–¨ –£–°–¢–û–ô–ß–ò–í"
        elif degradation < 0.3:
            status = "–£–°–¢–û–ô–ß–ò–í"
        elif degradation < 0.5:
            status = "–£–ú–ï–†–ï–ù–ù–û –£–°–¢–û–ô–ß–ò–í"
        else:
            status = "–ß–£–í–°–¢–í–ò–¢–ï–õ–ï–ù"
        
        report_lines.append(f"   {species:25} –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è: {degradation:.3f} ({status})")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    filename = f'alexnet_20_noise_analysis_report_{timestamp}.txt'
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"   –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
    return filename

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üîä" * 25)
    print("üîä –ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –®–£–ú–ê –ù–ê 1D ALEXNET")
    print("üîä" * 25)
    
    # –£—Ä–æ–≤–Ω–∏ —à—É–º–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    noise_levels = [0.0, 0.01, 0.05, 0.10, 0.20]  # 0%, 1%, 5%, 10%, 20%
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        spring_data, spring_labels = load_20_species_data()
        
        # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        X = preprocess_spectra(spring_data)
        
        # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(spring_labels)
        species_names = label_encoder.classes_
        
        # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        model, saved_label_encoder, saved_scaler = load_saved_models()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π label_encoder
        species_names = saved_label_encoder.classes_
        y_encoded = saved_label_encoder.transform(spring_labels)
        
        from tensorflow.keras.utils import to_categorical
        y_categorical = to_categorical(y_encoded)
        
        # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–µ –∂–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_categorical, 
            test_size=0.2, 
            random_state=42, 
            stratify=y_encoded
        )
        
        # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("üî¢ –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•...")
        scaler = StandardScaler()
        X_test_scaled = X_test.copy()
        
        for i in range(X_test.shape[0]):
            X_test_scaled[i, :, 0] = scaler.fit_transform(X_test[i, :, 0].reshape(-1, 1)).flatten()
        
        print(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test_scaled)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"üå≤ –í–∏–¥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(species_names)}")
        
        # 7. –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —à—É–º–∞
        results = create_noise_confusion_matrices(model, X_test_scaled, y_test, species_names, noise_levels)
        
        # 8. –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
        confusion_file = plot_noise_confusion_matrices(results, species_names, noise_levels)
        degradation_file = create_accuracy_degradation_plot(results, noise_levels)
        
        # 9. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_file = create_detailed_report(results, species_names, noise_levels)
        
        # 10. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
        print(f"\nüéâ –ê–ù–ê–õ–ò–ó –®–£–ú–ê –ó–ê–í–ï–†–®–ï–ù!")
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print(f"   üìä Confusion –º–∞—Ç—Ä–∏—Ü—ã: {confusion_file}")
        print(f"   üìà –ì—Ä–∞—Ñ–∏–∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏: {degradation_file}")
        print(f"   üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: {report_file}")
        
        # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
        print(f"\nüìã –ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê:")
        for noise_level in noise_levels:
            accuracy = results[noise_level]['accuracy']
            print(f"   {noise_level*100:2.0f}% —à—É–º–∞: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 