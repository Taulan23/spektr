#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
1D-AlexNet –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ 7 –≤–µ—Å–µ–Ω–Ω–∏—Ö –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å 3 –≥—Ä—É–ø–ø–∞–º–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import glob
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

def load_spring_7_species_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è 7 –≤–µ—Å–µ–Ω–Ω–∏—Ö –≤–∏–¥–æ–≤"""
    
    spring_folder = "../–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    
    print("üå± –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• 7 –í–ï–°–ï–ù–ù–ò–• –í–ò–î–û–í...")
    
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    
    all_data = []
    all_labels = []
    species_counts = {}
    
    for species in tree_types:
        species_folder = os.path.join(spring_folder, species)
        if not os.path.exists(species_folder):
            print(f"   ‚ö†Ô∏è  {species}: –ø–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            continue
            
        files = glob.glob(os.path.join(species_folder, "*.xlsx"))
        
        print(f"   üå≥ {species}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        species_counts[species] = len(files)
        
        species_data = []
        for file in files:
            try:
                df = pd.read_excel(file, header=None)
                spectrum = df.iloc[:, 1].values  # –í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞ - —Å–ø–µ–∫—Ç—Ä
                spectrum = spectrum[~pd.isna(spectrum)]  # –£–±–∏—Ä–∞–µ–º NaN
                species_data.append(spectrum)
            except Exception as e:
                print(f"     ‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {file}: {e}")
                continue
        
        if species_data:
            all_data.extend(species_data)
            all_labels.extend([species] * len(species_data))
    
    print(f"\nüìä –ò–¢–û–ì–û –ó–ê–ì–†–£–ñ–ï–ù–û:")
    for species, count in species_counts.items():
        print(f"   üå≥ {species}: {count} —Å–ø–µ–∫—Ç—Ä–æ–≤")
    
    print(f"\n‚úÖ –û–±—â–∏–π –∏—Ç–æ–≥: {len(all_data)} —Å–ø–µ–∫—Ç—Ä–æ–≤, {len(set(all_labels))} –≤–∏–¥–æ–≤")
    
    return all_data, all_labels, species_counts

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤"""
    
    print("üîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –°–ü–ï–ö–¢–†–û–í...")
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    min_length = min(len(spectrum) for spectrum in spectra_list)
    print(f"   üìè –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Å–ø–µ–∫—Ç—Ä—ã –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
    processed_spectra = []
    for spectrum in spectra_list:
        truncated = spectrum[:min_length]
        processed_spectra.append(truncated)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
    X = np.array(processed_spectra)
    print(f"   üìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    
    return X

def create_1d_alexnet_model(input_shape, num_classes):
    """–°–æ–∑–¥–∞–µ—Ç –†–ï–ê–õ–ò–°–¢–ò–ß–ù–£–Æ 1D-AlexNet —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤"""
    
    print("üèóÔ∏è –°–û–ó–î–ê–ù–ò–ï –†–ï–ê–õ–ò–°–¢–ò–ß–ù–û–ô 1D-AlexNet –ú–û–î–ï–õ–ò...")
    
    model = keras.Sequential([
        # –ì—Ä—É–ø–ø–∞ 1: –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ø–µ—Ä–≤–∞—è —Å–≤–µ—Ä—Ç–∫–∞
        layers.Conv1D(filters=32, kernel_size=50, strides=4, padding='same', 
                     activation='relu', input_shape=input_shape),
        layers.BatchNormalization(),
        layers.MaxPooling1D(pool_size=3, strides=2),
        layers.Dropout(0.25),
        
        # –ì—Ä—É–ø–ø–∞ 2: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        layers.Conv1D(filters=64, kernel_size=50, strides=1, padding='same', 
                     activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(pool_size=3, strides=2),
        layers.Dropout(0.25),
        
        # –ì—Ä—É–ø–ø–∞ 3: –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏
        layers.Conv1D(filters=128, kernel_size=2, strides=1, padding='same', 
                     activation='relu'),
        layers.BatchNormalization(),
        layers.Conv1D(filters=128, kernel_size=2, strides=1, padding='same', 
                     activation='relu'),
        layers.BatchNormalization(),
        layers.Conv1D(filters=64, kernel_size=2, strides=1, padding='same', 
                     activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(pool_size=3, strides=2),
        layers.Dropout(0.25),
        
        # Flatten –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º —Å–ª–æ—è–º
        layers.Flatten(),
        
        # –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print(f"   üìä –†–ï–ê–õ–ò–°–¢–ò–ß–ù–ê–Ø –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
    model.summary()
    
    return model

def add_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º –∫ –¥–∞–Ω–Ω—ã–º"""
    if noise_level == 0:
        return X
    noise = np.random.normal(0, noise_level, X.shape).astype(np.float32)
    return X + noise

def evaluate_with_noise(model, X_test, y_test, tree_types, noise_levels=[1, 5, 10]):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞"""
    
    print(f"\nüîä –û–¶–ï–ù–ö–ê –° –®–£–ú–û–ú...")
    
    results = {}
    confusion_matrices = {}
    
    for noise_level in noise_levels:
        print(f"\n{'='*60}")
        print(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞: {noise_level}%")
        print(f"{'='*60}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        X_test_noisy = add_noise(X_test, noise_level / 100.0)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)
        
        # –¢–æ—á–Ω–æ—Å—Ç—å
        accuracy = accuracy_score(y_test_classes, y_pred)
        results[noise_level] = accuracy
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ {noise_level}% —à—É–º–µ: {accuracy:.7f}")
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test_classes, y_pred)
        confusion_matrices[noise_level] = cm
        
        print(f"–û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_test_classes, y_pred, target_names=tree_types, digits=7))
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=tree_types, yticklabels=tree_types)
        plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {noise_level}% —à—É–º–∞')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.tight_layout()
        plt.savefig(f'confusion_matrix_{noise_level}percent.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º = 1)
        cm_normalized = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]
        cm_normalized = np.nan_to_num(cm_normalized)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0
        
        print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (—Å—É–º–º–∞ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º = 1):")
        print(cm_normalized)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        np.save(f'confusion_matrix_{noise_level}percent_normalized.npy', cm_normalized)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm_normalized, annot=True, fmt='.7f', cmap='Blues', 
                   xticklabels=tree_types, yticklabels=tree_types)
        plt.title(f'–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {noise_level}% —à—É–º–∞')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏')
        plt.tight_layout()
        plt.savefig(f'confusion_matrix_{noise_level}percent_normalized.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    return results, confusion_matrices

def save_network_params(model, tree_types, timestamp):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏"""
    
    params = {
        'architecture': '1D-AlexNet',
        'timestamp': timestamp,
        'input_shape': list(model.input_shape[1:]),
        'num_classes': len(tree_types),
        'tree_types': list(tree_types),
        'layers': []
    }
    
    for i, layer in enumerate(model.layers):
        layer_info = {
            'layer_number': i + 1,
            'type': layer.__class__.__name__,
            'config': layer.get_config()
        }
        params['layers'].append(layer_info)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    with open(f'network_params_{timestamp}.json', 'w', encoding='utf-8') as f:
        json.dump(params, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: network_params_{timestamp}.json")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üå≥" * 60)
    print("üå≥ 1D-AlexNet –î–õ–Ø 7 –í–ï–°–ï–ù–ù–ò–• –í–ò–î–û–í")
    print("üå≥ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –° –®–£–ú–û–ú")
    print("üå≥" * 60)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    spectra_list, labels, species_counts = load_spring_7_species_data()
    
    if len(spectra_list) == 0:
        print("‚ùå –û—à–∏–±–∫–∞: –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤
    X_spectra = preprocess_spectra(spectra_list)
    
    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    tree_types = label_encoder.classes_
    
    print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï:")
    print(f"   üî¢ –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X_spectra.shape}")
    print(f"   üè∑Ô∏è  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(tree_types)}")
    print(f"   üìã –í–∏–¥—ã: {list(tree_types)}")
    
    # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (80/20)
    X_train, X_test, y_train, y_test = train_test_split(
        X_spectra, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"\n‚úÇÔ∏è –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/TEST:")
    print(f"   üìä Train: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   üìä Test: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("\n‚öñÔ∏è –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 6. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è CNN
    X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
    X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)
    
    print(f"   üìä –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CNN: {X_train_cnn.shape}")
    
    # 7. One-hot encoding –¥–ª—è –º–µ—Ç–æ–∫
    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=len(tree_types))
    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=len(tree_types))
    
    # 8. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = create_1d_alexnet_model(
        input_shape=(X_train_cnn.shape[1], 1),
        num_classes=len(tree_types)
    )
    
    # 9. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüéì –û–ë–£–ß–ï–ù–ò–ï –†–ï–ê–õ–ò–°–¢–ò–ß–ù–û–ô –ú–û–î–ï–õ–ò...")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    batch_size = 16  # –ú–µ–Ω—å—à–∏–π batch size –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    epochs = 150     # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    validation_split = 0.2
    
    # –î–æ–±–∞–≤–ª—è–µ–º callbacks –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True
    )
    
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7
    )
    
    history = model.fit(
        X_train_cnn, y_train_onehot,
        batch_size=batch_size,
        epochs=epochs,
        validation_split=validation_split,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # 10. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –û–¶–ï–ù–ö–ê –ù–ê –ß–ò–°–¢–´–• –î–ê–ù–ù–´–•...")
    test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test_onehot, verbose=0)
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_accuracy:.7f}")
    
    # 11. –û—Ü–µ–Ω–∫–∞ —Å —à—É–º–æ–º
    results, confusion_matrices = evaluate_with_noise(
        model, X_test_cnn, y_test_onehot, tree_types, noise_levels=[1, 5, 10]
    )
    
    # 12. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å–µ—Ç–∏
    save_network_params(model, tree_types, timestamp)
    
    # 13. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.save(f'1d_alexnet_spring_7_species_{timestamp}.h5')
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: 1d_alexnet_spring_7_species_{timestamp}.h5")
    
    # 14. –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(f'training_history_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 15. –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ 1D-AlexNet:")
    print(f"   üìä –ß–∏—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ: {test_accuracy:.7f}")
    for noise_level, accuracy in results.items():
        print(f"   üìä {noise_level}% —à—É–º–∞: {accuracy:.7f}")
    
    print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print(f"üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"   üå≥ –ú–æ–¥–µ–ª—å: 1d_alexnet_spring_7_species_{timestamp}.h5")
    print(f"   ‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: network_params_{timestamp}.json")
    print(f"   üìà –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è: training_history_{timestamp}.png")
    print(f"   üìä –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫:")
    for noise_level in [1, 5, 10]:
        print(f"     üìä {noise_level}% —à—É–º–∞: confusion_matrix_{noise_level}percent.png")
        print(f"     üìä {noise_level}% —à—É–º–∞ (–Ω–æ—Ä–º.): confusion_matrix_{noise_level}percent_normalized.png")

if __name__ == "__main__":
    main() 