#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ú–ï–ì–ê-–°–ò–°–¢–ï–ú–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò 19 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –ª—É—á—à–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã
"""

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, PowerTransformer
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Attention, GlobalAveragePooling1D
from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Concatenate
import joblib
import warnings
from datetime import datetime
from scipy import signal
from scipy.stats import skew, kurtosis
import gc
import time

warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

def load_spring_data_19_species():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è 19 –≤–∏–¥–æ–≤ –¥–µ—Ä–µ–≤—å–µ–≤"""
    
    print("üå± –ó–ê–ì–†–£–ó–ö–ê –í–ï–°–ï–ù–ù–ò–• –î–ê–ù–ù–´–• (19 –í–ò–î–û–í)...")
    
    spring_folder = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    
    # –í—Å–µ –≤–∏–¥—ã –∫—Ä–æ–º–µ –ø—É—Å—Ç–æ–≥–æ –∫–ª–µ–Ω_–∞–º
    species_folders = [
        '–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–µ–ª—å_–≥–æ–ª—É–±–∞—è', '–∏–≤–∞', '–∫–∞—à—Ç–∞–Ω', '–∫–ª–µ–Ω', 
        '–ª–∏—Å—Ç–≤–µ–Ω–Ω–∏—Ü–∞', '–ª–∏–ø–∞', '–æ—Ä–µ—Ö', '–æ—Å–∏–Ω–∞', '—Ä—è–±–∏–Ω–∞', '—Å–∏—Ä–µ–Ω—å',
        '—Å–æ—Å–Ω–∞', '—Ç–æ–ø–æ–ª—å_–±–∞–ª—å–∑–∞–º–∏—á–µ—Å–∫–∏–π', '—Ç–æ–ø–æ–ª—å_—á–µ—Ä–Ω—ã–π', '—Ç—É—è', 
        '—á–µ—Ä–µ–º—É—Ö–∞', '—è—Å–µ–Ω—å'
    ]
    
    spring_data = []
    spring_labels = []
    
    for species in species_folders:
        folder_path = os.path.join(spring_folder, species)
        if os.path.exists(folder_path):
            files = glob.glob(os.path.join(folder_path, "*.xlsx"))
            print(f"   {species}: {len(files)} —Ñ–∞–π–ª–æ–≤")
            
            for file in files:
                try:
                    df = pd.read_excel(file)
                    if not df.empty and len(df.columns) >= 2:
                        # –ë–µ—Ä–µ–º –≤—Ç–æ—Ä—É—é –∫–æ–ª–æ–Ω–∫—É –∫–∞–∫ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        spectrum = df.iloc[:, 1].values
                        if len(spectrum) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞
                            spring_data.append(spectrum)
                            spring_labels.append(species)
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file}: {e}")
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(spring_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ {len(set(spring_labels))} –≤–∏–¥–∞–º")
    return spring_data, spring_labels

def load_summer_data_original():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ 7 –≤–∏–¥–æ–≤)"""
    
    print("‚òÄÔ∏è –ó–ê–ì–†–£–ó–ö–ê –õ–ï–¢–ù–ò–• –î–ê–ù–ù–´–• (7 –í–ò–î–û–í)...")
    
    species_folders = {
        '–±–µ—Ä–µ–∑–∞': '–±–µ—Ä–µ–∑–∞',
        '–¥—É–±': '–¥—É–±', 
        '–µ–ª—å': '–µ–ª—å',
        '–∫–ª–µ–Ω': '–∫–ª–µ–Ω',
        '–ª–∏–ø–∞': '–ª–∏–ø–∞',
        '–æ—Å–∏–Ω–∞': '–æ—Å–∏–Ω–∞',
        '—Å–æ—Å–Ω–∞': '—Å–æ—Å–Ω–∞'
    }
    
    summer_data = []
    summer_labels = []
    
    for species, folder in species_folders.items():
        files = glob.glob(os.path.join(folder, "*.xlsx"))
        print(f"   {species}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        for file in files:
            try:
                df = pd.read_excel(file)
                if not df.empty and len(df.columns) >= 2:
                    spectrum = df.iloc[:, 1].values
                    if len(spectrum) > 100:
                        summer_data.append(spectrum)
                        summer_labels.append(species)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file}: {e}")
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(summer_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ {len(set(summer_labels))} –≤–∏–¥–∞–º")
    return summer_data, summer_labels

def preprocess_spectra(spectra_list):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤ - –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω–µ"""
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
    min_length = min(len(spectrum) for spectrum in spectra_list)
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    # –û–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —Å–ø–µ–∫—Ç—Ä—ã –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
    processed_spectra = []
    for spectrum in spectra_list:
        # –£–¥–∞–ª—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        spectrum_clean = spectrum[~np.isnan(spectrum)]
        if len(spectrum_clean) >= min_length:
            processed_spectra.append(spectrum_clean[:min_length])
    
    return np.array(processed_spectra)

def extract_mega_features(spectra, labels=None):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è 19 –≤–∏–¥–æ–≤"""
    
    print("üß† –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ï–ì–ê-–ü–†–ò–ó–ù–ê–ö–û–í...")
    n_samples, n_channels = spectra.shape
    all_features = []
    
    # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∏–¥—ã
    if labels is not None:
        unique_species = sorted(list(set(labels)))
        print(f"   –í–∏–¥—ã: {unique_species}")
    
    for i, spectrum in enumerate(spectra):
        features = []
        
        # 1. –ë–ê–ó–û–í–´–ï –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò (20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        features.extend([
            np.mean(spectrum), np.std(spectrum), np.median(spectrum),
            np.min(spectrum), np.max(spectrum), np.ptp(spectrum),
            np.var(spectrum), skew(spectrum), kurtosis(spectrum),
            np.sqrt(np.mean(spectrum**2)),  # RMS
            np.mean(np.abs(spectrum - np.median(spectrum))),  # MAD
            np.percentile(spectrum, 10), np.percentile(spectrum, 25),
            np.percentile(spectrum, 75), np.percentile(spectrum, 90),
            np.percentile(spectrum, 95), np.percentile(spectrum, 99),
            np.sum(spectrum > np.mean(spectrum)) / len(spectrum),  # % –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
            np.sum(spectrum > 0) / len(spectrum),  # % –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
            len(np.where(np.diff(spectrum) > 0)[0]) / len(spectrum)  # % –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏—Ö
        ])
        
        # 2. –ö–í–ê–ù–¢–ò–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        quantiles = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99]
        for q in quantiles:
            features.append(np.percentile(spectrum, q*100))
        features.append(np.percentile(spectrum, 75) - np.percentile(spectrum, 25))  # IQR
        
        # 3. –ü–†–û–ò–ó–í–û–î–ù–´–ï (30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        if len(spectrum) > 3:
            # –ü–µ—Ä–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
            diff1 = np.diff(spectrum)
            features.extend([
                np.mean(diff1), np.std(diff1), np.median(diff1),
                np.min(diff1), np.max(diff1), np.ptp(diff1),
                skew(diff1), kurtosis(diff1), np.mean(np.abs(diff1)),
                np.sqrt(np.mean(diff1**2))
            ])
            
            # –í—Ç–æ—Ä–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
            if len(diff1) > 1:
                diff2 = np.diff(diff1)
                features.extend([
                    np.mean(diff2), np.std(diff2), np.median(diff2),
                    np.min(diff2), np.max(diff2), np.ptp(diff2),
                    skew(diff2), kurtosis(diff2), np.mean(np.abs(diff2)),
                    np.sqrt(np.mean(diff2**2))
                ])
                
                # –¢—Ä–µ—Ç—å—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
                if len(diff2) > 1:
                    diff3 = np.diff(diff2)
                    features.extend([
                        np.mean(diff3), np.std(diff3), np.median(diff3),
                        np.min(diff3), np.max(diff3), np.ptp(diff3),
                        skew(diff3), kurtosis(diff3), np.mean(np.abs(diff3)),
                        np.sqrt(np.mean(diff3**2))
                    ])
        
        # 4. –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –†–ï–ì–ò–û–ù–´ (60 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ø–µ–∫—Ç—Ä –Ω–∞ 20 —á–∞—Å—Ç–µ–π –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        n_regions = 20
        region_size = len(spectrum) // n_regions
        
        for j in range(n_regions):
            start_idx = j * region_size
            end_idx = min((j + 1) * region_size, len(spectrum))
            region = spectrum[start_idx:end_idx]
            
            if len(region) > 0:
                features.extend([
                    np.mean(region),
                    np.std(region),
                    np.max(region) - np.min(region)
                ])
        
        # 5. –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –ú–û–ú–ï–ù–¢–´ –ò –≠–ù–ï–†–ì–ò–ò (25 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –¶–µ–Ω—Ç—Ä–æ–∏–¥ —Å–ø–µ–∫—Ç—Ä–∞
        indices = np.arange(len(spectrum))
        if np.sum(spectrum) != 0:
            centroid = np.sum(indices * spectrum) / np.sum(spectrum)
        else:
            centroid = len(spectrum) / 2
        features.append(centroid)
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–∞–∑–±—Ä–æ—Å
        spread = np.sqrt(np.sum(((indices - centroid) ** 2) * spectrum) / np.sum(spectrum)) if np.sum(spectrum) != 0 else 0
        features.append(spread)
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è
        if spread != 0:
            spec_skew = np.sum(((indices - centroid) ** 3) * spectrum) / (np.sum(spectrum) * (spread ** 3))
        else:
            spec_skew = 0
        features.append(spec_skew)
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —ç–∫—Å—Ü–µ—Å—Å
        if spread != 0:
            spec_kurt = np.sum(((indices - centroid) ** 4) * spectrum) / (np.sum(spectrum) * (spread ** 4))
        else:
            spec_kurt = 0
        features.append(spec_kurt)
        
        # –≠–Ω–µ—Ä–≥–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ø–æ–ª–æ—Å–∞—Ö (20 –ø–æ–ª–æ—Å)
        band_size = len(spectrum) // 20
        for k in range(20):
            start_idx = k * band_size
            end_idx = min((k + 1) * band_size, len(spectrum))
            band_energy = np.sum(spectrum[start_idx:end_idx] ** 2)
            features.append(band_energy)
        
        # 6. –û–¢–ù–û–®–ï–ù–ò–Ø –ú–ï–ñ–î–£ –†–ï–ì–ò–û–ù–ê–ú–ò (45 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ —Å–ø–µ–∫—Ç—Ä–∞
        quarter_size = len(spectrum) // 4
        q1 = spectrum[:quarter_size]
        q2 = spectrum[quarter_size:2*quarter_size]
        q3 = spectrum[2*quarter_size:3*quarter_size]
        q4 = spectrum[3*quarter_size:]
        
        # –û—Ç–Ω–æ—à–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        features.extend([
            np.mean(q1) / (np.mean(q2) + 1e-8),
            np.mean(q1) / (np.mean(q3) + 1e-8),
            np.mean(q1) / (np.mean(q4) + 1e-8),
            np.mean(q2) / (np.mean(q3) + 1e-8),
            np.mean(q2) / (np.mean(q4) + 1e-8),
            np.mean(q3) / (np.mean(q4) + 1e-8)
        ])
        
        # –û—Ç–Ω–æ—à–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–∏–π
        features.extend([
            np.sum(q1**2) / (np.sum(q2**2) + 1e-8),
            np.sum(q1**2) / (np.sum(q3**2) + 1e-8),
            np.sum(q1**2) / (np.sum(q4**2) + 1e-8),
            np.sum(q2**2) / (np.sum(q3**2) + 1e-8),
            np.sum(q2**2) / (np.sum(q4**2) + 1e-8),
            np.sum(q3**2) / (np.sum(q4**2) + 1e-8)
        ])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        half_size = len(spectrum) // 2
        first_half = spectrum[:half_size]
        second_half = spectrum[half_size:]
        
        features.extend([
            np.mean(first_half) / (np.mean(second_half) + 1e-8),
            np.std(first_half) / (np.std(second_half) + 1e-8),
            np.max(first_half) / (np.max(second_half) + 1e-8),
            np.min(first_half) / (np.min(second_half) + 1e-8),
            np.sum(first_half**2) / (np.sum(second_half**2) + 1e-8)
        ])
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –Ω–∞—á–∞–ª–∞, —Å–µ—Ä–µ–¥–∏–Ω—ã –∏ –∫–æ–Ω—Ü–∞
        third_size = len(spectrum) // 3
        begin = spectrum[:third_size]
        middle = spectrum[third_size:2*third_size]
        end = spectrum[2*third_size:]
        
        features.extend([
            np.mean(begin) / (np.mean(middle) + 1e-8),
            np.mean(begin) / (np.mean(end) + 1e-8),
            np.mean(middle) / (np.mean(end) + 1e-8),
            np.std(begin) / (np.std(middle) + 1e-8),
            np.std(begin) / (np.std(end) + 1e-8),
            np.std(middle) / (np.std(end) + 1e-8)
        ])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        features.extend([
            np.sum(begin) / (np.sum(spectrum) + 1e-8),
            np.sum(middle) / (np.sum(spectrum) + 1e-8),
            np.sum(end) / (np.sum(spectrum) + 1e-8),
            np.max(begin) / (np.max(spectrum) + 1e-8),
            np.max(middle) / (np.max(spectrum) + 1e-8),
            np.max(end) / (np.max(spectrum) + 1e-8),
            np.ptp(begin) / (np.ptp(spectrum) + 1e-8),
            np.ptp(middle) / (np.ptp(spectrum) + 1e-8),
            np.ptp(end) / (np.ptp(spectrum) + 1e-8)
        ])
        
        # 7. –õ–û–ö–ê–õ–¨–ù–´–ï –≠–ö–°–¢–†–ï–ú–£–ú–´ (20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        try:
            # –ü–æ–∏—Å–∫ –ø–∏–∫–æ–≤
            peaks, _ = signal.find_peaks(spectrum, height=np.percentile(spectrum, 70))
            valleys, _ = signal.find_peaks(-spectrum, height=-np.percentile(spectrum, 30))
            
            features.extend([
                len(peaks), len(valleys),
                len(peaks) / len(spectrum), len(valleys) / len(spectrum),
                np.mean(spectrum[peaks]) if len(peaks) > 0 else np.mean(spectrum),
                np.mean(spectrum[valleys]) if len(valleys) > 0 else np.mean(spectrum),
                np.std(spectrum[peaks]) if len(peaks) > 0 else 0,
                np.std(spectrum[valleys]) if len(valleys) > 0 else 0,
                np.max(spectrum[peaks]) if len(peaks) > 0 else np.max(spectrum),
                np.min(spectrum[valleys]) if len(valleys) > 0 else np.min(spectrum)
            ])
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –ø–∏–∫–∞–º–∏
            if len(peaks) > 1:
                peak_distances = np.diff(peaks)
                features.extend([
                    np.mean(peak_distances), np.std(peak_distances),
                    np.min(peak_distances), np.max(peak_distances)
                ])
            else:
                features.extend([0, 0, 0, 0])
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–ø–∞–¥–∏–Ω–∞–º–∏
            if len(valleys) > 1:
                valley_distances = np.diff(valleys)
                features.extend([
                    np.mean(valley_distances), np.std(valley_distances),
                    np.min(valley_distances), np.max(valley_distances)
                ])
            else:
                features.extend([0, 0, 0, 0])
            
            # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–∏–∫–æ–≤ –∫ –≤–ø–∞–¥–∏–Ω–∞–º
            features.extend([
                len(peaks) / (len(valleys) + 1),
                (len(peaks) + len(valleys)) / len(spectrum)
            ])
            
        except:
            # –ï—Å–ª–∏ –ø–æ–∏—Å–∫ –ø–∏–∫–æ–≤ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            features.extend([0] * 20)
        
        # 8. –í–ò–î–û–°–ü–ï–¶–ò–§–ò–ß–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò (100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ —Å 7 –≤–∏–¥–∞–º–∏)
        
        # –ö–ª—é—á–µ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤–∏–¥–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ)
        key_channels = {
            '–±–µ—Ä–µ–∑–∞': [50, 75, 100, 150, 200, 250, 300],
            '–¥—É–±': [60, 90, 120, 180, 240, 280, 320],
            '–µ–ª—å': [40, 80, 110, 160, 210, 260, 310],
            '–µ–ª—å_–≥–æ–ª—É–±–∞—è': [45, 85, 115, 165, 215, 265, 315],
            '–∫–ª–µ–Ω': [70, 95, 130, 170, 220, 270, 330],
            '–ª–∏–ø–∞': [55, 85, 125, 175, 225, 275, 325],
            '–æ—Å–∏–Ω–∞': [65, 100, 135, 185, 235, 285, 335],
            '—Å–æ—Å–Ω–∞': [35, 70, 105, 140, 190, 240, 290]
        }
        
        # –î–ª—è –Ω–æ–≤—ã—Ö –≤–∏–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∫–∞–Ω–∞–ª—ã
        general_key_channels = [30, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–Ω–∞–ª–∞—Ö
        for ch in general_key_channels:
            if ch < len(spectrum):
                features.append(spectrum[ch])
            else:
                features.append(0)
        
        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–∫–Ω–∞—Ö –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
        window_size = 5
        for ch in general_key_channels:
            if ch < len(spectrum):
                start_idx = max(0, ch - window_size)
                end_idx = min(len(spectrum), ch + window_size + 1)
                window_mean = np.mean(spectrum[start_idx:end_idx])
                features.append(window_mean)
            else:
                features.append(0)
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–Ω–∞–ª–∞—Ö
        if len(spectrum) > 1:
            diff_spectrum = np.diff(spectrum)
            for ch in general_key_channels[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
                if ch < len(diff_spectrum):
                    features.append(diff_spectrum[ch])
                else:
                    features.append(0)
        else:
            features.extend([0] * 10)
        
        # –í—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–Ω–∞–ª–∞—Ö
        if len(spectrum) > 2:
            diff2_spectrum = np.diff(np.diff(spectrum))
            for ch in general_key_channels[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
                if ch < len(diff2_spectrum):
                    features.append(diff2_spectrum[ch])
                else:
                    features.append(0)
        else:
            features.extend([0] * 10)
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã –∏ –º–∏–Ω–∏–º—É–º—ã –≤ —Ä–µ–≥–∏–æ–Ω–∞—Ö
        for start_ch in range(0, min(len(spectrum), 400), 50):
            end_ch = min(start_ch + 50, len(spectrum))
            region = spectrum[start_ch:end_ch]
            if len(region) > 0:
                features.extend([np.max(region), np.min(region), np.argmax(region), np.argmin(region)])
            else:
                features.extend([0, 0, 0, 0])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∏—Å–ª–æ–≤—ã–µ
        features = [float(f) if not np.isnan(f) and not np.isinf(f) else 0.0 for f in features]
        all_features.append(features)
        
        if (i + 1) % 500 == 0:
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –æ–±—Ä–∞–∑—Ü–æ–≤...")
    
    features_array = np.array(all_features)
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {features_array.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {features_array.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    return features_array

def create_transformer_model(input_dim, num_classes, species_names):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –¥–ª—è 19 –≤–∏–¥–æ–≤"""
    
    print("ü§ñ –°–û–ó–î–ê–ù–ò–ï TRANSFORMER –ú–û–î–ï–õ–ò...")
    
    # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    inputs = Input(shape=(input_dim,), name='features_input')
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è Transformer
    x = tf.expand_dims(inputs, axis=1)  # [batch, 1, features]
    
    # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    x = Dense(512, activation='relu', name='embedding')(x)
    x = Dropout(0.2)(x)
    
    # Multi-Head Attention –±–ª–æ–∫–∏
    for i in range(3):
        # Self-attention
        attn_out = MultiHeadAttention(
            num_heads=8, 
            key_dim=64,
            name=f'multihead_attention_{i}'
        )(x, x)
        attn_out = Dropout(0.1)(attn_out)
        x = LayerNormalization(name=f'layer_norm_1_{i}')(x + attn_out)
        
        # Feed forward
        ff_out = Dense(1024, activation='relu', name=f'ff_1_{i}')(x)
        ff_out = Dropout(0.1)(ff_out)
        ff_out = Dense(512, name=f'ff_2_{i}')(ff_out)
        ff_out = Dropout(0.1)(ff_out)
        x = LayerNormalization(name=f'layer_norm_2_{i}')(x + ff_out)
    
    # Global pooling
    x = GlobalAveragePooling1D(name='global_avg_pool')(x)
    
    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ç–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø –≤–∏–¥–æ–≤
    
    # –í–µ—Ç–∫–∞ –¥–ª—è —Ö–≤–æ–π–Ω—ã—Ö
    conifer_species = ['–µ–ª—å', '–µ–ª—å_–≥–æ–ª—É–±–∞—è', '–ª–∏—Å—Ç–≤–µ–Ω–Ω–∏—Ü–∞', '—Å–æ—Å–Ω–∞', '—Ç—É—è']
    conifer_branch = Dense(256, activation='relu', name='conifer_branch_1')(x)
    conifer_branch = BatchNormalization(name='conifer_bn_1')(conifer_branch)
    conifer_branch = Dropout(0.3)(conifer_branch)
    conifer_branch = Dense(128, activation='relu', name='conifer_branch_2')(conifer_branch)
    conifer_branch = BatchNormalization(name='conifer_bn_2')(conifer_branch)
    conifer_branch = Dropout(0.2)(conifer_branch)
    
    # –í–µ—Ç–∫–∞ –¥–ª—è –ª–∏—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ä–µ–≤—å–µ–≤
    deciduous_species = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—è—Å–µ–Ω—å', '–∫–∞—à—Ç–∞–Ω', '–æ—Ä–µ—Ö']
    deciduous_branch = Dense(256, activation='relu', name='deciduous_branch_1')(x)
    deciduous_branch = BatchNormalization(name='deciduous_bn_1')(deciduous_branch)
    deciduous_branch = Dropout(0.3)(deciduous_branch)
    deciduous_branch = Dense(128, activation='relu', name='deciduous_branch_2')(deciduous_branch)
    deciduous_branch = BatchNormalization(name='deciduous_bn_2')(deciduous_branch)
    deciduous_branch = Dropout(0.2)(deciduous_branch)
    
    # –í–µ—Ç–∫–∞ –¥–ª—è –∫—É—Å—Ç–∞—Ä–Ω–∏–∫–æ–≤ –∏ —Ç–æ–ø–æ–ª–µ–π
    shrub_species = ['—Å–∏—Ä–µ–Ω—å', '—á–µ—Ä–µ–º—É—Ö–∞', '—Ä—è–±–∏–Ω–∞', '—Ç–æ–ø–æ–ª—å_—á–µ—Ä–Ω—ã–π', '—Ç–æ–ø–æ–ª—å_–±–∞–ª—å–∑–∞–º–∏—á–µ—Å–∫–∏–π', '–∏–≤–∞']
    shrub_branch = Dense(256, activation='relu', name='shrub_branch_1')(x)
    shrub_branch = BatchNormalization(name='shrub_bn_1')(shrub_branch)
    shrub_branch = Dropout(0.3)(shrub_branch)
    shrub_branch = Dense(128, activation='relu', name='shrub_branch_2')(shrub_branch)
    shrub_branch = BatchNormalization(name='shrub_bn_2')(shrub_branch)
    shrub_branch = Dropout(0.2)(shrub_branch)
    
    # –û–±—â–∞—è –≤–µ—Ç–∫–∞
    general_branch = Dense(512, activation='relu', name='general_branch_1')(x)
    general_branch = BatchNormalization(name='general_bn_1')(general_branch)
    general_branch = Dropout(0.4)(general_branch)
    general_branch = Dense(256, activation='relu', name='general_branch_2')(general_branch)
    general_branch = BatchNormalization(name='general_bn_2')(general_branch)
    general_branch = Dropout(0.3)(general_branch)
    general_branch = Dense(128, activation='relu', name='general_branch_3')(general_branch)
    general_branch = BatchNormalization(name='general_bn_3')(general_branch)
    general_branch = Dropout(0.2)(general_branch)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤–µ—Ç–∫–∏
    combined = Concatenate(name='combine_branches')([
        conifer_branch, deciduous_branch, shrub_branch, general_branch
    ])
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
    x = Dense(512, activation='relu', name='final_dense_1')(combined)
    x = BatchNormalization(name='final_bn_1')(x)
    x = Dropout(0.4)(x)
    
    x = Dense(256, activation='relu', name='final_dense_2')(x)
    x = BatchNormalization(name='final_bn_2')(x)
    x = Dropout(0.3)(x)
    
    x = Dense(128, activation='relu', name='final_dense_3')(x)
    x = BatchNormalization(name='final_bn_3')(x)
    x = Dropout(0.2)(x)
    
    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    outputs = Dense(num_classes, activation='softmax', name='classification_output')(x)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = Model(inputs=inputs, outputs=outputs, name='Tree_Species_Transformer_19')
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    model.compile(
        optimizer=Adam(learning_rate=0.0001, weight_decay=1e-5),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def create_ensemble_system(X_train, y_train, X_test, y_test, species_names):
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
    
    print("üéØ –°–û–ó–î–ê–ù–ò–ï –ú–ï–ì–ê-–ê–ù–°–ê–ú–ë–õ–Ø...")
    
    models = {}
    predictions = {}
    
    # 1. Random Forest —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    print("   –û–±—É—á–µ–Ω–∏–µ Random Forest...")
    rf = RandomForestClassifier(
        n_estimators=500,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )
    rf.fit(X_train, y_train)
    models['random_forest'] = rf
    predictions['random_forest'] = rf.predict_proba(X_test)
    
    # 2. Extra Trees
    print("   –û–±—É—á–µ–Ω–∏–µ Extra Trees...")
    et = ExtraTreesClassifier(
        n_estimators=500,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    )
    et.fit(X_train, y_train)
    models['extra_trees'] = et
    predictions['extra_trees'] = et.predict_proba(X_test)
    
    # 3. Gradient Boosting
    print("   –û–±—É—á–µ–Ω–∏–µ Gradient Boosting...")
    gb = GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=8,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        subsample=0.8
    )
    gb.fit(X_train, y_train)
    models['gradient_boosting'] = gb
    predictions['gradient_boosting'] = gb.predict_proba(X_test)
    
    # 4. SVM
    print("   –û–±—É—á–µ–Ω–∏–µ SVM...")
    svm = SVC(
        kernel='rbf',
        C=10,
        gamma='scale',
        probability=True,
        random_state=42,
        class_weight='balanced'
    )
    svm.fit(X_train, y_train)
    models['svm'] = svm
    predictions['svm'] = svm.predict_proba(X_test)
    
    # 5. Neural Network (sklearn)
    print("   –û–±—É—á–µ–Ω–∏–µ MLP...")
    mlp = MLPClassifier(
        hidden_layer_sizes=(512, 256, 128),
        activation='relu',
        solver='adam',
        alpha=0.001,
        learning_rate_init=0.001,
        max_iter=500,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.1
    )
    mlp.fit(X_train, y_train)
    models['mlp'] = mlp
    predictions['mlp'] = mlp.predict_proba(X_test)
    
    return models, predictions

def advanced_ensemble_fusion(predictions, method='weighted_average'):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    
    if method == 'weighted_average':
        # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        weights = {
            'random_forest': 0.25,
            'extra_trees': 0.20,
            'gradient_boosting': 0.20,
            'svm': 0.15,
            'mlp': 0.20
        }
        
        ensemble_pred = np.zeros_like(list(predictions.values())[0])
        for model_name, pred in predictions.items():
            ensemble_pred += weights[model_name] * pred
            
    elif method == 'voting':
        # –ü—Ä–æ—Å—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
        ensemble_pred = np.mean(list(predictions.values()), axis=0)
        
    elif method == 'rank_fusion':
        # –†–∞–Ω–≥–æ–≤–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        ranked_preds = []
        for pred in predictions.values():
            ranks = np.argsort(np.argsort(-pred, axis=1), axis=1)
            ranked_preds.append(ranks)
        
        avg_ranks = np.mean(ranked_preds, axis=0)
        ensemble_pred = np.argsort(np.argsort(avg_ranks, axis=1), axis=1)
        ensemble_pred = ensemble_pred / ensemble_pred.sum(axis=1, keepdims=True)
    
    return ensemble_pred

def analyze_mega_results(y_true, predictions, species_names, save_prefix="mega_19_species"):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è 19 –≤–∏–¥–æ–≤"""
    
    print("üìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
    
    results = {}
    
    for model_name, pred_proba in predictions.items():
        y_pred = np.argmax(pred_proba, axis=1)
        accuracy = accuracy_score(y_true, y_pred)
        results[model_name] = accuracy
        
        print(f"\nüîπ {model_name.upper()}: {accuracy:.3f}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        report = classification_report(y_true, y_pred, target_names=species_names, output_dict=True, zero_division=0)
        
        print("   –ü–æ –≤–∏–¥–∞–º:")
        for i, species in enumerate(species_names):
            if species in report:
                precision = report[species]['precision']
                recall = report[species]['recall']
                f1 = report[species]['f1-score']
                print(f"     {species}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
    
    # –ê–Ω—Å–∞–º–±–ª—å
    print("\nüéØ –ê–ù–°–ê–ú–ë–õ–¨ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    
    # –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    ensemble_methods = ['weighted_average', 'voting', 'rank_fusion']
    
    for method in ensemble_methods:
        ensemble_pred_proba = advanced_ensemble_fusion(predictions, method)
        ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)
        ensemble_accuracy = accuracy_score(y_true, ensemble_pred)
        
        print(f"\nüî∏ {method.upper()}: {ensemble_accuracy:.3f}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤–∏–¥–∞–º
        report = classification_report(y_true, ensemble_pred, target_names=species_names, output_dict=True, zero_division=0)
        
        print("   –ü–æ –≤–∏–¥–∞–º:")
        for i, species in enumerate(species_names):
            if species in report:
                precision = report[species]['precision']
                recall = report[species]['recall']
                f1 = report[species]['f1-score']
                print(f"     {species}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
    
    # –°–æ–∑–¥–∞–µ–º confusion matrix –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è
    best_ensemble = advanced_ensemble_fusion(predictions, 'weighted_average')
    best_pred = np.argmax(best_ensemble, axis=1)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(20, 16))
    
    # Confusion Matrix
    plt.subplot(2, 2, 1)
    cm = confusion_matrix(y_true, best_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=species_names, yticklabels=species_names)
    plt.title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
    plt.xlabel('Predicted', fontsize=12)
    plt.ylabel('True', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    
    # Accuracy by species
    plt.subplot(2, 2, 2)
    species_accuracy = []
    for i, species in enumerate(species_names):
        mask = y_true == i
        if np.sum(mask) > 0:
            acc = accuracy_score(y_true[mask], best_pred[mask])
            species_accuracy.append(acc)
        else:
            species_accuracy.append(0)
    
    bars = plt.bar(range(len(species_names)), species_accuracy, color='skyblue', alpha=0.8)
    plt.title('Accuracy by Species', fontsize=14, fontweight='bold')
    plt.xlabel('Species', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.xticks(range(len(species_names)), species_names, rotation=45, ha='right')
    plt.ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, bar in enumerate(bars):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{species_accuracy[i]:.2f}', ha='center', va='bottom', fontweight='bold')
    
    # Model comparison
    plt.subplot(2, 2, 3)
    model_names = list(results.keys())
    model_accuracies = list(results.values())
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω—Å–∞–º–±–ª—å
    ensemble_acc = accuracy_score(y_true, best_pred)
    model_names.append('Ensemble')
    model_accuracies.append(ensemble_acc)
    
    bars = plt.bar(model_names, model_accuracies, color='coral', alpha=0.8)
    plt.title('Model Comparison', fontsize=14, fontweight='bold')
    plt.xlabel('Model', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1)
    
    for i, bar in enumerate(bars):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{model_accuracies[i]:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # Statistics
    plt.subplot(2, 2, 4)
    plt.axis('off')
    
    stats_text = f"""
üèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ 19 –í–ò–î–û–í:

üìä –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {ensemble_acc:.1%}

ü•á –¢–æ–ø-5 –≤–∏–¥–æ–≤:
{chr(10).join([f"   {species_names[i]}: {species_accuracy[i]:.1%}" 
               for i in np.argsort(species_accuracy)[-5:][::-1]])}

üîÑ –•—É–¥—à–∏–µ –≤–∏–¥—ã:
{chr(10).join([f"   {species_names[i]}: {species_accuracy[i]:.1%}" 
               for i in np.argsort(species_accuracy)[:3]])}

üéØ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {max(results, key=results.get)}
   –¢–æ—á–Ω–æ—Å—Ç—å: {max(results.values()):.1%}

‚úÖ –í–∏–¥—ã > 70%: {sum(1 for acc in species_accuracy if acc > 0.7)}
‚ö° –í–∏–¥—ã > 50%: {sum(1 for acc in species_accuracy if acc > 0.5)}
üîß –í–∏–¥—ã < 30%: {sum(1 for acc in species_accuracy if acc < 0.3)}
    """
    
    plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,
            verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(f'{save_prefix}_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return results, ensemble_acc

def save_mega_system(models, scaler, label_encoder, species_names):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å—é —Å–∏—Å—Ç–µ–º—É"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
    for name, model in models.items():
        joblib.dump(model, f'mega_19_species_{name}_{timestamp}.pkl')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
    joblib.dump(scaler, f'mega_19_species_scaler_{timestamp}.pkl')
    joblib.dump(label_encoder, f'mega_19_species_label_encoder_{timestamp}.pkl')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'species_names': species_names,
        'n_species': len(species_names),
        'timestamp': timestamp,
        'models': list(models.keys())
    }
    
    joblib.dump(metadata, f'mega_19_species_metadata_{timestamp}.pkl')
    
    print(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å timestamp: {timestamp}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üå≤" * 20)
    print("üöÄ –ú–ï–ì–ê-–°–ò–°–¢–ï–ú–ê –î–õ–Ø 19 –í–ò–î–û–í –î–ï–†–ï–í–¨–ï–í")
    print("üå≤" * 20)
    
    start_time = time.time()
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    spring_data, spring_labels = load_spring_data_19_species()
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ–∫—Ç—Ä–æ–≤
    print("\nüîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –°–ü–ï–ö–¢–†–û–í...")
    X_spring = preprocess_spectra(spring_data)
    y_spring = np.array(spring_labels)
    
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_features = extract_mega_features(X_spring, spring_labels)
    
    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y_spring)
    species_names = label_encoder.classes_
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–¢–ê–°–ï–¢–ê:")
    print(f"   –í–∏–¥—ã: {len(species_names)}")
    print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {len(X_features)}")
    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_features.shape[1]}")
    
    unique, counts = np.unique(y_encoded, return_counts=True)
    for i, (species, count) in enumerate(zip(species_names, counts)):
        print(f"   {species}: {count} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y_encoded, 
        test_size=0.2, 
        random_state=42, 
        stratify=y_encoded
    )
    
    # 6. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüîß –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í...")
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
    imputer = SimpleImputer(strategy='mean')
    X_train_imputed = imputer.fit_transform(X_train)
    X_test_imputed = imputer.transform(X_test)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º PowerTransformer –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    power_transformer = PowerTransformer(method='yeo-johnson')
    X_train_power = power_transformer.fit_transform(X_train_imputed)
    X_test_power = power_transformer.transform(X_test_imputed)
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train_power)
    X_test_scaled = scaler.transform(X_test_power)
    
    print(f"   –ü—Ä–æ–≤–µ—Ä–∫–∞ NaN –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {np.isnan(X_train_scaled).sum()}")
    
    # 7. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    models, predictions = create_ensemble_system(
        X_train_scaled, y_train, X_test_scaled, y_test, species_names
    )
    
    # 8. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results, best_accuracy = analyze_mega_results(
        y_test, predictions, species_names, "mega_19_species"
    )
    
    # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    save_mega_system(models, scaler, label_encoder, species_names)
    
    # 10. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    total_time = time.time() - start_time
    
    print(f"\nüéâ –ú–ï–ì–ê-–°–ò–°–¢–ï–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"üèÜ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.1%}")
    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–¥–æ–≤: {len(species_names)}")
    print(f"üéØ –°—Ç–∞—Ç—É—Å: {'–£–°–ü–ï–•' if best_accuracy > 0.6 else '–¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–ô'}")
    
    return models, results, best_accuracy

if __name__ == "__main__":
    main() 