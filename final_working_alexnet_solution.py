import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, RobustScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

def load_spectral_data_7_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 7 –≤–∏–¥–æ–≤"""
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    species_folders = ["–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"]
    
    all_data = []
    all_labels = []
    
    for species in species_folders:
        species_path = f"{base_path}/{species}"
        try:
            import os
            import glob
            excel_files = glob.glob(f"{species_path}/*_vis.xlsx")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        spectral_data = df[numeric_cols[0]].values
                        if len(spectral_data) > 0:
                            all_data.append(spectral_data)
                            all_labels.append(species)
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {species}: {e}")
            continue
    
    if len(all_data) == 0:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return None, None
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {len(np.unique(y))} –≤–∏–¥–æ–≤")
    return X, y

def create_improved_alexnet(input_shape, num_classes):
    """–£–õ–£–ß–®–ï–ù–ù–ê–Ø –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è 1D-AlexNet"""
    model = Sequential([
        # –ì—Ä—É–ø–ø–∞ 1: 16 —Ñ–∏–ª—å—Ç—Ä–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 10)
        Conv1D(16, 50, strides=4, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        # –ì—Ä—É–ø–ø–∞ 2: 32 —Ñ–∏–ª—å—Ç—Ä–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 20)
        Conv1D(32, 50, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        # –ì—Ä—É–ø–ø–∞ 3: 64 ‚Üí 64 ‚Üí 32 —Ñ–∏–ª—å—Ç—Ä–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        Conv1D(64, 3, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        Conv1D(64, 3, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        Conv1D(32, 3, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        Flatten(),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏: 512 ‚Üí 256 ‚Üí 7 (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),  # –£–º–µ–Ω—å—à–µ–Ω dropout
        
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.0005),  # –£–º–µ–Ω—å—à–µ–Ω learning rate
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def add_gaussian_noise(data, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞"""
    if noise_level == 0:
        return data
    
    noise = np.random.normal(0, noise_level * np.std(data), data.shape)
    return data + noise

def test_improved_alexnet():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print("=== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–ê–Ø 1D-ALEXNET ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    data, labels = load_spectral_data_7_species()
    if data is None:
        return None, None, None, False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - min: {np.min(data):.4f}, max: {np.max(data):.4f}, std: {np.std(data):.4f}")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º RobustScaler (–º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
    print("2. –ú—è–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (RobustScaler)...")
    scaler = RobustScaler()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–ª—è —Å–∫–∞–ª–µ—Ä–∞
    data_reshaped = data.reshape(-1, data.shape[-1])
    data_scaled = scaler.fit_transform(data_reshaped)
    X_processed = data_scaled.reshape(data.shape)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    print(f"–ü–æ—Å–ª–µ RobustScaler - min: {np.min(X_processed):.4f}, max: {np.max(X_processed):.4f}, std: {np.std(X_processed):.4f}")
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(label_encoder.classes_)}")
    print(f"–ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    print("3. –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    model = create_improved_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    model.summary()
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("4. –û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    early_stopping = EarlyStopping(
        monitor='val_accuracy', 
        patience=20,
        restore_best_weights=True,
        verbose=1
    )
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', 
        factor=0.3, 
        patience=10,
        min_lr=1e-7,
        verbose=1
    )
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=200,
        batch_size=16,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π batch size
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ —à—É–º–∞
    print("\n5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ —à—É–º–∞...")
    y_pred_clean = model.predict(X_test_cnn, verbose=0)
    y_pred_classes_clean = np.argmax(y_pred_clean, axis=1)
    accuracy_clean = accuracy_score(y_test, y_pred_classes_clean)
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ —à—É–º–∞: {accuracy_clean*100:.2f}%")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –±–µ–∑ —à—É–º–∞
    max_probs_clean = np.max(y_pred_clean, axis=1)
    unique_probs_clean = len(np.unique(np.round(max_probs_clean, 4)))
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –±–µ–∑ —à—É–º–∞: {unique_probs_clean}/{len(max_probs_clean)} ({unique_probs_clean/len(max_probs_clean)*100:.1f}%)")
    print(f"–ü–µ—Ä–≤—ã–µ 10 –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: {max_probs_clean[:10]}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: min={np.min(max_probs_clean):.4f}, max={np.max(max_probs_clean):.4f}, std={np.std(max_probs_clean):.4f}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º
    print("\n6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º...")
    noise_levels = [0, 0.01, 0.05, 0.1, 0.2, 0.5]
    results = []
    
    for noise_level in noise_levels:
        print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º {noise_level*100}%...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        max_probs = np.max(y_pred_proba, axis=1)
        mean_max_prob = np.mean(max_probs)
        std_max_prob = np.std(max_probs)
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
        unique_probs = len(np.unique(np.round(max_probs, 4)))
        
        results.append({
            'noise_level': noise_level,
            'noise_percent': noise_level * 100,
            'mean_max_probability': mean_max_prob,
            'std_max_probability': std_max_prob,
            'accuracy': accuracy,
            'min_prob': np.min(max_probs),
            'max_prob': np.max(max_probs),
            'unique_probs': unique_probs,
            'total_samples': len(max_probs),
            'uniqueness_ratio': unique_probs / len(max_probs)
        })
        
        print(f"      –°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {mean_max_prob:.4f}")
        print(f"      –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_max_prob:.4f}")
        print(f"      –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.2f}%")
        print(f"      –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π: {unique_probs}/{len(max_probs)} ({unique_probs/len(max_probs)*100:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df_results = pd.DataFrame(results)
    print("\n" + "="*80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò:")
    print("="*80)
    print(df_results.to_string(index=False))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ
    print("\n" + "="*80)
    print("üîç –ü–†–û–í–ï–†–ö–ê –†–ê–ë–û–¢–û–°–ü–û–°–û–ë–ù–û–°–¢–ò:")
    print("="*80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
    if accuracy_clean > 0.5:  # –ë–æ–ª–µ–µ 50% —Ç–æ—á–Ω–æ—Å—Ç–∏
        print("‚úÖ –¢–û–ß–ù–û–°–¢–¨: –û—Ç–ª–∏—á–Ω–∞—è! > 50%")
        accuracy_status = "‚úÖ"
    elif accuracy_clean > 0.3:
        print("‚úÖ –¢–û–ß–ù–û–°–¢–¨: –ü—Ä–∏–µ–º–ª–µ–º–∞—è > 30%")
        accuracy_status = "‚úÖ"
    else:
        print("‚ùå –¢–û–ß–ù–û–°–¢–¨: –°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è < 30%")
        accuracy_status = "‚ùå"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ–Ω–¥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–¥–æ–ª–∂–Ω—ã —Å–Ω–∏–∂–∞—Ç—å—Å—è —Å —à—É–º–æ–º)
    prob_trend = df_results['mean_max_probability'].iloc[-1] - df_results['mean_max_probability'].iloc[0]
    if prob_trend < -0.05:  # –°–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ 5%
        print("‚úÖ –í–ï–†–û–Ø–¢–ù–û–°–¢–ò: –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–Ω–∏–∂–∞—é—Ç—Å—è —Å —à—É–º–æ–º!")
        prob_status = "‚úÖ"
    elif abs(prob_trend) < 0.02:  # –ù–µ–±–æ–ª—å—à–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        print("‚ö†Ô∏è –í–ï–†–û–Ø–¢–ù–û–°–¢–ò: –ú–∞–ª–æ –º–µ–Ω—è—é—Ç—Å—è (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
        prob_status = "‚ö†Ô∏è"
    else:
        print("‚ùå –í–ï–†–û–Ø–¢–ù–û–°–¢–ò: –†–∞—Å—Ç—É—Ç —Å —à—É–º–æ–º (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ)")
        prob_status = "‚ùå"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    min_uniqueness = df_results['uniqueness_ratio'].min()
    if min_uniqueness > 0.5:  # –ë–æ–ª–µ–µ 50% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
        print("‚úÖ –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨: –û—Ç–ª–∏—á–Ω–∞—è > 50%")
        unique_status = "‚úÖ"
    elif min_uniqueness > 0.2:
        print("‚úÖ –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨: –ü—Ä–∏–µ–º–ª–µ–º–∞—è > 20%")
        unique_status = "‚úÖ"
    else:
        print("‚ùå –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨: –°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è < 20%")
        unique_status = "‚ùå"
    
    # –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å
    overall_working = accuracy_status == "‚úÖ" and (prob_status in ["‚úÖ", "‚ö†Ô∏è"]) and unique_status == "‚úÖ"
    
    if overall_working:
        print("\nüéâ –ú–û–î–ï–õ–¨ –ü–û–õ–ù–û–°–¢–¨–Æ –†–ê–ë–û–¢–û–°–ü–û–°–û–ë–ù–ê!")
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_clean*100:.1f}%")
        print(f"‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
        print(f"‚úÖ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['accuracy'])}")
    else:
        print("\n‚ö†Ô∏è –ú–û–î–ï–õ–¨ –¢–†–ï–ë–£–ï–¢ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ô –ù–ê–°–¢–†–û–ô–ö–ò")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_clean*100:.1f}% ({accuracy_status})")
        print(f"   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {prob_status}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {unique_status}")
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ vs —à—É–º
    axes[0,0].plot(df_results['noise_percent'], df_results['mean_max_probability'], 'bo-', linewidth=2, markersize=8)
    axes[0,0].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[0,0].set_ylabel('–°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes[0,0].set_title('–í–ª–∏—è–Ω–∏–µ —à—É–º–∞ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–£–õ–£–ß–®–ï–ù–ù–ê–Ø)')
    axes[0,0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–æ—á–Ω–æ—Å—Ç—å vs —à—É–º
    axes[0,1].plot(df_results['noise_percent'], df_results['accuracy']*100, 'ro-', linewidth=2, markersize=8)
    axes[0,1].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[0,1].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    axes[0,1].set_title('–í–ª–∏—è–Ω–∏–µ —à—É–º–∞ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å (–£–õ–£–ß–®–ï–ù–ù–ê–Ø)')
    axes[0,1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    axes[1,0].plot(df_results['noise_percent'], df_results['uniqueness_ratio']*100, 'go-', linewidth=2, markersize=8)
    axes[1,0].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[1,0].set_ylabel('–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (%)')
    axes[1,0].set_title('–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–£–õ–£–ß–®–ï–ù–ù–ê–Ø)')
    axes[1,0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    axes[1,1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
    axes[1,1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
    axes[1,1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1,1].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    axes[1,1].set_title('–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è (–£–õ–£–ß–®–ï–ù–ù–ê–Ø)')
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/—É–ª—É—á—à–µ–Ω–Ω–∞—è_–º–æ–¥–µ–ª—å_—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    for i, noise_level in enumerate(noise_levels):
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        cm = confusion_matrix(y_test, y_pred_classes)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', 
                   xticklabels=label_encoder.classes_, 
                   yticklabels=label_encoder.classes_, ax=axes[i])
        
        accuracy = accuracy_score(y_test, y_pred_classes)
        axes[i].set_title(f'–®—É–º: {noise_level*100}%\n–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.1f}% (–£–õ–£–ß–®.)')
        axes[i].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        axes[i].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/—É–ª—É—á—à–µ–Ω–Ω—ã–µ_–º–∞—Ç—Ä–∏—Ü—ã_–æ—à–∏–±–æ–∫.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_results.to_csv('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/—É–ª—É—á—à–µ–Ω–Ω–∞—è_–º–æ–¥–µ–ª—å_—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.csv', index=False)
    
    print(f"\n‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {len(history.history['accuracy'])} —ç–ø–æ—Ö")
    print(f"üéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy_clean*100:.2f}%")
    print(f"üî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {model.count_params():,}")
    
    return model, history, df_results, overall_working

if __name__ == "__main__":
    result = test_improved_alexnet()
    
    if result and len(result) == 4:
        model, history, results, is_working = result
        if is_working:
            print("\nüéâ –í–°–ï –†–ê–ë–û–¢–ê–ï–¢! –ì–û–¢–û–í–û –ö –ü–£–®–£ –ù–ê GITHUB!")
        else:
            print("\n‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê")
    else:
        print("‚ùå –û–®–ò–ë–ö–ê –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–•")