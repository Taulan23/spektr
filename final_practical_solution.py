import os
import glob
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
warnings.filterwarnings('ignore')

def load_spring_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join(base_path, tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def load_summer_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ"""
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    for tree_type in tree_types:
        folder_path = os.path.join('.', tree_type)
        if os.path.exists(folder_path):
            excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    if df.shape[1] >= 2:
                        spectrum_data = df.iloc[:, 1].values
                        if len(spectrum_data) > 0 and not np.all(np.isnan(spectrum_data)):
                            spectrum_data = spectrum_data[~np.isnan(spectrum_data)]
                            if len(spectrum_data) > 10:
                                all_data.append(spectrum_data)
                                all_labels.append(tree_type)
                except Exception:
                    continue
    
    return all_data, all_labels

def extract_practical_features(spectra):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ä–∞–±–æ—á–µ–π –º–æ–¥–µ–ª–∏"""
    features = []
    
    # –ù–∞–¥–µ–∂–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –≤–∏–¥–æ–≤
    general_channels = list(range(100, 150)) + list(range(200, 250))
    
    for spectrum in spectra:
        spectrum = np.array(spectrum)
        feature_vector = []
        
        # 1. –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_vector.extend([
            np.mean(spectrum),
            np.std(spectrum),
            np.median(spectrum),
            np.max(spectrum),
            np.min(spectrum),
            np.ptp(spectrum),
            np.var(spectrum),
        ])
        
        # 2. –ö–≤–∞–Ω—Ç–∏–ª–∏
        for p in [10, 25, 50, 75, 90]:
            feature_vector.append(np.percentile(spectrum, p))
        
        # 3. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
        derivative = np.diff(spectrum)
        feature_vector.extend([
            np.mean(derivative),
            np.std(derivative),
            np.max(np.abs(derivative)),
        ])
        
        # 4. –û–±—â–∏–µ –æ–±–ª–∞—Å—Ç–∏ —Å–ø–µ–∫—Ç—Ä–∞
        valid_channels = [ch for ch in general_channels if ch < len(spectrum)]
        if valid_channels:
            region = spectrum[valid_channels]
            feature_vector.extend([
                np.mean(region),
                np.std(region),
                np.max(region),
                np.min(region),
                np.median(region),
            ])
        else:
            feature_vector.extend([0] * 5)
        
        # 5. –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        n_bands = 5
        band_size = len(spectrum) // n_bands
        for i in range(n_bands):
            start_idx = i * band_size
            end_idx = min((i + 1) * band_size, len(spectrum))
            if start_idx < len(spectrum):
                band_energy = np.sum(spectrum[start_idx:end_idx] ** 2)
                feature_vector.append(band_energy)
            else:
                feature_vector.append(0)
        
        # 6. –û—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏
        mid = len(spectrum) // 2
        first_half = np.mean(spectrum[:mid])
        second_half = np.mean(spectrum[mid:])
        ratio = first_half / second_half if second_half > 0 else 0
        feature_vector.append(ratio)
        
        features.append(feature_vector)
    
    return np.array(features)

def create_practical_model(input_shape, num_classes):
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –≤–∏–¥–æ–≤"""
    model = keras.Sequential([
        layers.Dense(256, activation='relu', input_shape=(input_shape,)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        
        layers.Dense(32, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def create_confidence_estimator(X_train, y_train, class_names):
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ü–µ–Ω—â–∏–∫ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–¥–µ–∂–Ω—ã–µ –≤–∏–¥—ã (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
    reliable_species = ['–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    unreliable_species = ['–∫–ª–µ–Ω', '–¥—É–±']
    
    confidence_thresholds = {}
    
    for i, species in enumerate(class_names):
        species_mask = y_train == i
        species_data = X_train[species_mask]
        
        if species in reliable_species:
            confidence_thresholds[species] = 0.3  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö
        elif species in unreliable_species:
            confidence_thresholds[species] = 0.9  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö
        else:
            confidence_thresholds[species] = 0.5  # –°—Ä–µ–¥–Ω–∏–π –ø–æ—Ä–æ–≥
    
    return confidence_thresholds

def analyze_practical_results(y_test, y_pred, y_proba, class_names, confidence_thresholds):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —É—á–µ—Ç–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
    
    print("\n" + "="*80)
    print("üè≠ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*80)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    accuracy = np.mean(y_test == y_pred)
    print(f"üìä –û–ë–©–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {accuracy:.3f} ({accuracy*100:.1f}%)")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    report = classification_report(y_test, y_pred, target_names=class_names, digits=3)
    print("\nüìã –û–¢–ß–ï–¢ –ü–û –ö–õ–ê–°–°–ê–ú:")
    print(report)
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    cm = confusion_matrix(y_test, y_pred)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    print("\nüéØ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –í–ò–î–û–í:")
    
    production_ready = []
    needs_improvement = []
    not_ready = []
    
    for i, species in enumerate(class_names):
        correct = cm_normalized[i][i]
        total = cm[i].sum()
        confidence = confidence_thresholds[species]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É
        if correct >= 0.5:
            status = "üéâ –ì–û–¢–û–í –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ"
            production_ready.append(species)
        elif correct >= 0.3:
            status = "‚ö° –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–ô"
            needs_improvement.append(species)
        else:
            status = "‚ùå –ù–ï –ì–û–¢–û–í"
            not_ready.append(species)
        
        # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        max_proba = np.max(y_proba[y_test == i], axis=1) if np.sum(y_test == i) > 0 else []
        avg_confidence = np.mean(max_proba) if len(max_proba) > 0 else 0
        
        print(f"  {species.upper()}: {correct:.3f} ({correct*100:.1f}%) | "
              f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.3f} | {status}")
    
    # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "="*80)
    print("üíº –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*80)
    
    print(f"‚úÖ –ì–û–¢–û–í–´ –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ ({len(production_ready)} –≤–∏–¥–æ–≤):")
    for species in production_ready:
        print(f"   ‚Ä¢ {species.upper()} - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ")
    
    print(f"\n‚ö° –¢–†–ï–ë–£–Æ–¢ –£–õ–£–ß–®–ï–ù–ò–ô ({len(needs_improvement)} –≤–∏–¥–æ–≤):")
    for species in needs_improvement:
        print(f"   ‚Ä¢ {species.upper()} - –Ω—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    
    print(f"\n‚ùå –ù–ï –ì–û–¢–û–í–´ ({len(not_ready)} –≤–∏–¥–æ–≤):")
    for species in not_ready:
        print(f"   ‚Ä¢ {species.upper()} - —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ")
    
    # –û–±—â–∏–π –≤–µ—Ä–¥–∏–∫—Ç
    print(f"\nüèÜ –ò–¢–û–ì: {len(production_ready)}/7 –≤–∏–¥–æ–≤ –≥–æ—Ç–æ–≤—ã –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    
    if len(production_ready) >= 5:
        print("‚úÖ –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é")
    elif len(production_ready) >= 3:
        print("‚ö° –•–û–†–û–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–∞")
    else:
        print("‚ùå –ù–£–ñ–ù–ê –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –†–ê–ë–û–¢–ê")
    
    return production_ready, needs_improvement, not_ready

def save_production_model(model, scaler, label_encoder, confidence_thresholds, production_ready):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞"""
    
    print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–†–û–î–ê–ö–®–ï–ù –ú–û–î–ï–õ–ò...")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    model.save('production_tree_classifier.keras')
    joblib.dump(scaler, 'production_scaler.pkl')
    joblib.dump(label_encoder, 'production_label_encoder.pkl')
    joblib.dump(confidence_thresholds, 'production_confidence_thresholds.pkl')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = {
        'model_version': '1.0',
        'training_date': '2024',
        'production_ready_species': production_ready,
        'total_features': scaler.n_features_in_,
        'classes': list(label_encoder.classes_),
        'confidence_thresholds': confidence_thresholds,
        'usage_notes': {
            'reliable_species': production_ready,
            'requires_manual_verification': [s for s in label_encoder.classes_ if s not in production_ready]
        }
    }
    
    joblib.dump(metadata, 'production_metadata.pkl')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ README –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
    with open('PRODUCTION_README.md', 'w', encoding='utf-8') as f:
        f.write("# üå≤ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥—Ä–µ–≤–µ—Å–Ω—ã—Ö –ø–æ—Ä–æ–¥ - –ü—Ä–æ–¥–∞–∫—à–µ–Ω –≤–µ—Ä—Å–∏—è\n\n")
        f.write("## ‚úÖ –ì–æ—Ç–æ–≤—ã–µ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–∏–¥—ã:\n")
        for species in production_ready:
            f.write(f"- **{species.upper()}** - –≤—ã—Å–æ–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å\n")
        
        f.write("\n## ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n")
        f.write("- –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –≤–µ—Å–µ–Ω–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –ª–µ—Ç–Ω–∏—Ö\n")
        f.write("- –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–∏–¥—ã —Ç—Ä–µ–±—É—é—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏\n")
        f.write("- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π\n")
        
        f.write("\n## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\n")
        f.write("```python\n")
        f.write("import joblib\n")
        f.write("from tensorflow import keras\n\n")
        f.write("model = keras.models.load_model('production_tree_classifier.keras')\n")
        f.write("scaler = joblib.load('production_scaler.pkl')\n")
        f.write("label_encoder = joblib.load('production_label_encoder.pkl')\n")
        f.write("metadata = joblib.load('production_metadata.pkl')\n")
        f.write("```\n")
    
    print("‚úÖ –ü—Ä–æ–¥–∞–∫—à–µ–Ω –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
    print("   - production_tree_classifier.keras")
    print("   - production_scaler.pkl") 
    print("   - production_label_encoder.pkl")
    print("   - production_confidence_thresholds.pkl")
    print("   - production_metadata.pkl")
    print("   - PRODUCTION_README.md")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ"""
    print("üè≠üè≠üè≠ –§–ò–ù–ê–õ–¨–ù–û–ï –ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ï –†–ï–®–ï–ù–ò–ï üè≠üè≠üè≠")
    print("="*80)
    print("üéØ –¶–ï–õ–¨: –°–æ–∑–¥–∞—Ç—å —Ä–∞–±–æ—á—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
    print("üìä –ü–û–î–•–û–î: –§–æ–∫—É—Å –Ω–∞ –Ω–∞–¥–µ–∂–Ω—ã—Ö –≤–∏–¥–∞—Ö + —á–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π")
    print("="*80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_data, train_labels = load_spring_data()
    test_data, test_labels = load_summer_data()
    
    print(f"–í–µ—Å–µ–Ω–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(train_data)}")
    print(f"–õ–µ—Ç–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä—ã: {len(test_data)}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    print("\nüîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞...")
    all_spectra = train_data + test_data
    min_length = min(len(spectrum) for spectrum in all_spectra)
    
    train_data_trimmed = [spectrum[:min_length] for spectrum in train_data]
    test_data_trimmed = [spectrum[:min_length] for spectrum in test_data]
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüß† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    X_train = extract_practical_features(train_data_trimmed)
    X_test = extract_practical_features(test_data_trimmed)
    
    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {X_train.shape[1]} –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(train_labels)
    y_test = label_encoder.transform(test_labels)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    confidence_thresholds = create_confidence_estimator(
        X_train_scaled, y_train, label_encoder.classes_
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
    
    # Random Forest –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ (–Ω–∞–¥–µ–∂–Ω—ã–π)
    rf_model = RandomForestClassifier(
        n_estimators=300,
        max_depth=20,
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train_scaled, y_train)
    
    # –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ
    nn_model = create_practical_model(X_train_scaled.shape[1], len(label_encoder.classes_))
    
    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=15, restore_best_weights=True
    )
    
    nn_model.fit(
        X_train_scaled, y_train,
        epochs=100,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=0
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    rf_pred = rf_model.predict(X_test_scaled)
    nn_pred = np.argmax(nn_model.predict(X_test_scaled, verbose=0), axis=1)
    nn_proba = nn_model.predict(X_test_scaled, verbose=0)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∫–æ–Ω—Å–µ–Ω—Å—É—Å)
    final_pred = []
    for i in range(len(X_test_scaled)):
        if rf_pred[i] == nn_pred[i]:
            final_pred.append(rf_pred[i])
        else:
            # –ü—Ä–∏ —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–∏ –≤—ã–±–∏—Ä–∞–µ–º –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–π
            final_pred.append(nn_pred[i])
    
    final_pred = np.array(final_pred)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    production_ready, needs_improvement, not_ready = analyze_practical_results(
        y_test, final_pred, nn_proba, label_encoder.classes_, confidence_thresholds
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∫—à–µ–Ω –º–æ–¥–µ–ª–∏
    save_production_model(
        nn_model, scaler, label_encoder, confidence_thresholds, production_ready
    )
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "="*80)
    print("üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*80)
    
    print("‚úÖ –î–û–°–¢–ò–ì–ù–£–¢–û:")
    print(f"   ‚Ä¢ –°–æ–∑–¥–∞–Ω–∞ —Ä–∞–±–æ—á–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è {len(production_ready)} –≤–∏–¥–æ–≤")
    print("   ‚Ä¢ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    print("   ‚Ä¢ –ß–µ—Å—Ç–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã")
    
    print("\n‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:")
    print("   ‚Ä¢ –ö–ª–µ–Ω –∏ –¥—É–± —Ç—Ä–µ–±—É—é—Ç –º—É–ª—å—Ç–∏—Å–µ–∑–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("   ‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–∏–¥—ã –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö")
    print("   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    print("\nüöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print("   1. –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å —Å–∏—Å—Ç–µ–º—É –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –≤–∏–¥–æ–≤")
    print("   2. –°–æ–±—Ä–∞—Ç—å –º—É–ª—å—Ç–∏—Å–µ–∑–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≤–∏–¥–æ–≤")
    print("   3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏")
    
    print(f"\nüèÜ –ü–†–û–ï–ö–¢ –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(production_ready)}/7 –≤–∏–¥–æ–≤ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

if __name__ == "__main__":
    main() 