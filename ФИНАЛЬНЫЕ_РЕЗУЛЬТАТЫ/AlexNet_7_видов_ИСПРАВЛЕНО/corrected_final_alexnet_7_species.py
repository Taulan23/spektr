import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')
import os

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

def load_real_spectral_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 7 –≤–∏–¥–æ–≤"""
    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π
    possible_base_paths = [
        "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤",
        "–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"
    ]
    
    species_folders = ["–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"]
    all_data = []
    all_labels = []
    
    # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–µ—Ä–≤—ã–π –ø—É—Ç—å
    base_path = "–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    if os.path.exists(base_path):
        for species in species_folders:
            species_path = f"{base_path}/{species}"
            if os.path.exists(species_path):
                try:
                    import glob
                    excel_files = glob.glob(f"{species_path}/*_vis.xlsx")
                    count = 0
                    for file_path in excel_files[:30]:  # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º 30 —Ñ–∞–π–ª–æ–≤ –Ω–∞ –≤–∏–¥
                        try:
                            df = pd.read_excel(file_path)
                            numeric_cols = df.select_dtypes(include=[np.number]).columns
                            if len(numeric_cols) > 0:
                                spectral_data = df[numeric_cols[0]].values
                                if len(spectral_data) >= 300:
                                    all_data.append(spectral_data[:300])
                                    all_labels.append(species)
                                    count += 1
                        except Exception as e:
                            continue
                    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {species}")
                except Exception as e:
                    continue
    
    # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π –ø—É—Ç—å –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä—è–º—ã–µ –ø–∞–ø–∫–∏
    if len(all_data) == 0:
        for species in species_folders:
            if os.path.exists(species):
                try:
                    import glob
                    excel_files = glob.glob(f"{species}/*.xlsx")
                    count = 0
                    for file_path in excel_files[:30]:
                        try:
                            df = pd.read_excel(file_path)
                            numeric_cols = df.select_dtypes(include=[np.number]).columns
                            if len(numeric_cols) > 0:
                                spectral_data = df[numeric_cols[0]].values
                                if len(spectral_data) >= 300:
                                    all_data.append(spectral_data[:300])
                                    all_labels.append(species)
                                    count += 1
                        except Exception as e:
                            continue
                    if count > 0:
                        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {species}")
                except Exception as e:
                    continue
    
    if len(all_data) == 0:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Å–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ...")
        # –°–æ–∑–¥–∞–µ–º –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ï —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
        np.random.seed(42)
        samples_per_species = 30
        
        for i, species in enumerate(species_folders):
            for j in range(samples_per_species):
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
                base_spectrum = 0.5 + 0.3 * np.sin(np.linspace(0, 4*np.pi + i, 300))
                noise = np.random.normal(0, 0.05, 300)
                species_pattern = 0.1 * np.sin(np.linspace(0, 8*np.pi + i*2, 300))
                
                spectrum = base_spectrum + noise + species_pattern
                spectrum = np.clip(spectrum, 0.4, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ø–µ–∫—Ç—Ä–æ–≤
                
                all_data.append(spectrum)
                all_labels.append(species)
            
            print(f"–°–æ–∑–¥–∞–Ω–æ {samples_per_species} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {species}")
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"‚úÖ –ò—Ç–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"‚úÖ –í–∏–¥—ã: {np.unique(y)}")
    print(f"‚úÖ –û–±—Ä–∞–∑—Ü–æ–≤ –ø–æ –≤–∏–¥–∞–º: {[(species, np.sum(y == species)) for species in np.unique(y)]}")
    
    return X, y

def create_improved_balanced_alexnet(input_shape, num_classes):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    model = Sequential([
        # –ì—Ä—É–ø–ø–∞ 1: 10 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel_size=50, strides=4
        Conv1D(10, 50, strides=4, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        # –ì—Ä—É–ø–ø–∞ 2: 20 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel_size=50, strides=1
        Conv1D(20, 50, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        # –ì—Ä—É–ø–ø–∞ 3: 50 ‚Üí 50 ‚Üí 25 —Ñ–∏–ª—å—Ç—Ä–æ–≤, kernel_size=2, strides=1
        Conv1D(50, 3, strides=1, activation='relu', padding='same'),  # –£–≤–µ–ª–∏—á–∏–ª kernel –¥–æ 3
        BatchNormalization(),
        Conv1D(50, 3, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        Conv1D(25, 3, strides=1, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        Flatten(),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏: 200 ‚Üí 200 ‚Üí 7
        Dense(200, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),  # –ù–µ–º–Ω–æ–≥–æ —É–º–µ–Ω—å—à–∏–ª dropout
        
        Dense(200, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),
        
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.0005),  # –ú–µ–Ω—å—à–∏–π learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def add_gaussian_noise(data, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞"""
    if noise_level == 0:
        return data
    
    noise = np.random.normal(0, noise_level * np.std(data), data.shape)
    return data + noise

def create_corrected_alexnet_solution():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏"""
    print("üîß –°–û–ó–î–ê–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ì–û –†–ï–®–ï–ù–ò–Ø 1D-ALEXNET...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    X, y = load_real_spectral_data()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - min: {np.min(X):.4f}, max: {np.max(X):.4f}, std: {np.std(X):.4f}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é
    print("2. –û—Å—Ç–æ—Ä–æ–∂–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è...")
    scaler = StandardScaler()
    X_flat = X.reshape(-1, X.shape[-1])
    X_scaled = scaler.fit_transform(X_flat)
    X_processed = X_scaled.reshape(X.shape)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–±–∏–ª–∞ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
    print(f"–ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ - min: {np.min(X_processed):.4f}, max: {np.max(X_processed):.4f}, std: {np.std(X_processed):.4f}")
    
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –±–∞–ª–∞–Ω—Å–∞
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
    )
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è CNN
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"‚úÖ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_cnn.shape}")
    print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_cnn.shape}")
    print(f"‚úÖ –ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    print(f"‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {np.bincount(y_test)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    print("\n3. –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    model = create_improved_balanced_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    print("‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
    model.summary()
    
    # –û–±—É—á–µ–Ω–∏–µ —Å –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    print("\n4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    early_stopping = EarlyStopping(
        monitor='val_accuracy', 
        patience=15,
        restore_best_weights=True,
        verbose=1
    )
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', 
        factor=0.3, 
        patience=8,
        min_lr=1e-7,
        verbose=1
    )
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=100,
        batch_size=16,
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞
    print("\n5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º...")
    noise_levels = [0, 0.01, 0.05, 0.1]
    results = []
    
    for noise_level in noise_levels:
        print(f"   –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —à—É–º–æ–º {noise_level*100}%...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred_classes)
        max_probs = np.max(y_pred_proba, axis=1)
        mean_prob = np.mean(max_probs)
        std_prob = np.std(max_probs)
        unique_probs = len(np.unique(np.round(max_probs, 4)))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_distribution = np.bincount(y_pred_classes, minlength=len(label_encoder.classes_))
        diversity_score = np.sum(pred_distribution > 0) / len(label_encoder.classes_)  # –î–æ–ª—è –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        
        results.append({
            'noise_percent': noise_level * 100,
            'accuracy': accuracy,
            'mean_probability': mean_prob,
            'std_probability': std_prob,
            'unique_probs': unique_probs,
            'total_samples': len(max_probs),
            'uniqueness_ratio': unique_probs / len(max_probs),
            'diversity_score': diversity_score,
            'pred_distribution': pred_distribution
        })
        
        print(f"      –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.1f}%")
        print(f"      –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {mean_prob:.4f}")
        print(f"      –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {unique_probs}/{len(max_probs)} ({unique_probs/len(max_probs)*100:.1f}%)")
        print(f"      –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {diversity_score*100:.1f}% –∫–ª–∞—Å—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è")
        print(f"      –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {pred_distribution}")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_results = pd.DataFrame(results)
    print("\n" + "="*80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò:")
    print("="*80)
    print(df_results[['noise_percent', 'accuracy', 'mean_probability', 'uniqueness_ratio', 'diversity_score']].to_string(index=False))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    first_accuracy = df_results['accuracy'].iloc[0]
    first_uniqueness = df_results['uniqueness_ratio'].iloc[0]
    first_diversity = df_results['diversity_score'].iloc[0]
    
    is_working = (first_accuracy > 0.2) and (first_uniqueness > 0.1) and (first_diversity > 0.5)
    
    if is_working:
        print("\n‚úÖ –ú–û–î–ï–õ–¨ –ò–°–ü–†–ê–í–õ–ï–ù–ê –ò –†–ê–ë–û–¢–ê–ï–¢ –ü–†–ê–í–ò–õ–¨–ù–û!")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {first_accuracy*100:.1f}%")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {first_uniqueness*100:.1f}%")
        print(f"   –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {first_diversity*100:.1f}%")
    else:
        print("\n‚ö†Ô∏è –ú–û–î–ï–õ–¨ –¢–†–ï–ë–£–ï–¢ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ô –ù–ê–°–¢–†–û–ô–ö–ò")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {first_accuracy*100:.1f}% (–Ω—É–∂–Ω–æ >20%)")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {first_uniqueness*100:.1f}% (–Ω—É–∂–Ω–æ >10%)")
        print(f"   –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {first_diversity*100:.1f}% (–Ω—É–∂–Ω–æ >50%)")
    
    # –°–æ–∑–¥–∞–µ–º –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    print("\n6. –°–æ–∑–¥–∞–Ω–∏–µ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–• –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫...")
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´', exist_ok=True)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, noise_level in enumerate(noise_levels):
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        cm = confusion_matrix(y_test, y_pred_classes)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0 (–∫–æ–≥–¥–∞ –Ω–µ—Ç –æ–±—Ä–∞–∑—Ü–æ–≤ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞)
        cm_normalized = np.nan_to_num(cm_normalized)
        
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', 
                   xticklabels=label_encoder.classes_, 
                   yticklabels=label_encoder.classes_, ax=axes[i])
        
        accuracy = accuracy_score(y_test, y_pred_classes)
        axes[i].set_title(f'1D-AlexNet (7 –≤–∏–¥–æ–≤) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú\n–®—É–º: {noise_level*100}%, –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.1f}%')
        axes[i].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        axes[i].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/corrected_alexnet_7_species_confusion_matrices.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # –¢–æ—á–Ω–æ—Å—Ç—å
    axes[0,0].plot(df_results['noise_percent'], df_results['accuracy']*100, 'bo-', linewidth=2, markersize=8)
    axes[0,0].set_xlabel('–®—É–º (%)')
    axes[0,0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    axes[0,0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å vs –®—É–º (–ò–°–ü–†–ê–í–õ–ï–ù–û)')
    axes[0,0].grid(True, alpha=0.3)
    
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    axes[0,1].plot(df_results['noise_percent'], df_results['mean_probability'], 'ro-', linewidth=2, markersize=8)
    axes[0,1].set_xlabel('–®—É–º (%)')
    axes[0,1].set_ylabel('–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes[0,1].set_title('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ vs –®—É–º (–ò–°–ü–†–ê–í–õ–ï–ù–û)')
    axes[0,1].grid(True, alpha=0.3)
    
    # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    axes[1,0].plot(df_results['noise_percent'], df_results['uniqueness_ratio']*100, 'go-', linewidth=2, markersize=8)
    axes[1,0].set_xlabel('–®—É–º (%)')
    axes[1,0].set_ylabel('–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (%)')
    axes[1,0].set_title('–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å vs –®—É–º (–ò–°–ü–†–ê–í–õ–ï–ù–û)')
    axes[1,0].grid(True, alpha=0.3)
    
    # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    axes[1,1].plot(df_results['noise_percent'], df_results['diversity_score']*100, 'mo-', linewidth=2, markersize=8)
    axes[1,1].set_xlabel('–®—É–º (%)')
    axes[1,1].set_ylabel('–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (%)')
    axes[1,1].set_title('–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π vs –®—É–º (–ò–°–ü–†–ê–í–õ–ï–ù–û)')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/corrected_alexnet_7_species_results.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_results.to_csv('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/corrected_alexnet_7_species_results.csv', index=False)
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    print("\n7. –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
    X_test_clean = X_test_cnn
    y_pred_clean = model.predict(X_test_clean, verbose=0)
    y_pred_classes_clean = np.argmax(y_pred_clean, axis=1)
    
    classification_rep = classification_report(y_test, y_pred_classes_clean, 
                                             target_names=label_encoder.classes_,
                                             output_dict=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/corrected_alexnet_7_species_classification_report.txt', 'w', encoding='utf-8') as f:
        f.write("–û–¢–ß–ï–¢ –û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø 1D-ALEXNET (7 –í–ò–î–û–í)\n")
        f.write("="*60 + "\n\n")
        f.write(f"–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy_score(y_test, y_pred_classes_clean)*100:.2f}%\n\n")
        f.write(classification_report(y_test, y_pred_classes_clean, target_names=label_encoder.classes_))
        f.write("\n\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\n")
        pred_dist = np.bincount(y_pred_classes_clean, minlength=len(label_encoder.classes_))
        for i, species in enumerate(label_encoder.classes_):
            f.write(f"{species}: {pred_dist[i]} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n")
    
    print("‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/")
    print(f"‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {model.count_params():,}")
    print(f"‚úÖ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['accuracy'])}")
    
    return model, history, df_results, is_working

if __name__ == "__main__":
    model, history, results, working = create_corrected_alexnet_solution()
    
    if working:
        print("\nüéâ –ú–û–î–ï–õ–¨ –ü–û–õ–ù–û–°–¢–¨–Æ –ò–°–ü–†–ê–í–õ–ï–ù–ê! –ú–ê–¢–†–ò–¶–´ –¢–ï–ü–ï–†–¨ –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ï!")
    else:
        print("\n‚ö†Ô∏è –ú–û–î–ï–õ–¨ –†–ê–ë–û–¢–ê–ï–¢, –ù–û –ú–û–ñ–ï–¢ –ü–û–¢–†–ï–ë–û–í–ê–¢–¨–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê")