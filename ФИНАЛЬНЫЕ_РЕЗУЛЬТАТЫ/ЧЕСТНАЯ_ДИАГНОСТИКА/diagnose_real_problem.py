import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')
import os

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seeds
np.random.seed(42)
tf.random.set_seed(42)

def create_diagnostic_alexnet(input_shape, num_classes):
    """–ü—Ä–æ—Å—Ç–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å"""
    model = Sequential([
        Conv1D(32, 10, activation='relu', input_shape=input_shape),
        MaxPooling1D(2),
        Conv1D(64, 5, activation='relu'),
        MaxPooling1D(2),
        Flatten(),
        Dense(100, activation='relu'),
        Dropout(0.3),
        Dense(50, activation='relu'),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

def add_gaussian_noise(data, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞"""
    if noise_level == 0:
        return data
    noise = np.random.normal(0, noise_level * np.std(data), data.shape)
    return data + noise

def create_realistic_synthetic_data():
    """–°–æ–∑–¥–∞–Ω–∏–µ –î–ï–ô–°–¢–í–ò–¢–ï–õ–¨–ù–û —Ä–∞–∑–ª–∏—á–∞—é—â–∏—Ö—Å—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –Ø–í–ù–´–ú–ò —Ä–∞–∑–ª–∏—á–∏—è–º–∏...")
    
    species_names = ["–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"]
    all_data = []
    all_labels = []
    
    samples_per_species = 30
    spectrum_length = 300
    
    for i, species in enumerate(species_names):
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ {samples_per_species} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {species}...")
        
        for j in range(samples_per_species):
            # –°–æ–∑–¥–∞—ë–º –£–ù–ò–ö–ê–õ–¨–ù–´–ï —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
            base_freq = 2 + i * 0.5  # –†–∞–∑–Ω–∞—è –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
            amplitude = 0.3 + i * 0.1  # –†–∞–∑–Ω–∞—è –∞–º–ø–ª–∏—Ç—É–¥–∞
            
            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∏–≥–Ω–∞–ª
            x = np.linspace(0, 10, spectrum_length)
            spectrum = 0.5 + amplitude * np.sin(base_freq * x)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
            if species == "–±–µ—Ä–µ–∑–∞":
                spectrum += 0.2 * np.sin(8 * x) * np.exp(-((x-3)**2)/2)
            elif species == "–¥—É–±":
                spectrum += 0.3 * np.sin(12 * x) * np.exp(-((x-5)**2)/3)
            elif species == "–µ–ª—å":
                spectrum += 0.25 * np.sin(6 * x) * np.exp(-((x-7)**2)/2.5)
            elif species == "–∫–ª–µ–Ω":
                spectrum += 0.2 * np.sin(15 * x) * np.exp(-((x-2)**2)/1.5)
            elif species == "–ª–∏–ø–∞":
                spectrum += 0.35 * np.sin(10 * x) * np.exp(-((x-8)**2)/3)
            elif species == "–æ—Å–∏–Ω–∞":
                spectrum += 0.3 * np.sin(4 * x) * np.exp(-((x-4)**2)/2)
            elif species == "—Å–æ—Å–Ω–∞":
                spectrum += 0.25 * np.sin(20 * x) * np.exp(-((x-6)**2)/2)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º
            spectrum += np.random.normal(0, 0.02, spectrum_length)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            spectrum = np.clip(spectrum, 0.1, 1.0)
            
            all_data.append(spectrum)
            all_labels.append(species)
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"‚úÖ –†–∞–∑–º–µ—Ä —Å–ø–µ–∫—Ç—Ä–∞: {X.shape[1]}")
    print(f"‚úÖ –í–∏–¥—ã: {np.unique(y)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≤–∏–¥–∞–º–∏
    print("\nüîç –ü–†–û–í–ï–†–ö–ê –†–ê–ó–õ–ò–ß–ò–ô –ú–ï–ñ–î–£ –í–ò–î–ê–ú–ò:")
    for i, species in enumerate(species_names):
        species_data = X[y == species]
        mean_spectrum = np.mean(species_data, axis=0)
        print(f"{species}: —Å—Ä–µ–¥–Ω–µ–µ={np.mean(mean_spectrum):.3f}, std={np.std(mean_spectrum):.3f}")
    
    return X, y

def honest_diagnosis():
    """–ß–ï–°–¢–ù–ê–Ø –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã"""
    print("üîç –ß–ï–°–¢–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú–´...")
    print("="*60)
    
    # –°–æ–∑–¥–∞—ë–º –¥–∞–Ω–Ω—ã–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏
    X, y = create_realistic_synthetic_data()
    
    # –ë–ï–ó –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    print("\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –ë–ï–ó –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
    X_processed = X  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
    
    print(f"–î–∞–Ω–Ω—ã–µ: min={np.min(X_processed):.3f}, max={np.max(X_processed):.3f}, std={np.std(X_processed):.3f}")
    
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
    )
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è CNN
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"\n–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_cnn.shape}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_cnn.shape}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ —Ç–µ—Å—Ç–µ: {np.bincount(y_test)}")
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
    print("\n–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
    model = create_diagnostic_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
    model.summary()
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=50,
        batch_size=16,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–ï–ó —à—É–º–∞
    print("\n" + "="*60)
    print("üìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ë–ï–ó –®–£–ú–ê:")
    print("="*60)
    
    y_pred_proba = model.predict(X_test_cnn, verbose=0)
    y_pred_classes = np.argmax(y_pred_proba, axis=1)
    accuracy = accuracy_score(y_test, y_pred_classes)
    
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.2f}%")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    pred_distribution = np.bincount(y_pred_classes, minlength=len(label_encoder.classes_))
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_distribution}")
    
    for i, species in enumerate(label_encoder.classes_):
        print(f"{species}: {pred_distribution[i]} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ë–ï–ó —à—É–º–∞
    cm = confusion_matrix(y_test, y_pred_classes)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    cm_normalized = np.nan_to_num(cm_normalized)
    
    print(f"\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ë–ï–ó —à—É–º–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è):")
    cm_df = pd.DataFrame(cm_normalized, 
                         index=label_encoder.classes_, 
                         columns=label_encoder.classes_)
    print(cm_df.round(3))
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –° –®–£–ú–û–ú
    print("\n" + "="*60)
    print("üîä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –†–ê–ó–ù–´–ú–ò –£–†–û–í–ù–Ø–ú–ò –®–£–ú–ê:")
    print("="*60)
    
    noise_levels = [0, 0.05, 0.1, 0.2]
    noise_results = []
    
    for noise_level in noise_levels:
        print(f"\n--- –®–£–ú {noise_level*100}% ---")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba_noisy = model.predict(X_test_noisy, verbose=0)
        y_pred_classes_noisy = np.argmax(y_pred_proba_noisy, axis=1)
        accuracy_noisy = accuracy_score(y_test, y_pred_classes_noisy)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        pred_dist_noisy = np.bincount(y_pred_classes_noisy, minlength=len(label_encoder.classes_))
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_noisy*100:.2f}%")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {pred_dist_noisy}")
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm_noisy = confusion_matrix(y_test, y_pred_classes_noisy)
        cm_noisy_norm = cm_noisy.astype('float') / cm_noisy.sum(axis=1)[:, np.newaxis]
        cm_noisy_norm = np.nan_to_num(cm_noisy_norm)
        
        noise_results.append({
            'noise_level': noise_level,
            'accuracy': accuracy_noisy,
            'matrix': cm_noisy_norm,
            'distribution': pred_dist_noisy
        })
    
    # –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —à—É–º–∞
    print("\n" + "="*60)
    print("üìà –ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –®–£–ú–ê:")
    print("="*60)
    
    base_accuracy = noise_results[0]['accuracy']
    print(f"–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (0% —à—É–º–∞): {base_accuracy*100:.2f}%")
    
    for result in noise_results[1:]:
        change = result['accuracy'] - base_accuracy
        print(f"–®—É–º {result['noise_level']*100}%: {result['accuracy']*100:.2f}% (–∏–∑–º–µ–Ω–µ–Ω–∏–µ: {change*100:+.2f}%)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ß–ï–°–¢–ù–´–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    os.makedirs('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/–ß–ï–°–¢–ù–ê–Ø_–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê', exist_ok=True)
    
    # –°–æ–∑–¥–∞—ë–º —á–µ—Å—Ç–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, result in enumerate(noise_results):
        sns.heatmap(result['matrix'], 
                   annot=True, 
                   fmt='.3f',
                   cmap='Blues',
                   xticklabels=label_encoder.classes_,
                   yticklabels=label_encoder.classes_,
                   ax=axes[i])
        
        axes[i].set_title(f'–®–£–ú: {result["noise_level"]*100}%\n–¢–æ—á–Ω–æ—Å—Ç—å: {result["accuracy"]*100:.1f}%')
        axes[i].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        axes[i].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/–ß–ï–°–¢–ù–ê–Ø_–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê/honest_confusion_matrices.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Å—Ç–Ω—ã–π –æ—Ç—á—ë—Ç
    with open('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/–ß–ï–°–¢–ù–ê–Ø_–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê/honest_report.txt', 'w', encoding='utf-8') as f:
        f.write("–ß–ï–°–¢–ù–´–ô –û–¢–ß–Å–¢ –û –î–ò–ê–ì–ù–û–°–¢–ò–ö–ï\n")
        f.write("="*50 + "\n\n")
        f.write(f"–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {base_accuracy*100:.2f}%\n")
        f.write(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {model.count_params():,}\n")
        f.write(f"–≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['accuracy'])}\n\n")
        
        f.write("–í–õ–ò–Ø–ù–ò–ï –®–£–ú–ê –ù–ê –¢–û–ß–ù–û–°–¢–¨:\n")
        f.write("-"*30 + "\n")
        for result in noise_results:
            change = result['accuracy'] - base_accuracy
            f.write(f"–®—É–º {result['noise_level']*100:4.1f}%: —Ç–æ—á–Ω–æ—Å—Ç—å {result['accuracy']*100:5.1f}% (–∏–∑–º–µ–Ω–µ–Ω–∏–µ: {change*100:+5.1f}%)\n")
        
        f.write(f"\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:\n")
        f.write("-"*30 + "\n")
        for result in noise_results:
            f.write(f"–®—É–º {result['noise_level']*100:4.1f}%: {result['distribution']}\n")
    
    print(f"\n‚úÖ –ß–ï–°–¢–ù–ê–Ø –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/–ß–ï–°–¢–ù–ê–Ø_–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê/")
    print(f"üéØ –ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {base_accuracy*100:.2f}%")
    print(f"üî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {model.count_params():,}")
    
    return noise_results

if __name__ == "__main__":
    results = honest_diagnosis()
    print("\nüéØ –ß–ï–°–¢–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")