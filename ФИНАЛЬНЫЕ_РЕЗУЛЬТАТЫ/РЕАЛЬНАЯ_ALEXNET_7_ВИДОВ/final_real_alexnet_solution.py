import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, RobustScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import os
import glob
import warnings
warnings.filterwarnings('ignore')

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

def load_real_spectral_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Ñ–∞–π–ª–æ–≤"""
    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ú–∞–ø–ø–∏–Ω–≥ –ø–∞–ø–æ–∫ –∫ –Ω–∞–∑–≤–∞–Ω–∏—è–º –≤–∏–¥–æ–≤
    species_mapping = {
        '–±–µ—Ä–µ–∑–∞': '–±–µ—Ä–µ–∑–∞',
        '–¥—É–±': '–¥—É–±', 
        '–µ–ª—å': '–µ–ª—å',
        '–∫–ª–µ–Ω': '–∫–ª–µ–Ω',
        '–ª–∏–ø–∞': '–ª–∏–ø–∞',
        '–æ—Å–∏–Ω–∞': '–æ—Å–∏–Ω–∞',
        '—Å–æ—Å–Ω–∞': '—Å–æ—Å–Ω–∞'
    }
    
    all_data = []
    all_labels = []
    successful_loads = 0
    failed_loads = 0
    
    for folder_name, species_name in species_mapping.items():
        folder_path = os.path.join('–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ', folder_name)
        if not os.path.exists(folder_path):
            print(f"‚ùå –ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            continue
            
        print(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–ø–∫–∏: {folder_name} -> {species_name}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ Excel —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
        excel_files = glob.glob(os.path.join(folder_path, "*.xlsx"))
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(excel_files)} Excel —Ñ–∞–π–ª–æ–≤")
        
        species_count = 0
        for file_path in excel_files[:30]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 30 —Ñ–∞–π–ª–æ–≤ –Ω–∞ –≤–∏–¥
            try:
                # –ß–∏—Ç–∞–µ–º Excel —Ñ–∞–π–ª
                df = pd.read_excel(file_path)
                
                # –ë–µ—Ä—ë–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –≤—Ç–æ—Ä–æ–π –∫–æ–ª–æ–Ω–∫–∏ (–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏)
                if df.shape[1] >= 2 and df.shape[0] >= 50:  # –ú–∏–Ω–∏–º—É–º 50 —Ç–æ—á–µ–∫
                    # –í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
                    spectrum = df.iloc[:, 1].values
                    
                    # –£–¥–∞–ª—è–µ–º NaN –∏ inf –∑–Ω–∞—á–µ–Ω–∏—è
                    spectrum = spectrum[~np.isnan(spectrum)]
                    spectrum = spectrum[~np.isinf(spectrum)]
                    
                    if len(spectrum) > 50:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ø–µ–∫—Ç—Ä
                        if np.std(spectrum) > 0:
                            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –¥–ª–∏–Ω–µ 300 —Ç–æ—á–µ–∫
                            if len(spectrum) > 300:
                                spectrum = spectrum[:300]
                            elif len(spectrum) < 300:
                                # –î–æ–ø–æ–ª–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
                                spectrum = np.pad(spectrum, (0, 300 - len(spectrum)), 'mean')
                            
                            all_data.append(spectrum)
                            all_labels.append(species_name)
                            species_count += 1
                            successful_loads += 1
                            
            except Exception as e:
                failed_loads += 1
                continue
        
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {species_count} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {species_name}")
    
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò:")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {successful_loads} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"‚ùå –û—à–∏–±–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏: {failed_loads}")
    
    if len(all_data) == 0:
        print("‚ùå –ù–ï –£–î–ê–õ–û–°–¨ –ó–ê–ì–†–£–ó–ò–¢–¨ –î–ê–ù–ù–´–ï!")
        return None, None
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"üìà –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"üè∑Ô∏è –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∏–¥—ã: {np.unique(y)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∏–¥–∞–º
    unique, counts = np.unique(y, return_counts=True)
    print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –í–ò–î–ê–ú:")
    for species, count in zip(unique, counts):
        print(f"   {species}: {count} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    return X, y

def create_real_alexnet(input_shape, num_classes):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π AlexNet –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö"""
    model = Sequential([
        # –ì—Ä—É–ø–ø–∞ 1
        Conv1D(32, 10, strides=2, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(2, strides=2),
        
        # –ì—Ä—É–ø–ø–∞ 2
        Conv1D(64, 5, strides=1, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(2, strides=2),
        
        # –ì—Ä—É–ø–ø–∞ 3
        Conv1D(128, 3, strides=1, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(2, strides=2),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        Flatten(),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def add_gaussian_noise(data, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞ –∫ –¥–∞–Ω–Ω—ã–º"""
    if noise_level == 0:
        return data
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
    noise = np.zeros_like(data)
    for i in range(data.shape[0]):
        std_dev = np.std(data[i])
        noise[i] = np.random.normal(0, noise_level * std_dev, data[i].shape)
    
    return data + noise

def plot_confusion_matrices_with_precision(matrices_data, class_names, save_path, precision=7):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    axes = axes.ravel()
    
    for i, data in enumerate(matrices_data):
        noise_level = data['noise_level']
        cm_normalized = data['matrix']
        accuracy = data['accuracy']
        
        # –°–æ–∑–¥–∞—ë–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
        annotations = []
        for row in range(cm_normalized.shape[0]):
            ann_row = []
            for col in range(cm_normalized.shape[1]):
                value = cm_normalized[row, col]
                ann_row.append(f"{value:.{precision}f}")
            annotations.append(ann_row)
        
        # –°–æ–∑–¥–∞—ë–º heatmap
        sns.heatmap(cm_normalized, 
                   annot=annotations,
                   fmt='',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                   cmap='Blues',
                   cbar=True,
                   xticklabels=class_names,
                   yticklabels=class_names,
                   ax=axes[i],
                   square=True)
        
        axes[i].set_title(f'–®–£–ú: {noise_level:.1f}% | –¢–û–ß–ù–û–°–¢–¨: {accuracy*100:.4f}%', 
                         fontsize=14, fontweight='bold')
        axes[i].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
        axes[i].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
        
        # –ü–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –º–µ—Ç–∫–∏
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].tick_params(axis='y', rotation=0)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def save_detailed_matrices(matrices_data, class_names, save_path, precision=7):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("–î–ï–¢–ê–õ–¨–ù–´–ï –ú–ê–¢–†–ò–¶–´ –û–®–ò–ë–û–ö –° –í–´–°–û–ö–û–ô –¢–û–ß–ù–û–°–¢–¨–Æ\n")
        f.write("=" * 80 + "\n\n")
        
        for data in matrices_data:
            noise_level = data['noise_level']
            cm_normalized = data['matrix']
            accuracy = data['accuracy']
            
            f.write(f"–®–£–ú: {noise_level:.1f}% | –¢–û–ß–ù–û–°–¢–¨: {accuracy*100:.7f}%\n")
            f.write("-" * 70 + "\n")
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
            header = "        "
            for class_name in class_names:
                header += f"{class_name:>12s} "
            f.write(header + "\n")
            
            # –°—Ç—Ä–æ–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã
            for i, true_class in enumerate(class_names):
                row = f"{true_class:>8s}"
                for j in range(len(class_names)):
                    value = cm_normalized[i, j]
                    row += f" {value:.{precision}f}"
                f.write(row + "\n")
            
            f.write("\n" + "=" * 80 + "\n\n")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    print("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    X, y = load_real_spectral_data()
    
    if X is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –ó–ê–ì–†–£–ñ–ï–ù–ù–´–• –î–ê–ù–ù–´–•:")
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π: [{np.min(X):.3f}, {np.max(X):.3f}]")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {np.mean(X):.3f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(X):.3f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –≤–∏–¥–∞–º–∏
    unique_species = np.unique(y)
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –í–ò–î–ê–ú:")
    for species in unique_species:
        species_data = X[y == species]
        print(f"{species:>8s}: {len(species_data):>3d} –æ–±—Ä–∞–∑—Ü–æ–≤, "
              f"—Å—Ä–µ–¥–Ω–µ–µ={np.mean(species_data):.3f}, std={np.std(species_data):.3f}")
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å
    unique, counts = np.unique(y, return_counts=True)
    min_samples = np.min(counts)
    
    if min_samples < 10:
        print(f"‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö! –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å: {min_samples}")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X)
    
    print(f"–ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: [{np.min(X_scaled):.3f}, {np.max(X_scaled):.3f}], std={np.std(X_scaled):.3f}")
    
    # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    test_size = 0.3 if len(X) > 50 else 0.2  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, 
        test_size=test_size, 
        random_state=42, 
        stratify=y_encoded
    )
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è CNN
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"\nüìã –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_cnn.shape}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_cnn.shape}")
    print(f"–ö–ª–∞—Å—Å—ã: {len(label_encoder.classes_)}")
    
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    print(f"\nüß† –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò...")
    model = create_real_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {model.count_params():,}")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º callbacks
    early_stopping = EarlyStopping(
        monitor='val_accuracy', 
        patience=15, 
        restore_best_weights=True,
        verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüéØ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø...")
    batch_size = min(16, len(X_train) // 4)  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=100,
        batch_size=batch_size,
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {len(history.history['accuracy'])} —ç–ø–æ—Ö")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞
    print(f"\nüîä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –†–ê–ó–õ–ò–ß–ù–´–ú–ò –£–†–û–í–ù–Ø–ú–ò –®–£–ú–ê:")
    print("=" * 60)
    
    noise_levels = [0.0, 0.01, 0.05, 0.1]
    results = []
    
    for noise_level in noise_levels:
        print(f"\n--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º {noise_level*100:.1f}% ---")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred_classes)
        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)
        cm_normalized = np.nan_to_num(cm_normalized)
        
        results.append({
            'noise_level': noise_level * 100,
            'accuracy': accuracy,
            'matrix': cm_normalized,
            'raw_matrix': cm
        })
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.4f}%")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_counts = np.bincount(y_pred_classes, minlength=len(label_encoder.classes_))
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        for i, (species, count) in enumerate(zip(label_encoder.classes_, pred_counts)):
            print(f"  {species}: {count} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir = "–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/–†–ï–ê–õ–¨–ù–ê–Ø_ALEXNET_7_–í–ò–î–û–í"
    os.makedirs(output_dir, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—ã —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
    matrices_path = os.path.join(output_dir, "real_alexnet_confusion_matrices_7_digits.png")
    plot_confusion_matrices_with_precision(results, label_encoder.classes_, matrices_path, precision=7)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    detailed_path = os.path.join(output_dir, "detailed_matrices_7_digits.txt")
    save_detailed_matrices(results, label_encoder.classes_, detailed_path, precision=7)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    report_path = os.path.join(output_dir, "real_alexnet_classification_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("–û–¢–ß–Å–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"–î–∞—Ç–∞: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X)}\n")
        f.write(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}\n")
        f.write(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)}\n")
        f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(label_encoder.classes_)}\n")
        f.write(f"–ö–ª–∞—Å—Å—ã: {', '.join(label_encoder.classes_)}\n")
        f.write(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {model.count_params():,}\n")
        f.write(f"–≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['accuracy'])}\n\n")
        
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –®–£–ú–£:\n")
        f.write("-" * 30 + "\n")
        for result in results:
            f.write(f"–®—É–º {result['noise_level']:5.1f}%: —Ç–æ—á–Ω–æ—Å—Ç—å {result['accuracy']*100:7.4f}%\n")
    
    print(f"\n‚úÖ –í–°–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´ –í: {output_dir}")
    print(f"üìä –ú–∞—Ç—Ä–∏—Ü—ã: {matrices_path}")
    print(f"üìÑ –î–µ—Ç–∞–ª–∏: {detailed_path}")
    print(f"üìã –û—Ç—á—ë—Ç: {report_path}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    base_accuracy = results[0]['accuracy']
    final_accuracy = results[-1]['accuracy']
    accuracy_drop = (base_accuracy - final_accuracy) * 100
    
    print(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"–ë–∞–∑–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (0% —à—É–º–∞): {base_accuracy*100:.4f}%")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å —Å —à—É–º–æ–º 10%: {final_accuracy*100:.4f}%")
    print(f"–°–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: {accuracy_drop:.4f} –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤")
    
    return results

if __name__ == "__main__":
    results = main()
    print("\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")