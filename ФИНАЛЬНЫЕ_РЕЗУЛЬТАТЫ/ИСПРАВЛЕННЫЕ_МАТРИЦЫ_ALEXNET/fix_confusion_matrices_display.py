import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, confusion_matrix
import os
import glob
import warnings
warnings.filterwarnings('ignore')

# Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ seeds Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
np.random.seed(42)
tf.random.set_seed(42)

def load_real_spectral_data():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ Ğ•ĞĞ›Ğ¬ĞĞ«Ğ¥ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Excel Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²"""
    print("ğŸ” Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ Ğ•ĞĞ›Ğ¬ĞĞ«Ğ¥ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
    
    # ĞœĞ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ğ¿Ğ°Ğ¿Ğ¾Ğº Ğº Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ²Ğ¸Ğ´Ğ¾Ğ²
    species_mapping = {
        'Ğ±ĞµÑ€ĞµĞ·Ğ°': 'Ğ±ĞµÑ€ĞµĞ·Ğ°',
        'Ğ´ÑƒĞ±': 'Ğ´ÑƒĞ±', 
        'ĞµĞ»ÑŒ': 'ĞµĞ»ÑŒ',
        'ĞºĞ»ĞµĞ½': 'ĞºĞ»ĞµĞ½',
        'Ğ»Ğ¸Ğ¿Ğ°': 'Ğ»Ğ¸Ğ¿Ğ°',
        'Ğ¾ÑĞ¸Ğ½Ğ°': 'Ğ¾ÑĞ¸Ğ½Ğ°',
        'ÑĞ¾ÑĞ½Ğ°': 'ÑĞ¾ÑĞ½Ğ°'
    }
    
    all_data = []
    all_labels = []
    successful_loads = 0
    
    for folder_name, species_name in species_mapping.items():
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ²ĞµÑĞµĞ½Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°
        folder_path = os.path.join('Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ_Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ', 'Ğ¡Ğ¿ĞµĞºÑ‚Ñ€Ñ‹, Ğ²ĞµÑĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´, 7 Ğ²Ğ¸Ğ´Ğ¾Ğ²', folder_name)
        if not os.path.exists(folder_path):
            print(f"âŒ ĞŸĞ°Ğ¿ĞºĞ° {folder_path} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°")
            continue
            
        print(f"ğŸ“ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ğ°Ğ¿ĞºĞ¸: {folder_name} -> {species_name}")
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Excel Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ² Ğ¿Ğ°Ğ¿ĞºĞµ
        excel_files = glob.glob(os.path.join(folder_path, "*.xlsx"))
        
        species_count = 0
        for file_path in excel_files[:50]:  # 50 Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¸Ğ´
            try:
                # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Excel Ñ„Ğ°Ğ¹Ğ»
                df = pd.read_excel(file_path)
                
                # Ğ‘ĞµÑ€Ñ‘Ğ¼ Ğ²ÑĞµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ (Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸)
                if df.shape[1] >= 2 and df.shape[0] >= 50:
                    spectrum = df.iloc[:, 1].values
                    
                    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ NaN Ğ¸ inf Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
                    spectrum = spectrum[~np.isnan(spectrum)]
                    spectrum = spectrum[~np.isinf(spectrum)]
                    
                    if len(spectrum) > 50:
                        if np.std(spectrum) > 0:
                            # ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº Ğ´Ğ»Ğ¸Ğ½Ğµ 300 Ñ‚Ğ¾Ñ‡ĞµĞº
                            if len(spectrum) > 300:
                                spectrum = spectrum[:300]
                            elif len(spectrum) < 300:
                                spectrum = np.pad(spectrum, (0, 300 - len(spectrum)), 'mean')
                            
                            all_data.append(spectrum)
                            all_labels.append(species_name)
                            species_count += 1
                            successful_loads += 1
                            
            except Exception as e:
                continue
        
        print(f"   âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {species_count} Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ {species_name}")
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    return X, y

def create_original_alexnet(input_shape, num_classes):
    """ĞĞ Ğ˜Ğ“Ğ˜ĞĞĞ›Ğ¬ĞĞĞ¯ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ AlexNet Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸"""
    model = Sequential([
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ° 1 - ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ stride Ğ´Ğ»Ñ 300 Ñ‚Ğ¾Ñ‡ĞµĞº
        Conv1D(10, 50, strides=2, activation='relu', input_shape=input_shape),
        MaxPooling1D(3, strides=2),
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ° 2 - ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ´Ñ€Ğ°
        Conv1D(20, 25, strides=1, activation='relu'),
        MaxPooling1D(3, strides=2),
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ° 3
        Conv1D(50, 2, strides=1, activation='relu'),
        Conv1D(50, 2, strides=1, activation='relu'),
        Conv1D(25, 2, strides=1, activation='relu'),
        MaxPooling1D(3, strides=2),
        
        # ĞŸĞ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸
        Flatten(),
        Dense(200, activation='relu'),
        Dropout(0.5),
        Dense(200, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def add_gaussian_noise(data, noise_level):
    """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼"""
    if noise_level == 0:
        return data
    
    noise = np.zeros_like(data)
    for i in range(data.shape[0]):
        std_dev = np.std(data[i])
        noise[i] = np.random.normal(0, noise_level * std_dev, data[i].shape)
    
    return data + noise

def plot_fixed_confusion_matrices(matrices_data, class_names, save_path):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ¥ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… 4 ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑˆÑƒĞ¼Ğ°"""
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ñ„Ğ¸Ğ³ÑƒÑ€Ñƒ Ñ 4 Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ğ¼Ğ¸
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    axes = axes.ravel()
    
    # Ğ£Ñ€Ğ¾Ğ²Ğ½Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
    noise_levels_display = [0, 1, 5, 10]
    
    for i, noise_level in enumerate(noise_levels_display):
        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        if noise_level == 0:
            # Ğ”Ğ»Ñ 0% Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ 1% (Ğ¾Ğ½Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹)
            data = matrices_data[0]
        else:
            # Ğ˜Ñ‰ĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑˆÑƒĞ¼Ğ°
            data = None
            for result in matrices_data:
                if abs(result['noise_level'] - noise_level) < 0.1:
                    data = result
                    break
            
            # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹
            if data is None:
                data = matrices_data[0]
        
        cm_normalized = data['matrix']
        accuracy = data['accuracy']
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ 3 Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸
        annotations = []
        for row in range(cm_normalized.shape[0]):
            ann_row = []
            for col in range(cm_normalized.shape[1]):
                value = cm_normalized[row, col]
                ann_row.append(f"{value:.3f}")
            annotations.append(ann_row)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ heatmap
        sns.heatmap(cm_normalized, 
                   annot=annotations,
                   fmt='',
                   cmap='Blues',
                   cbar=True,
                   xticklabels=class_names,
                   yticklabels=class_names,
                   ax=axes[i],
                   square=True,
                   vmin=0.0,
                   vmax=1.0)
        
        axes[i].set_title(f'Î´ = {noise_level}% | Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {accuracy:.3f}', 
                         fontsize=16, fontweight='bold', pad=20)
        axes[i].set_xlabel('ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ', fontsize=12)
        axes[i].set_ylabel('Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ', fontsize=12)
        
        # ĞŸĞ¾Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¼ĞµÑ‚ĞºĞ¸
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].tick_params(axis='y', rotation=0)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def run_quick_test():
    """Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†"""
    print("ğŸš€ Ğ‘Ğ«Ğ¡Ğ¢Ğ ĞĞ• Ğ¡ĞĞ—Ğ”ĞĞĞ˜Ğ• Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ¥ ĞœĞĞ¢Ğ Ğ˜Ğ¦")
    print("=" * 50)
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    X, y = load_real_spectral_data()
    
    if X is None:
        print("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ")
        return
    
    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded
    )
    
    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ñ CNN
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹: {X_train_cnn.shape} Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, {X_test_cnn.shape} Ñ‚ĞµÑÑ‚")
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    print("ğŸ§  Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
    model = create_original_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, verbose=0)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=100,  # ĞœĞµĞ½ÑŒÑˆĞµ ÑĞ¿Ğ¾Ñ… Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ñ‚Ñ‹
        batch_size=32,
        validation_split=0.15,
        callbacks=[early_stopping],
        verbose=0  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´
    )
    
    print(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ·Ğ° {len(history.history['accuracy'])} ÑĞ¿Ğ¾Ñ…")
    
    # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑˆÑƒĞ¼Ğ°
    noise_levels = [1.0, 5.0, 10.0]
    results = []
    
    for noise_level in noise_levels:
        print(f"ğŸ“Š Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼ {noise_level:.1f}%...")
        
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑˆÑƒĞ¼
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level/100.0)
        
        # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        # ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
        cm = confusion_matrix(y_test, y_pred_classes)
        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)
        cm_normalized = np.nan_to_num(cm_normalized)
        
        results.append({
            'noise_level': noise_level,
            'accuracy': accuracy,
            'matrix': cm_normalized
        })
        
        print(f"   Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: {accuracy:.3f}")
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    output_dir = "Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ•_Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ«/Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ•_ĞœĞĞ¢Ğ Ğ˜Ğ¦Ğ«_ALEXNET"
    os.makedirs(output_dir, exist_ok=True)
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ• Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹
    matrices_path = os.path.join(output_dir, "alexnet_confusion_matrices_FIXED.png")
    plot_fixed_confusion_matrices(results, label_encoder.classes_, matrices_path)
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
    report_path = os.path.join(output_dir, "fixed_matrices_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("ĞĞ¢Ğ§ĞĞ¢ ĞŸĞ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğœ ĞœĞĞ¢Ğ Ğ˜Ğ¦ĞĞœ ĞĞ¨Ğ˜Ğ‘ĞĞš\n")
        f.write("=" * 50 + "\n\n")
        f.write("ĞŸĞ ĞĞ‘Ğ›Ğ•ĞœĞ: Ğ’ PNG Ñ„Ğ°Ğ¹Ğ»Ğµ Ğ¾Ğ´Ğ½Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿ÑƒÑÑ‚Ğ°Ñ\n")
        f.write("Ğ Ğ•Ğ¨Ğ•ĞĞ˜Ğ•: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑˆÑƒĞ¼Ğ°\n\n")
        f.write("Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ«:\n")
        for result in results:
            f.write(f"Î´ = {result['noise_level']:4.1f}%: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ {result['accuracy']:.3f}\n")
        f.write(f"\nĞ’ÑĞµ 4 Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾\n")
        f.write(f"ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ: 0%, 1%, 5%, 10% ÑˆÑƒĞ¼Ğ°\n")
    
    print(f"\nâœ… Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ• ĞœĞĞ¢Ğ Ğ˜Ğ¦Ğ« Ğ¡ĞĞ—Ğ”ĞĞĞ«!")
    print(f"ğŸ“Š Ğ¤Ğ°Ğ¹Ğ»: {matrices_path}")
    print(f"ğŸ“‹ ĞÑ‚Ñ‡Ñ‘Ñ‚: {report_path}")
    
    # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    print(f"\nğŸ¯ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ĞŸĞ Ğ£Ğ ĞĞ’ĞĞ¯Ğœ Ğ¨Ğ£ĞœĞ:")
    for result in results:
        print(f"Î´ = {result['noise_level']:4.1f}%: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ {result['accuracy']:.3f}")
        # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒ (Pd Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ)
        diag_values = [result['matrix'][i, i] for i in range(len(label_encoder.classes_))]
        print(f"   Pd Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ: {[f'{v:.3f}' for v in diag_values]}")
    
    return results

if __name__ == "__main__":
    results = run_quick_test()
    print("\nğŸ‰ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ• ĞœĞĞ¢Ğ Ğ˜Ğ¦ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!")