import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import ExtraTreesClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

def load_spectral_data_7_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 7 –≤–∏–¥–æ–≤ –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–∞–ø–æ–∫"""
    base_path = "–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    species_folders = ["–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"]
    
    all_data = []
    all_labels = []
    
    for species in species_folders:
        species_path = f"{base_path}/{species}"
        try:
            import os
            import glob
            excel_files = glob.glob(f"{species_path}/*_vis.xlsx")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        spectral_data = df[numeric_cols[0]].values
                        if len(spectral_data) > 0:
                            all_data.append(spectral_data)
                            all_labels.append(species)
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {species}: {e}")
            continue
    
    if len(all_data) == 0:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return np.array([]), np.array([])
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {len(np.unique(y))} –≤–∏–¥–æ–≤")
    return X, y

def load_spectral_data_20_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 20 –≤–∏–¥–æ–≤ –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–∞–ø–æ–∫"""
    base_path = "–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 20 –≤–∏–¥–æ–≤"
    species_folders = [
        "–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–µ–ª—å_–≥–æ–ª—É–±–∞—è", "–∏–≤–∞", "–∫–∞—à—Ç–∞–Ω", 
        "–∫–ª–µ–Ω", "–∫–ª–µ–Ω_–∞–º", "–ª–∏–ø–∞", "–ª–∏—Å—Ç–≤–µ–Ω–Ω–∏—Ü–∞", "–æ—Ä–µ—Ö", "–æ—Å–∏–Ω–∞", 
        "—Ä—è–±–∏–Ω–∞", "—Å–∏—Ä–µ–Ω—å", "—Å–æ—Å–Ω–∞", "—Ç–æ–ø–æ–ª—å_–±–∞–ª—å–∑–∞–º–∏—á–µ—Å–∫–∏–π", 
        "—Ç–æ–ø–æ–ª—å_—á–µ—Ä–Ω—ã–π", "—Ç—É—è", "—á–µ—Ä–µ–º—É—Ö–∞", "—è—Å–µ–Ω—å"
    ]
    
    all_data = []
    all_labels = []
    
    for species in species_folders:
        species_path = f"{base_path}/{species}"
        try:
            import os
            import glob
            excel_files = glob.glob(f"{species_path}/*_vis.xlsx")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        spectral_data = df[numeric_cols[0]].values
                        if len(spectral_data) > 0:
                            all_data.append(spectral_data)
                            all_labels.append(species)
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {species}: {e}")
            continue
    
    if len(all_data) == 0:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return np.array([]), np.array([])
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {len(np.unique(y))} –≤–∏–¥–æ–≤")
    return X, y

def add_gaussian_noise_correct(data, noise_level):
    """–ü–†–ê–í–ò–õ–¨–ù–û–ï –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —à—É–º–∞"""
    if noise_level == 0:
        return data
    
    # noise_level - —ç—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (0.1 –¥–ª—è 10%)
    std_dev = noise_level * np.std(data)
    noise = np.random.normal(0, std_dev, data.shape)
    return data + noise

def create_1d_alexnet(input_shape, num_classes):
    """–°–æ–∑–¥–∞–Ω–∏–µ 1D-AlexNet"""
    model = Sequential([
        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫ —Å–≤–µ—Ä—Ç–∫–∏
        Conv1D(96, 11, strides=4, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫ —Å–≤–µ—Ä—Ç–∫–∏
        Conv1D(256, 5, padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling1D(3, strides=2),
        
        # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫ —Å–≤–µ—Ä—Ç–∫–∏
        Conv1D(384, 3, padding='same', activation='relu'),
        
        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫ —Å–≤–µ—Ä—Ç–∫–∏
        Conv1D(384, 3, padding='same', activation='relu'),
        
        # –ü—è—Ç—ã–π –±–ª–æ–∫ —Å–≤–µ—Ä—Ç–∫–∏
        Conv1D(256, 3, padding='same', activation='relu'),
        MaxPooling1D(3, strides=2),
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        Flatten(),
        Dense(4096, activation='relu'),
        Dropout(0.5),
        Dense(4096, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def extra_trees_2_species():
    """ExtraTrees –¥–ª—è 2 –≤–∏–¥–æ–≤ (–æ—Å–∏–Ω–∞ –∏ —Å–∏—Ä–µ–Ω—å) —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú —à—É–º–æ–º"""
    print("=== EXTRA TREES –î–õ–Ø 2 –í–ò–î–û–í (–û–°–ò–ù–ê –ò –°–ò–†–ï–ù–¨) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú ===")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è 20 –≤–∏–¥–æ–≤
    data, labels = load_spectral_data_20_species()
    if len(data) == 0:
        return
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–∏–Ω—É –∏ —Å–∏—Ä–µ–Ω—å
    osina_mask = labels == '–æ—Å–∏–Ω–∞'
    siren_mask = labels == '—Å–∏—Ä–µ–Ω—å'
    
    if not np.any(osina_mask) or not np.any(siren_mask):
        print("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Å–∏–Ω—ã –∏–ª–∏ —Å–∏—Ä–µ–Ω–∏!")
        return
    
    # –í—ã–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Å–∏–Ω—ã –∏ —Å–∏—Ä–µ–Ω–∏
    X_filtered = data[osina_mask | siren_mask]
    y_filtered = labels[osina_mask | siren_mask]
    
    print(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(X_filtered)} –æ–±—Ä–∞–∑—Ü–æ–≤ (–æ—Å–∏–Ω–∞: {np.sum(osina_mask)}, —Å–∏—Ä–µ–Ω—å: {np.sum(siren_mask)})")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_filtered)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y_filtered)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (80% –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ)
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}, —Ç–µ—Å—Ç–æ–≤–∞—è: {len(X_test)}")
    
    # –û–±—É—á–µ–Ω–∏–µ ExtraTrees
    model = ExtraTreesClassifier(
        n_estimators=200, 
        max_depth=20, 
        min_samples_split=5, 
        min_samples_leaf=2,
        random_state=42
    )
    
    model.fit(X_train, y_train)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú —à—É–º–æ–º
    noise_levels = [0, 0.01, 0.05, 0.1]  # 0%, 1%, 5%, 10%
    
    for noise_level in noise_levels:
        if noise_level == 0:
            X_test_noisy = X_test
        else:
            X_test_noisy = add_gaussian_noise_correct(X_test, noise_level)
        
        y_pred = model.predict(X_test_noisy)
        y_pred_proba = model.predict_proba(X_test_noisy)
        
        accuracy = accuracy_score(y_test, y_pred)
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å —Å {noise_level*100}% —à—É–º–æ–º: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Excel —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–±–µ–∑ —à—É–º–∞)
    results = []
    for i in range(len(X_test)):
        max_prob_idx = np.argmax(y_pred_proba[i])
        row = [0] * len(label_encoder.classes_)
        row[max_prob_idx] = 1
        results.append(row)
    
    df_results = pd.DataFrame(results, columns=label_encoder.classes_)
    output_path = '–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/ExtraTrees_2_–≤–∏–¥–∞/extra_trees_2_species_results_CORRECTED.xlsx'
    df_results.to_excel(output_path, index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - ExtraTrees (2 –≤–∏–¥–∞) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú')
    plt.ylabel('–†–µ–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/ExtraTrees_2_–≤–∏–¥–∞/confusion_matrix_2_species_CORRECTED.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/ExtraTrees_2_–≤–∏–¥–∞/")

def alexnet_7_species():
    """1D-AlexNet –¥–ª—è 7 –≤–∏–¥–æ–≤ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú —à—É–º–æ–º"""
    print("=== 1D-ALEXNET –î–õ–Ø 7 –í–ò–î–û–í - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú ===")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data, labels = load_spectral_data_7_species()
    if len(data) == 0:
        return
    
    # Preprocessing
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}, —Ç–µ—Å—Ç–æ–≤–∞—è: {len(X_test)}")
    
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    model = create_1d_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=100,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú —à—É–º–æ–º
    noise_levels = [0, 0.01, 0.05, 0.1]  # 0%, 1%, 5%, 10%
    
    for noise_level in noise_levels:
        if noise_level == 0:
            X_test_noisy = X_test_cnn
        else:
            X_test_noisy = add_gaussian_noise_correct(X_test_cnn, noise_level)
        
        y_pred = model.predict(X_test_noisy)
        y_pred_classes = np.argmax(y_pred, axis=1)
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å —Å {noise_level*100}% —à—É–º–æ–º: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–±–µ–∑ —à—É–º–∞)
    y_pred = model.predict(X_test_cnn)
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    cm = confusion_matrix(y_test, y_pred_classes)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - 1D-AlexNet (7 –≤–∏–¥–æ–≤) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú')
    plt.ylabel('–†–µ–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_7_–≤–∏–¥–æ–≤/confusion_matrix_7_species_CORRECTED.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.plot(history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è –æ—à–∏–±–∫–∞')
    plt.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞')
    plt.title('–û—à–∏–±–∫–∞')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–û—à–∏–±–∫–∞')
    plt.legend()
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_7_–≤–∏–¥–æ–≤/training_history_7_species_CORRECTED.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_7_–≤–∏–¥–æ–≤/")

def alexnet_20_species():
    """1D-AlexNet –¥–ª—è 20 –≤–∏–¥–æ–≤ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú —à—É–º–æ–º"""
    print("=== 1D-ALEXNET –î–õ–Ø 20 –í–ò–î–û–í - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú ===")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data, labels = load_spectral_data_20_species()
    if len(data) == 0:
        return
    
    # Preprocessing
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)}, —Ç–µ—Å—Ç–æ–≤–∞—è: {len(X_test)}")
    
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    model = create_1d_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=150,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú —à—É–º–æ–º
    noise_levels = [0, 0.01, 0.05, 0.1]  # 0%, 1%, 5%, 10%
    
    for noise_level in noise_levels:
        if noise_level == 0:
            X_test_noisy = X_test_cnn
        else:
            X_test_noisy = add_gaussian_noise_correct(X_test_cnn, noise_level)
        
        y_pred = model.predict(X_test_noisy)
        y_pred_classes = np.argmax(y_pred, axis=1)
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å —Å {noise_level*100}% —à—É–º–æ–º: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–±–µ–∑ —à—É–º–∞)
    y_pred = model.predict(X_test_cnn)
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    cm = confusion_matrix(y_test, y_pred_classes)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - 1D-AlexNet (20 –≤–∏–¥–æ–≤) - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –®–£–ú')
    plt.ylabel('–†–µ–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_20_–≤–∏–¥–æ–≤/confusion_matrix_20_species_CORRECTED.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='–û–±—É—á–∞—é—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.plot(history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è –æ—à–∏–±–∫–∞')
    plt.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞')
    plt.title('–û—à–∏–±–∫–∞')
    plt.xlabel('–≠–ø–æ—Ö–∞')
    plt.ylabel('–û—à–∏–±–∫–∞')
    plt.legend()
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_20_–≤–∏–¥–æ–≤/training_history_20_species_CORRECTED.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_20_–≤–∏–¥–æ–≤/")

def main():
    print("=== –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ü–†–ê–í–ò–õ–¨–ù–´–ú –®–£–ú–û–ú ===")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    np.random.seed(42)
    tf.random.set_seed(42)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    import os
    os.makedirs('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/ExtraTrees_2_–≤–∏–¥–∞', exist_ok=True)
    os.makedirs('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_7_–≤–∏–¥–æ–≤', exist_ok=True)
    os.makedirs('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/AlexNet_20_–≤–∏–¥–æ–≤', exist_ok=True)
    
    print("\nüîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –®—É–º —Ç–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
    print("üìä –°—Ç–∞—Ä–∞—è —Ñ–æ—Ä–º—É–ª–∞: std_dev = noise_level / 100.0 * np.std(data)")
    print("‚úÖ –ù–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞: std_dev = noise_level * np.std(data)")
    print("üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: –®—É–º –≤ 100 —Ä–∞–∑ —Å–∏–ª—å–Ω–µ–µ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–µ–µ!")
    
    # 1. ExtraTrees –¥–ª—è 2 –≤–∏–¥–æ–≤
    extra_trees_2_species()
    
    # 2. AlexNet –¥–ª—è 7 –≤–∏–¥–æ–≤
    alexnet_7_species()
    
    # 3. AlexNet –¥–ª—è 20 –≤–∏–¥–æ–≤
    alexnet_20_species()
    
    print("\n=== –í–°–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú –®–£–ú–û–ú –ó–ê–í–ï–†–®–ï–ù–´ ===")
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ –§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/")

if __name__ == "__main__":
    main() 