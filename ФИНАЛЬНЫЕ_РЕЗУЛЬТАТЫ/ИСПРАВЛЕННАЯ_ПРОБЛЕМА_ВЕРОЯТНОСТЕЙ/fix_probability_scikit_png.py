#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π + PNG –º–∞—Ç—Ä–∏—Ü—ã

–ü–†–û–ë–õ–ï–ú–ê: –° —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —à—É–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ù–ï –ú–ï–ù–Ø–Æ–¢–°–Ø (0.1519 = –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞)
–†–ï–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å scikit-learn —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π

–†–ï–ó–£–õ–¨–¢–ê–¢: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ü–ê–î–ê–Æ–¢ —Å —Ä–æ—Å—Ç–æ–º —à—É–º–∞ + PNG –º–∞—Ç—Ä–∏—Ü—ã
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

# –†—É—Å—Å–∫–∏–µ —à—Ä–∏—Ñ—Ç—ã –¥–ª—è matplotlib
plt.rcParams['font.family'] = 'Arial Unicode MS'
plt.rcParams['axes.unicode_minus'] = False

def load_spring_data_7_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö 7 –≤–∏–¥–æ–≤ (–≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥)"""
    base_path = "–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    
    tree_types = ['–±–µ—Ä–µ–∑–∞', '–¥—É–±', '–µ–ª—å', '–∫–ª–µ–Ω', '–ª–∏–ø–∞', '–æ—Å–∏–Ω–∞', '—Å–æ—Å–Ω–∞']
    all_data = []
    all_labels = []
    
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    for i, tree_type in enumerate(tree_types):
        tree_path = os.path.join(base_path, tree_type)
        if not os.path.exists(tree_path):
            print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {tree_path}")
            continue
            
        files = [f for f in os.listdir(tree_path) if f.endswith('.xlsx')]
        print(f"   {tree_type}: {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        for file in files:
            try:
                file_path = os.path.join(tree_path, file)
                df = pd.read_excel(file_path)
                
                if df.shape[1] >= 2:
                    spectrum = df.iloc[:, 1].values
                    spectrum = spectrum[~np.isnan(spectrum)]
                    
                    if len(spectrum) > 100:
                        all_data.append(spectrum)
                        all_labels.append(i)
            except Exception as e:
                continue
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_data)} —Å–ø–µ–∫—Ç—Ä–æ–≤, {len(set(all_labels))} –∫–ª–∞—Å—Å–æ–≤")
    return np.array(all_data), np.array(all_labels), tree_types

def preprocess_data_properly(X, y):
    """–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    
    # 1. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω–µ
    min_length = min(len(spectrum) for spectrum in X)
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞: {min_length}")
    
    X_processed = np.array([spectrum[:min_length] for spectrum in X])
    
    # 2. –ú–Ø–ì–ö–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (RobustScaler)
    scaler = RobustScaler()
    X_processed = scaler.fit_transform(X_processed)
    
    print(f"   –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {X_processed.shape}")
    print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {X_processed.std():.6f}")
    
    return X_processed, y, scaler

def add_gaussian_noise(X, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —à—É–º–∞ –∫ –¥–∞–Ω–Ω—ã–º"""
    if noise_level == 0:
        return X
    
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

def create_robust_classifier():
    """–°–æ–∑–¥–∞–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
    return RandomForestClassifier(
        n_estimators=1000,          # –ú–Ω–æ–≥–æ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        max_depth=10,               # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        min_samples_split=5,        # –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        min_samples_leaf=2,         # –ú–∏–Ω–∏–º—É–º –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ
        max_features='sqrt',        # –°–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        bootstrap=True,             # –ë—É—Ç—Å—Ç—Ä–∞–ø –≤—ã–±–æ—Ä–∫–∞
        oob_score=True,            # Out-of-bag –æ—Ü–µ–Ω–∫–∞
        random_state=42,           # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
        n_jobs=-1                  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —è–¥–µ—Ä
    )

def evaluate_with_noise_levels(model, X_test, y_test, noise_levels, tree_types):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞"""
    results = []
    confusion_matrices = []
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞:")
    
    for noise_level in noise_levels:
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        X_test_noisy = add_gaussian_noise(X_test, noise_level)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        predictions = model.predict(X_test_noisy)
        probabilities = model.predict_proba(X_test_noisy)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        accuracy = accuracy_score(y_test, predictions)
        
        # –ê–Ω–∞–ª–∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        max_probabilities = np.max(probabilities, axis=1)
        mean_max_prob = np.mean(max_probabilities)
        std_max_prob = np.std(max_probabilities)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, predictions)
        confusion_matrices.append(cm)
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        unique_probs = len(np.unique(np.round(max_probabilities, 4)))
        uniqueness_ratio = unique_probs / len(max_probabilities) * 100
        
        results.append({
            'noise_level': noise_level,
            'noise_percent': noise_level * 100,
            'accuracy': accuracy,
            'mean_max_probability': mean_max_prob,
            'std_max_probability': std_max_prob,
            'unique_probs': unique_probs,
            'total_samples': len(max_probabilities),
            'uniqueness_ratio': uniqueness_ratio,
            'min_prob': np.min(max_probabilities),
            'max_prob': np.max(max_probabilities),
            'confusion_matrix': cm
        })
        
        print(f"   –®—É–º {noise_level*100:3.0f}%: —Ç–æ—á–Ω–æ—Å—Ç—å={accuracy:.3f}, "
              f"—Å—Ä–µ–¥–Ω—è—è_–º–∞–∫—Å_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={mean_max_prob:.3f}, "
              f"—É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å={uniqueness_ratio:.1f}%")
    
    return results, confusion_matrices

def create_confusion_matrices_png(results, tree_types, save_path="confusion_matrices_fixed.png"):
    """–°–æ–∑–¥–∞–Ω–∏–µ PNG —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —à—É–º–∞"""
    n_matrices = len(results)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª—å—à–æ–π —Ñ–∏–≥—É—Ä—ã
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (7 –≤–∏–¥–æ–≤)', 
                 fontsize=16, fontweight='bold')
    
    # –ü–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –æ—Å–µ–π –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    axes_flat = axes.flatten()
    
    for i, result in enumerate(results):
        if i >= 6:  # –ú–∞–∫—Å–∏–º—É–º 6 –º–∞—Ç—Ä–∏—Ü
            break
            
        ax = axes_flat[i]
        cm = result['confusion_matrix']
        noise_level = result['noise_percent']
        accuracy = result['accuracy']
        mean_prob = result['mean_max_probability']
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π [[memory:4010318]]
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        sns.heatmap(cm_normalized, 
                   annot=True, 
                   fmt='.3f',
                   cmap='Blues',
                   ax=ax,
                   xticklabels=tree_types,
                   yticklabels=tree_types,
                   cbar_kws={'shrink': 0.8})
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        if mean_prob < 0.9:  # –ï—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–∞–¥–∞—é—Ç
            color = 'green'
            status = '‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û'
        else:
            color = 'red' 
            status = '‚ùå –ü–†–û–ë–õ–ï–ú–ê'
            
        ax.set_title(f'–®—É–º: {noise_level:.0f}% | –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}\n'
                    f'–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {mean_prob:.3f} {status}',
                    fontsize=12, color=color, fontweight='bold')
        ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"üìä PNG –º–∞—Ç—Ä–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")
    plt.show()

def create_probability_analysis_png(results, save_path="probability_analysis_fixed.png"):
    """–°–æ–∑–¥–∞–Ω–∏–µ PNG —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
    df_results = pd.DataFrame(results)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è —à—É–º–∞ –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (7 –≤–∏–¥–æ–≤)', 
                 fontsize=16, fontweight='bold')
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å vs –®—É–º (–ì–õ–ê–í–ù–´–ô!)
    axes[0,0].plot(df_results['noise_percent'], df_results['mean_max_probability'], 
                   'ro-', linewidth=3, markersize=10, label='–°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes[0,0].fill_between(df_results['noise_percent'], 
                          df_results['mean_max_probability'] - df_results['std_max_probability'],
                          df_results['mean_max_probability'] + df_results['std_max_probability'],
                          alpha=0.3, color='red')
    axes[0,0].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[0,0].set_ylabel('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
    prob_start = df_results.iloc[0]['mean_max_probability']
    prob_end = df_results.iloc[-1]['mean_max_probability']
    if prob_end < prob_start:
        title_color = 'green'
        title = '‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ü–ê–î–ê–ï–¢ —Å —Ä–æ—Å—Ç–æ–º —à—É–º–∞'
    else:
        title_color = 'red'
        title = '‚ùå –ü–†–û–ë–õ–ï–ú–ê: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ù–ï –ø–∞–¥–∞–µ—Ç'
        
    axes[0,0].set_title(title, color=title_color, fontweight='bold')
    axes[0,0].grid(True, alpha=0.3)
    axes[0,0].legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–æ—á–Ω–æ—Å—Ç—å vs –®—É–º
    axes[0,1].plot(df_results['noise_percent'], df_results['accuracy']*100, 
                   'bo-', linewidth=2, markersize=8)
    axes[0,1].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[0,1].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    axes[0,1].set_title('–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç —à—É–º–∞')
    axes[0,1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    axes[0,2].plot(df_results['noise_percent'], df_results['uniqueness_ratio'], 
                   'go-', linewidth=2, markersize=8)
    axes[0,2].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[0,2].set_ylabel('–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å (%)')
    axes[0,2].set_title('–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
    axes[0,2].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –î–∏–∞–ø–∞–∑–æ–Ω –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    axes[1,0].fill_between(df_results['noise_percent'], 
                          df_results['min_prob'], df_results['max_prob'],
                          alpha=0.5, color='purple', label='–î–∏–∞–ø–∞–∑–æ–Ω –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
    axes[1,0].plot(df_results['noise_percent'], df_results['mean_max_probability'], 
                   'k-', linewidth=2, label='–°—Ä–µ–¥–Ω–µ–µ')
    axes[1,0].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[1,0].set_ylabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    axes[1,0].set_title('–î–∏–∞–ø–∞–∑–æ–Ω –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
    axes[1,0].grid(True, alpha=0.3)
    axes[1,0].legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ 5: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
    axes[1,1].plot(df_results['noise_percent'], df_results['std_max_probability'], 
                   'mo-', linewidth=2, markersize=8)
    axes[1,1].set_xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    axes[1,1].set_ylabel('–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ')
    axes[1,1].set_title('–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
    axes[1,1].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 6: –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    axes[1,2].axis('off')
    table_data = []
    for _, row in df_results.iterrows():
        table_data.append([
            f"{row['noise_percent']:.0f}%",
            f"{row['accuracy']*100:.1f}%",
            f"{row['mean_max_probability']:.3f}",
            f"{row['uniqueness_ratio']:.1f}%"
        ])
    
    table = axes[1,2].table(cellText=table_data,
                           colLabels=['–®—É–º', '–¢–æ—á–Ω–æ—Å—Ç—å', '–ú–∞–∫—Å. –≤–µ—Ä.', '–£–Ω–∏–∫.'],
                           cellLoc='center',
                           loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    axes[1,2].set_title('–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"üìà PNG –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
    plt.show()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï –ø—Ä–æ–±–ª–µ–º—ã —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏"""
    print("üöÄ –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ –° –í–ï–†–û–Ø–¢–ù–û–°–¢–Ø–ú–ò\n")
    print("=" * 60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y, tree_types = load_spring_data_7_species()
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    X_processed, y_processed, scaler = preprocess_data_properly(X, y)
    
    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed
    )
    
    print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û–±—É—á–µ–Ω–∏–µ: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   –¢–µ—Å—Ç: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–æ–±–∞—Å—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
    model = create_robust_classifier()
    
    print("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.fit(X_train, y_train)
    
    print(f"   OOB Score: {model.oob_score_:.3f}")
    
    # 5. –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑ —à—É–º–∞
    predictions_clean = model.predict(X_test)
    accuracy_clean = accuracy_score(y_test, predictions_clean)
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ —à—É–º–∞: {accuracy_clean:.1%}")
    
    # 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —à—É–º–∞
    noise_levels = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]
    results, confusion_matrices = evaluate_with_noise_levels(
        model, X_test, y_test, noise_levels, tree_types
    )
    
    # 7. –°–æ–∑–¥–∞–Ω–∏–µ PNG —Ñ–∞–π–ª–æ–≤
    print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ PNG –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    create_confusion_matrices_png(results, tree_types, "confusion_matrices_FIXED.png")
    create_probability_analysis_png(results, "probability_analysis_FIXED.png")
    
    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df_results = pd.DataFrame(results)
    df_results.to_csv('FIXED_probability_results.csv', index=False)
    
    # 9. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "="*60)
    print("üìà –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("="*60)
    
    print(f"\nüéØ –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
    for _, row in df_results.iterrows():
        print(f"   –®—É–º {row['noise_percent']:3.0f}%: "
              f"—Ç–æ—á–Ω–æ—Å—Ç—å={row['accuracy']*100:5.1f}%, "
              f"–º–∞–∫—Å_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={row['mean_max_probability']:.3f}, "
              f"—É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å={row['uniqueness_ratio']:4.1f}%")
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê
    prob_0 = df_results.iloc[0]['mean_max_probability']
    prob_max = df_results.iloc[-1]['mean_max_probability']
    
    print(f"\n‚úÖ –ü–†–û–í–ï–†–ö–ê –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:")
    if prob_max < prob_0:
        print(f"   ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û! –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ü–ê–î–ê–ï–¢: {prob_0:.3f} ‚Üí {prob_max:.3f}")
        print(f"   ‚úÖ –°–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ {(prob_0-prob_max)/prob_0*100:.1f}%")
        status = "–£–°–ü–ï–®–ù–û –ò–°–ü–†–ê–í–õ–ï–ù–û"
    else:
        print(f"   ‚ùå –ü–†–û–ë–õ–ï–ú–ê –û–°–¢–ê–ï–¢–°–Ø! –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ù–ï –ø–∞–¥–∞–µ—Ç: {prob_0:.3f} ‚Üí {prob_max:.3f}")
        status = "–¢–†–ï–ë–£–ï–¢ –î–ê–õ–¨–ù–ï–ô–®–ï–ô –†–ê–ë–û–¢–´"
    
    print(f"\nüèÜ –ò–¢–û–ì: {status}")
    print(f"üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"   ‚Ä¢ confusion_matrices_FIXED.png - –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫")
    print(f"   ‚Ä¢ probability_analysis_FIXED.png - –ê–Ω–∞–ª–∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
    print(f"   ‚Ä¢ FIXED_probability_results.csv - –ß–∏—Å–ª–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")

if __name__ == "__main__":
    main()