import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

def load_spectral_data_7_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 7 –≤–∏–¥–æ–≤"""
    base_path = "–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    species_folders = ["–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"]
    
    all_data = []
    all_labels = []
    
    for species in species_folders:
        species_path = f"{base_path}/{species}"
        try:
            import os
            import glob
            excel_files = glob.glob(f"{species_path}/*_vis.xlsx")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        spectral_data = df[numeric_cols[0]].values
                        if len(spectral_data) > 0:
                            all_data.append(spectral_data)
                            all_labels.append(species)
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {species}: {e}")
            continue
    
    if len(all_data) == 0:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return np.array([]), np.array([])
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {len(np.unique(y))} –≤–∏–¥–æ–≤")
    return X, y

def create_original_1d_alexnet(input_shape, num_classes):
    """–°–æ–∑–¥–∞–Ω–∏–µ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ô 1D-AlexNet –ë–ï–ó Dropout"""
    model = Sequential([
        Conv1D(96, 11, strides=4, activation='relu', input_shape=input_shape),
        MaxPooling1D(3, strides=2),
        
        Conv1D(256, 5, padding='same', activation='relu'),
        MaxPooling1D(3, strides=2),
        
        Conv1D(384, 3, padding='same', activation='relu'),
        Conv1D(384, 3, padding='same', activation='relu'),
        Conv1D(256, 3, padding='same', activation='relu'),
        MaxPooling1D(3, strides=2),
        
        Flatten(),
        Dense(4096, activation='relu'),
        Dense(4096, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def diagnose_model_problem():
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –º–æ–¥–µ–ª—å—é"""
    print("=== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú–´ –° –ú–û–î–ï–õ–¨–Æ ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    data, labels = load_spectral_data_7_species()
    if len(data) == 0:
        return
    
    # Preprocessing
    print("2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(label_encoder.classes_)}")
    print(f"–ö–ª–∞—Å—Å—ã: {label_encoder.classes_}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("3. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_original_1d_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=100,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("5. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    y_pred_proba = model.predict(X_test_cnn, verbose=0)
    y_pred_classes = np.argmax(y_pred_proba, axis=1)
    
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
    print(f"–§–æ—Ä–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {y_pred_proba.shape}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö: {np.unique(y_pred_proba)}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É:")
    for i, class_name in enumerate(label_encoder.classes_):
        mean_prob = np.mean(y_pred_proba[:, i])
        print(f"  {class_name}: {mean_prob:.6f}")
    
    print(f"\nüîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó:")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
    max_probs = np.max(y_pred_proba, axis=1)
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {np.mean(max_probs):.6f}")
    print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(max_probs):.6f}")
    print(f"  –ú–∏–Ω–∏–º—É–º: {np.min(max_probs):.6f}")
    print(f"  –ú–∞–∫—Å–∏–º—É–º: {np.max(max_probs):.6f}")
    
    print(f"\nüéØ –ü–†–û–ë–õ–ï–ú–ê –û–ë–ù–ê–†–£–ñ–ï–ù–ê:")
    if np.std(max_probs) < 1e-6:
        print("‚ùå –í–°–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –û–î–ò–ù–ê–ö–û–í–´–ï!")
        print("   –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ–º")
        print("   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   1. –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (4096 –Ω–µ–π—Ä–æ–Ω–æ–≤)")
        print("   2. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–Ω–µ—Ç Dropout)")
        print("   3. –ü—Ä–æ–±–ª–µ–º–∞ —Å —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞")
        print("   4. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –±–∞—Ç—á–∞
    print(f"\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –†–ê–ó–ù–´–ú–ò –†–ê–ó–ú–ï–†–ê–ú–ò –ë–ê–¢–ß–ê:")
    batch_sizes = [1, 8, 16, 32, 64]
    
    for batch_size in batch_sizes:
        print(f"\n–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
        test_model = create_original_1d_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
        
        # –û–±—É—á–∞–µ–º —Å –Ω–æ–≤—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞
        test_model.fit(
            X_train_cnn, y_train,
            epochs=5,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
            batch_size=batch_size,
            validation_split=0.2,
            verbose=0
        )
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º
        test_pred_proba = test_model.predict(X_test_cnn, verbose=0)
        test_max_probs = np.max(test_pred_proba, axis=1)
        test_accuracy = accuracy_score(y_test, np.argmax(test_pred_proba, axis=1))
        
        print(f"  –°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {np.mean(test_max_probs):.6f}")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(test_max_probs):.6f}")
        print(f"  –¢–æ—á–Ω–æ—Å—Ç—å: {test_accuracy*100:.2f}%")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print(f"1. –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤ (4096 -> 512)")
    print(f"2. –î–æ–±–∞–≤–∏—Ç—å Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏")
    print(f"3. –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–æ 8-16")
    print(f"4. –î–æ–±–∞–≤–∏—Ç—å BatchNormalization")
    print(f"5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º–∞–ª–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö")

if __name__ == "__main__":
    diagnose_model_problem() 