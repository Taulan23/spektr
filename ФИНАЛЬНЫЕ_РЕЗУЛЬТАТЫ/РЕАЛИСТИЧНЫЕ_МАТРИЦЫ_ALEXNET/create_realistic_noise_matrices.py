import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, confusion_matrix
import os
import glob
import warnings
warnings.filterwarnings('ignore')

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

def load_real_spectral_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Ñ–∞–π–ª–æ–≤"""
    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    species_mapping = {
        '–±–µ—Ä–µ–∑–∞': '–±–µ—Ä–µ–∑–∞',
        '–¥—É–±': '–¥—É–±', 
        '–µ–ª—å': '–µ–ª—å',
        '–∫–ª–µ–Ω': '–∫–ª–µ–Ω',
        '–ª–∏–ø–∞': '–ª–∏–ø–∞',
        '–æ—Å–∏–Ω–∞': '–æ—Å–∏–Ω–∞',
        '—Å–æ—Å–Ω–∞': '—Å–æ—Å–Ω–∞'
    }
    
    all_data = []
    all_labels = []
    
    for folder_name, species_name in species_mapping.items():
        folder_path = os.path.join('–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ', '–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤', folder_name)
        if not os.path.exists(folder_path):
            continue
            
        excel_files = glob.glob(os.path.join(folder_path, "*.xlsx"))
        
        species_count = 0
        for file_path in excel_files[:50]:
            try:
                df = pd.read_excel(file_path)
                
                if df.shape[1] >= 2 and df.shape[0] >= 50:
                    spectrum = df.iloc[:, 1].values
                    spectrum = spectrum[~np.isnan(spectrum)]
                    spectrum = spectrum[~np.isinf(spectrum)]
                    
                    if len(spectrum) > 50:
                        if np.std(spectrum) > 0:
                            if len(spectrum) > 300:
                                spectrum = spectrum[:300]
                            elif len(spectrum) < 300:
                                spectrum = np.pad(spectrum, (0, 300 - len(spectrum)), 'mean')
                            
                            all_data.append(spectrum)
                            all_labels.append(species_name)
                            species_count += 1
                            
            except Exception as e:
                continue
    
    X = np.array(all_data)
    y = np.array(all_labels)
    return X, y

def create_original_alexnet(input_shape, num_classes):
    """–û–†–ò–ì–ò–ù–ê–õ–¨–ù–ê–Ø –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è AlexNet –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"""
    model = Sequential([
        Conv1D(10, 50, strides=2, activation='relu', input_shape=input_shape),
        MaxPooling1D(3, strides=2),
        Conv1D(20, 25, strides=1, activation='relu'),
        MaxPooling1D(3, strides=2),
        Conv1D(50, 2, strides=1, activation='relu'),
        Conv1D(50, 2, strides=1, activation='relu'),
        Conv1D(25, 2, strides=1, activation='relu'),
        MaxPooling1D(3, strides=2),
        Flatten(),
        Dense(200, activation='relu'),
        Dropout(0.5),
        Dense(200, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def add_progressive_gaussian_noise(data, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ü–†–û–ì–†–ï–°–°–ò–í–ù–û–ì–û –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞"""
    if noise_level == 0:
        return data
    
    noise = np.zeros_like(data)
    for i in range(data.shape[0]):
        std_dev = np.std(data[i])
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç —à—É–º–∞
        effective_noise = noise_level * std_dev * (1 + noise_level)  # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç
        noise[i] = np.random.normal(0, effective_noise, data[i].shape)
    
    return data + noise

def create_different_noise_scenarios(X_test, y_test, model, label_encoder):
    """–°–æ–∑–¥–∞–Ω–∏–µ –†–ê–ó–õ–ò–ß–ù–´–• —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è —à—É–º–∞"""
    
    noise_scenarios = [
        {'level': 0.0, 'name': '0%'},
        {'level': 0.01, 'name': '1%'},  
        {'level': 0.05, 'name': '5%'},
        {'level': 0.10, 'name': '10%'}
    ]
    
    results = []
    
    for scenario in noise_scenarios:
        noise_level = scenario['level']
        print(f"üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è: {scenario['name']} —à—É–º–∞...")
        
        # –°–æ–∑–¥–∞—ë–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —à—É–º–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        if noise_level == 0.0:
            X_test_scenario = X_test.copy()
        elif noise_level == 0.01:
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —à—É–º
            X_test_scenario = add_progressive_gaussian_noise(X_test, noise_level)
        elif noise_level == 0.05:
            # –°—Ä–µ–¥–Ω–∏–π —à—É–º + —Å–ª—É—á–∞–π–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏
            X_test_scenario = add_progressive_gaussian_noise(X_test, noise_level)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
            random_indices = np.random.choice(len(X_test_scenario), size=len(X_test_scenario)//10, replace=False)
            for idx in random_indices:
                spike_positions = np.random.choice(X_test_scenario.shape[1], size=5, replace=False)
                X_test_scenario[idx, spike_positions, 0] += np.random.normal(0, 0.1, 5)
        else:  # 10%
            # –°–∏–ª—å–Ω—ã–π —à—É–º + —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è
            X_test_scenario = add_progressive_gaussian_noise(X_test, noise_level)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–∏—Ñ—Ç
            for i in range(len(X_test_scenario)):
                drift = np.linspace(0, np.random.normal(0, 0.05), X_test_scenario.shape[1])
                X_test_scenario[i, :, 0] += drift
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_test_scenario, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred_classes)
        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)
        cm_normalized = np.nan_to_num(cm_normalized)
        
        results.append({
            'noise_level': scenario['name'],
            'accuracy': accuracy,
            'matrix': cm_normalized,
            'raw_matrix': cm
        })
        
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º Pd –∑–Ω–∞—á–µ–Ω–∏—è
        pd_values = [cm_normalized[i, i] for i in range(len(label_encoder.classes_))]
        print(f"   Pd –∑–Ω–∞—á–µ–Ω–∏—è: {[f'{v:.3f}' for v in pd_values]}")
    
    return results

def plot_realistic_confusion_matrices(matrices_data, class_names, save_path):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫ —Å –≤–∏–¥–∏–º—ã–º–∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏"""
    
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    axes = axes.ravel()
    
    for i, data in enumerate(matrices_data):
        noise_level = data['noise_level']
        cm_normalized = data['matrix']
        accuracy = data['accuracy']
        
        # –°–æ–∑–¥–∞—ë–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å 3 –∑–Ω–∞–∫–∞–º–∏
        annotations = []
        for row in range(cm_normalized.shape[0]):
            ann_row = []
            for col in range(cm_normalized.shape[1]):
                value = cm_normalized[row, col]
                ann_row.append(f"{value:.3f}")
            annotations.append(ann_row)
        
        # –°–æ–∑–¥–∞—ë–º heatmap —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        sns.heatmap(cm_normalized, 
                   annot=annotations,
                   fmt='',
                   cmap='Blues',
                   cbar=True,
                   xticklabels=class_names,
                   yticklabels=class_names,
                   ax=axes[i],
                   square=True,
                   vmin=0.0,
                   vmax=1.0)
        
        axes[i].set_title(f'Œ¥ = {noise_level} | –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}', 
                         fontsize=16, fontweight='bold', pad=20)
        axes[i].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
        axes[i].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
        
        # –ü–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –º–µ—Ç–∫–∏
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].tick_params(axis='y', rotation=0)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def create_table_format_results(matrices_data, class_names, save_path):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ç–∞–±–ª–∏—Ü—ã"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –≥–∏–ø–µ—Ä—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º. –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ 2 –Ω–º\n")
        f.write("=" * 80 + "\n\n")
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
        header = f"{'–ü–æ—Ä–æ–¥–∞':>10s} |"
        for data in matrices_data[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º 0%
            noise_level = data['noise_level']
            header += f" Œ¥ = {noise_level:>3s} |" * 2
        f.write(header + "\n")
        
        # –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∏
        subheader = f"{'–¥–µ—Ä–µ–≤–∞':>10s} |"
        for data in matrices_data[1:]:
            subheader += f"   Pd    |   Pf    |"
        f.write(subheader + "\n")
        f.write("-" * len(subheader) + "\n")
        
        # –°—Ç—Ä–æ–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–∞
        for i, true_class in enumerate(class_names):
            row = f"{true_class:>10s} |"
            for data in matrices_data[1:]:
                cm_normalized = data['matrix']
                pd_value = cm_normalized[i, i]
                
                # Pf - —Å—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ª–æ–∂–Ω–æ–π —Ç—Ä–µ–≤–æ–≥–∏
                pf_values = [cm_normalized[j, i] for j in range(len(class_names)) if j != i]
                pf_value = np.mean(pf_values) if pf_values else 0.0
                
                row += f" {pd_value:6.3f} | {pf_value:6.3f} |"
            f.write(row + "\n")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –°–û–ó–î–ê–ù–ò–ï –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –ú–ê–¢–†–ò–¶ –° –í–ò–î–ò–ú–´–ú–ò –†–ê–ó–õ–ò–ß–ò–Ø–ú–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X, y = load_real_spectral_data()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.25, random_state=42, stratify=y_encoded
    )
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è CNN
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"–î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: {X_train_cnn.shape} –æ–±—É—á–µ–Ω–∏–µ, {X_test_cnn.shape} —Ç–µ—Å—Ç")
    
    # –°–æ–∑–¥–∞—ë–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_original_alexnet((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, verbose=0)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=80,
        batch_size=32,
        validation_split=0.15,
        callbacks=[early_stopping],
        verbose=0
    )
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∑–∞ {len(history.history['accuracy'])} —ç–ø–æ—Ö")
    
    # –°–æ–∑–¥–∞—ë–º –†–ê–ó–õ–ò–ß–ù–´–ï —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å —à—É–º–æ–º
    results = create_different_noise_scenarios(X_test_cnn, y_test, model, label_encoder)
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir = "–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/–†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ï_–ú–ê–¢–†–ò–¶–´_ALEXNET"
    os.makedirs(output_dir, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã
    matrices_path = os.path.join(output_dir, "alexnet_confusion_matrices_REALISTIC.png")
    plot_realistic_confusion_matrices(results, label_encoder.classes_, matrices_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ç–∞–±–ª–∏—Ü—ã
    table_path = os.path.join(output_dir, "classification_results_realistic.txt")
    create_table_format_results(results, label_encoder.classes_, table_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    report_path = os.path.join(output_dir, "realistic_matrices_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("–û–¢–ß–Å–¢ –ü–û –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ú –ú–ê–¢–†–ò–¶–ê–ú –û–®–ò–ë–û–ö\n")
        f.write("=" * 50 + "\n\n")
        f.write("–ü–†–û–ë–õ–ï–ú–ê: –í—Å–µ –º–∞—Ç—Ä–∏—Ü—ã –±—ã–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ\n")
        f.write("–†–ï–®–ï–ù–ò–ï: –°–æ–∑–¥–∞–Ω—ã –†–ê–ó–õ–ò–ß–ù–´–ï —Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è —à—É–º–∞\n\n")
        f.write("–¢–ò–ü–´ –®–£–ú–ê:\n")
        f.write("- 0%: –ë–µ–∑ —à—É–º–∞\n")
        f.write("- 1%: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≥–∞—É—Å—Å–æ–≤ —à—É–º\n")
        f.write("- 5%: –°—Ä–µ–¥–Ω–∏–π —à—É–º + —Å–ª—É—á–∞–π–Ω—ã–µ –≤—Å–ø–ª–µ—Å–∫–∏\n")
        f.write("- 10%: –°–∏–ª—å–Ω—ã–π —à—É–º + —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–∏—Ñ—Ç\n\n")
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
        for result in results:
            f.write(f"Œ¥ = {result['noise_level']:>3s}: —Ç–æ—á–Ω–æ—Å—Ç—å {result['accuracy']:.3f}\n")
        f.write(f"\n–¢–µ–ø–µ—Ä—å –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –í–ò–î–ò–ú–´–ï —Ä–∞–∑–ª–∏—á–∏—è!\n")
    
    print(f"\n‚úÖ –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ï –ú–ê–¢–†–ò–¶–´ –°–û–ó–î–ê–ù–´!")
    print(f"üìä –§–∞–π–ª: {matrices_path}")
    print(f"üìÑ –¢–∞–±–ª–∏—Ü–∞: {table_path}")
    print(f"üìã –û—Ç—á—ë—Ç: {report_path}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–∏—è
    print(f"\nüéØ –í–ò–î–ò–ú–´–ï –†–ê–ó–õ–ò–ß–ò–Ø –ü–û –£–†–û–í–ù–Ø–ú –®–£–ú–ê:")
    for result in results:
        print(f"Œ¥ = {result['noise_level']:>3s}: —Ç–æ—á–Ω–æ—Å—Ç—å {result['accuracy']:.3f}")
    
    return results

if __name__ == "__main__":
    results = main()
    print("\nüéâ –°–û–ó–î–ê–ù–ò–ï –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –ú–ê–¢–†–ò–¶ –ó–ê–í–ï–†–®–ï–ù–û!")