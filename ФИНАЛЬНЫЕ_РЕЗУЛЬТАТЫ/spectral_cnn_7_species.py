import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

def load_spectral_data_7_species():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 7 –≤–∏–¥–æ–≤"""
    base_path = "–ò—Å—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ/–°–ø–µ–∫—Ç—Ä—ã, –≤–µ—Å–µ–Ω–Ω–∏–π –ø–µ—Ä–∏–æ–¥, 7 –≤–∏–¥–æ–≤"
    species_folders = ["–±–µ—Ä–µ–∑–∞", "–¥—É–±", "–µ–ª—å", "–∫–ª–µ–Ω", "–ª–∏–ø–∞", "–æ—Å–∏–Ω–∞", "—Å–æ—Å–Ω–∞"]
    
    all_data = []
    all_labels = []
    
    for species in species_folders:
        species_path = f"{base_path}/{species}"
        try:
            import os
            import glob
            excel_files = glob.glob(f"{species_path}/*_vis.xlsx")
            
            for file_path in excel_files:
                try:
                    df = pd.read_excel(file_path)
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        spectral_data = df[numeric_cols[0]].values
                        if len(spectral_data) > 0:
                            all_data.append(spectral_data)
                            all_labels.append(species)
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {species}: {e}")
            continue
    
    if len(all_data) == 0:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return np.array([]), np.array([])
    
    X = np.array(all_data)
    y = np.array(all_labels)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è {len(np.unique(y))} –≤–∏–¥–æ–≤")
    return X, y

def create_spectral_cnn(input_shape, num_classes):
    """–°–æ–∑–¥–∞–Ω–∏–µ CNN —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    model = Sequential([
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        Conv1D(32, 7, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(2),
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π - –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        Conv1D(64, 5, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(2),
        
        # –¢—Ä–µ—Ç–∏–π —Å–ª–æ–π - –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        Conv1D(128, 3, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(2),
        
        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π —Å–ª–æ–π - —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        Conv1D(256, 3, activation='relu'),
        BatchNormalization(),
        GlobalAveragePooling1D(),  # –í–º–µ—Å—Ç–æ Flatten
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def add_gaussian_noise(data, noise_level):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∞—É—Å—Å–æ–≤–æ–≥–æ —à—É–º–∞"""
    if noise_level == 0:
        return data
    
    noise = np.random.normal(0, noise_level * np.std(data), data.shape)
    return data + noise

def train_spectral_cnn():
    """–û–±—É—á–µ–Ω–∏–µ CNN –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("=== CNN –î–õ–Ø –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• 7 –í–ò–î–û–í ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    data, labels = load_spectral_data_7_species()
    if len(data) == 0:
        return
    
    # Preprocessing
    print("2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data)
    
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(labels)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
    
    print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(label_encoder.classes_)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("3. –°–æ–∑–¥–∞–Ω–∏–µ CNN –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    model = create_spectral_cnn((X_train_cnn.shape[1], 1), len(label_encoder.classes_))
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)
    
    history = model.fit(
        X_train_cnn, y_train,
        epochs=150,
        batch_size=8,  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        validation_split=0.2,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ —à—É–º–∞
    print("5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ —à—É–º–∞...")
    y_pred_proba = model.predict(X_test_cnn, verbose=0)
    y_pred_classes = np.argmax(y_pred_proba, axis=1)
    accuracy_clean = accuracy_score(y_test, y_pred_classes)
    
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ —à—É–º–∞: {accuracy_clean*100:.2f}%")
    
    # –ê–Ω–∞–ª–∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å —à—É–º–æ–º
    print("6. –ê–Ω–∞–ª–∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å —à—É–º–æ–º...")
    noise_levels = [0, 0.1, 0.2, 0.5, 1.0, 2.0]
    results = []
    
    for noise_level in noise_levels:
        print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —à—É–º–æ–º {noise_level*100}%...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        X_test_noisy = add_gaussian_noise(X_test_cnn, noise_level)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = model.predict(X_test_noisy, verbose=0)
        y_pred_classes = np.argmax(y_pred_proba, axis=1)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        max_probs = np.max(y_pred_proba, axis=1)
        mean_max_prob = np.mean(max_probs)
        std_max_prob = np.std(max_probs)
        accuracy = accuracy_score(y_test, y_pred_classes)
        
        results.append({
            'noise_level': noise_level,
            'noise_percent': noise_level * 100,
            'mean_max_probability': mean_max_prob,
            'std_max_probability': std_max_prob,
            'accuracy': accuracy,
            'min_prob': np.min(max_probs),
            'max_prob': np.max(max_probs)
        })
        
        print(f"      –°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å. –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {mean_max_prob:.4f}")
        print(f"      –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_max_prob:.4f}")
        print(f"      –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy*100:.2f}%")
    
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    df_results = pd.DataFrame(results)
    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ CNN –î–õ–Ø –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•:")
    print("="*60)
    print(df_results.to_string(index=False))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(15, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å vs —à—É–º
    plt.subplot(2, 2, 1)
    plt.plot(df_results['noise_percent'], df_results['mean_max_probability'], 'bo-', linewidth=2, markersize=8)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    plt.title('–í–ª–∏—è–Ω–∏–µ —à—É–º–∞ –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–æ—á–Ω–æ—Å—Ç—å vs —à—É–º
    plt.subplot(2, 2, 2)
    plt.plot(df_results['noise_percent'], df_results['accuracy']*100, 'ro-', linewidth=2, markersize=8)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å (%)')
    plt.title('–í–ª–∏—è–Ω–∏–µ —à—É–º–∞ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –î–∏–∞–ø–∞–∑–æ–Ω –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    plt.subplot(2, 2, 3)
    plt.fill_between(df_results['noise_percent'], 
                     df_results['min_prob'], 
                     df_results['max_prob'], 
                     alpha=0.3, color='green')
    plt.plot(df_results['noise_percent'], df_results['mean_max_probability'], 'go-', linewidth=2, markersize=8)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
    plt.title('–î–∏–∞–ø–∞–∑–æ–Ω –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
    plt.subplot(2, 2, 4)
    plt.plot(df_results['noise_percent'], df_results['std_max_probability'], 'mo-', linewidth=2, markersize=8)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (%)')
    plt.ylabel('–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ')
    plt.title('–ò–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/spectral_cnn_—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df_results.to_csv('–§–ò–ù–ê–õ–¨–ù–´–ï_–†–ï–ó–£–õ–¨–¢–ê–¢–´/spectral_cnn_—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.csv', index=False)
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã
    print("\n" + "="*60)
    print("üîç –ê–ù–ê–õ–ò–ó CNN –î–õ–Ø –°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•:")
    print("="*60)
    
    if df_results['mean_max_probability'].iloc[-1] < df_results['mean_max_probability'].iloc[0]:
        print("‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–Ω–∏–∂–∞—é—Ç—Å—è —Å —à—É–º–æ–º!")
        print("   –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
    else:
        print("‚ùå –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ –µ—â–µ —Ä–∞—Å—Ç—É—Ç —Å —à—É–º–æ–º")
    
    print(f"\nüìà –ò–ó–ú–ï–ù–ï–ù–ò–Ø:")
    print(f"–ë–µ–∑ —à—É–º–∞:     {df_results['mean_max_probability'].iloc[0]:.4f}")
    print(f"–° –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —à—É–º–æ–º: {df_results['mean_max_probability'].iloc[-1]:.4f}")
    print(f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ:    {df_results['mean_max_probability'].iloc[-1] - df_results['mean_max_probability'].iloc[0]:.4f}")
    
    print(f"\nüéØ –ù–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:")
    print(f"- GlobalAveragePooling1D –≤–º–µ—Å—Ç–æ Flatten")
    print(f"- –ú–µ–Ω—å—à–∏–µ —Ä–∞–∑–º–µ—Ä—ã —Å–ª–æ–µ–≤ (32, 64, 128, 256)")
    print(f"- –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (8)")
    print(f"- –ë–æ–ª—å—à–µ —ç–ø–æ—Ö (150)")
    print(f"- –ë–æ–ª—å—à–µ —Ç–µ—Ä–ø–µ–Ω–∏–µ (20)")
    
    print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢:")
    print(f"- –ú–æ–¥–µ–ª—å: CNN –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print(f"- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Å–ø–µ–∫—Ç—Ä–æ–≤")
    print(f"- –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 8")
    print(f"- –≠–ø–æ—Ö–∏: {len(history.history['accuracy'])}")

if __name__ == "__main__":
    train_spectral_cnn() 